{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"archives","text":"","link":"/archives/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"contact","text":"","link":"/contact/index.html"},{"title":"friends","text":"","link":"/friends/index.html"},{"title":"友情链接","text":"","link":"/link/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"JVM-1","text":"JVM内存模型的初步认识 内存模型 堆 上图其实就是jdk1.8之前和之后的一个堆内容的区别，就是把永久代改名为元空间而已 如上图所示 方法区、堆是所有线程共享的，java栈、本地方法栈、程序计数器都是线程私有的 默认情况下，JVM使用的内存的最大内存为电脑内存的四分之一，初始化的内存为六十四分之一 内存布局 对象头对象头中主要是有两类数据 官方称它为“Mark Word”，用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等 类型指针，即对象指向它的类型元数据的指针，Java虚拟机通过这个指针来确定该对象是哪个类的实例 通过类型指针来确定该对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不一定要经过对象本身 果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是如果数组的长度是不确定的，将无法通过元数据中的信息推断出数组的大小。 实例数据这部分是对象真正存储的有效信息，即我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的字段都必须记录起来 在默认的分配策略来看 前提条件：相同宽度的字段总是被分配到一起存放 在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前 对齐填充由于HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说就是任何对象的大小都必须是8字节的整数倍。对象头部分已经被精心设计成正好是8字节的倍数（1倍或者2倍），因此，如果对象实例数据部分没有对齐的话，就需要通过对齐填充来补全。 对象指针句柄·如果使用句柄访问的话，Java堆中将可能会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息 直接指针如果使用直接指针访问的话，Java堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销 对比直接指针的优势：速度更快，它节省了一次指针定位的时间开销，由于对象访问在Java中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本 就虚拟机HotSpot而言，采用的是直接指针 句柄的优势： 当在进行垃圾回收的时候，会产生对象移动，而句柄，只需要修改句柄池到对象实例数据的指针即可，速度非常快；而直接指针是需要修改java栈中的reference的 永久代（元空间）作用：这个区域是常驻内存的，一般用来存放JDK自身携带的Class对象，interface原数据，存储的是java运行时的一些环境吗，这个区域不会存在垃圾回收，关闭jvm会释放这个区域的内存。 jdk1.6之前：永久代，常量池在永久代中 jdk1.7：永久代，但是慢慢退化了，“去永久代”，常量池在堆中 jdk1.8之后：无永久代，改名为元空间，常量池在元空间内 元空间逻辑上存在，但是物理上不存在。 新生代内存+老年代内存=JVM最大内存 本地方法栈本地方法栈、java栈、程序计数器这三个部分中是不会有垃圾垃圾产生的，栈的作用是就是排序，用完就弹出了，所以不会有垃圾，而程序计数器只是单纯的进行数字的加减，当然不会有垃圾。 所以说平常说的jvm调优通常都是在堆（方法区其实是特殊的堆）中进行调优的 本地方法栈中的方法就是 带有native关键字的方法，带有native关键字表示java语言已经访问不到，需要调用c的库来执行，通过JNI来调用 JNI作用：扩展java的使用，融合不同的编程语言供java所使用 方法区方法区时被所有线程所共享的，所有字段和方法字节码，以及一些特殊方法，如构造函数，接口代码也在此定义，简单说，所有定义的方法的信息都保存在该区域，此区域属于共享区间 静态变量、常量、类信息（构造方法、接口定义）、运行时的常量池存在方法区中，但是实例变量存在堆内存中，和方法区无关 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池表（Constant Pool Table），用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可以将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 程序计数器程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是本地（Native）方法，这个计数器值则应为空（Undefined）。此内存区域是唯一一个在《Java虚拟机规范》中没有规定任何OutOfMemoryError情况的区域。 垃圾清理过程垃圾清理过程 gc日志查看young gc full gc 垃圾回收算法引用计数法在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的 问题在于，当a引用b，b也引用a，除此之外，无任何引用的时候，两个对象再无任何引用，实际上这两个对象已经不可能再被访问，但是它们因为互相引用着对方，导致它们的引用计数都不为零，引用计数算法也就无法回收它们。 可达性分析算法从GC Roots开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”（Reference Chain） 如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。 GC Roots 在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。· 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。 ·在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。· 在本地方法栈中JNI（即通常所说的Native方法）引用的对象。 ·Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（比如NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。· 所有被同步锁（synchronized关键字）持有的对象。· 反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整GC Roots集合。 譬如后文将会提到的分代收集和局部回收（Partial GC），如果只针对Java堆中某一块区域发起垃圾收集时（如最典型的只针对新生代的垃圾收集），必须考虑到内存区域是虚拟机自己的实现细节（在用户视角里任何内存区域都是不可见的），更不是孤立封闭的，这时候就需要将这些关联区域的对象也一并加入GC Roots集合中去，才能保证可达性分析的正确性 引用强引用强引用是最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似“Object obj=new Object()”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用·软引用是用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK 1.2版之后提供了SoftReference类来实现软引用 弱引用·弱引用也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2版之后提供了WeakReference类来实现弱引用。 虚引用·虚引用也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2版之后提供了PhantomReference类来实现虚引用。 对象死亡即使在可达性分析算法中判定为不可达的对象，也不是“非死不可”的，这时候它们暂时还处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程： 如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记，随后进行一次筛选 筛选的条件是此对象是否有必要执行finalize()方法。假如对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为“没有必要执行”。 任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行 不建议使用finalize 它的运行代价高昂，不确定性大，无法保证各个对象的调用顺序，如今已被官方明确声明为不推荐使用的语法。有些教材中描述它适合做“关闭外部资源”之类的清理性工作，这完全是对finalize()方法用途的一种自我安慰。finalize()能做的所有工作，使用try-finally或者其他方式都可以做得更好、更及时，所以笔者建议大家完全可以忘掉Java语言里面的这个方法。 时机metadatametadata区域满了之后会触发full gc young gcyoung gc之前 空间担保机制：检查老年代可用对象是否大于新生代所有空间的大小，大于则进行young gc，小于则走第二步 是否开启HandlePromotionFailure 这个参数，开启则判断 判断老年代的内存大小是都大于之前每一次minor gc之后的进入老年代的对象的平均大小，如果大于则冒险直接往下走，小于则提前进行fullgc，否则提前进行full gc young gc之后 年龄太大了 suvivor区域放不下新生的对象 动态年龄判断：相同年龄对象总和大于s区的50%，此时年龄n以上的对象进入老年代 full gc 老年代可以设置一个阈值，大于这个这个阈值就会触发full gc 在执行Young GC之前，如果判断发现老年代可用空间小于了历次Young GC后升入老年代的平均对象大小的话，那么就会在Young GC之前触发Full GC，先回收掉老年代一批对象，然后再执行Young GC 如果Young GC过后的存活对象太多，Survivor区域放不下，就要放入老年代，要是此时老年代也放不下，就会触发Full GC，回收老年代一批对象，再把这些年轻代的存活对象放入老年代中 进入老年代时机 年龄到默认的15 对象太大了，直接进入老年代，不经过新生代 Minor gc之后，发现剩余对象太多，s区放不下 动态年龄判断，所有年龄大小的内存加起来超过一个s区的50%，n个年龄及以上对象直接进入老年代 oom如果是full gc之后，老年代还是没有足够的大小来放minor gc存活的对象，就会触发oom 三种情况 metaspace 元空间设置参数使用默认的 动态代理动态生成了一些类，可能会导致内存溢出 虚拟机栈可能是方法递归调用，就是有些残忍 堆内存 高并发 内存泄露","link":"/posts/867279821.html"},{"title":"JVM常用参数","text":"常用参数和日志打印 常用设置 参数 解释 -Xms java堆内存的大小 -Xmx java堆内存的最大大小 -XX:PeremSize 永久代大小 -XX:MaxpermSize 永久代最大大小 -Xmn 堆内存中的新生代大小 -Xss 每个线程栈内存的大小 -XX:Max Tenuring Threshold 多少岁进入老年代，默认是15 -XX PretenureSize Threshold 对象多大直接进入老年代，不经过新生代 -XX HandlePromotionFailure 是否要进行判断 判断老年代的内存大小是都大于之前每一次minor gc之后的进入老年代的对象的平均大小 -XX：SurvivorRatio=8 表示eden区域占比为80% -XX: +UseParNewGC 定垃圾回收器为ParNew，一般来说机器是几核，垃圾回收器并发执行的线程就会有几个，四核机器就是4个，8核机器就是8个 -XX：ParallelGCThreads 设置垃圾回收器使用线程数量，一般来说不要动 -XX: NewSize 新生代 -XX: +UserCMSCOmpactAtFullCollection 整理碎片 -XX: CMSFullBeforeCompaction=5 五次fullgc之后进行一次碎片整理 XX:CMSInitiatingOccupancyFraction 设置CMS老年代回收阀值百分比 -XX:+CMSParallelInitialMarkEnabled CMS垃圾回收器的“初始标记”阶段开启多线程并发执行 -XX:+CMSScavengeBeforeRemark CMS的重新标记阶段之前，先尽量执行一次Young GC，原因：如果大部分新生代的对象被回收掉了，那么作为GC根的部分少了，从而提高重新标记的效率 -XX：TraceClassLoding 追踪类加载情况 -XX: TraceClassUnloading 追踪类卸载情况 -XX: SoftRefLRUPolicyMSPerMB 软引用存活时间 -XX:+DisableExplicltGC 禁止显示执行GC，为了避免开发工程师调用system.gc()，在流量高时候频发触发full gc -XX:MetaspaceSize=10M 元空间内存设置 -XX:MaxMetaspaceSize=10M 元空间最大内存设置 -XX:+UseConcMarkSweepGC 使用CMS垃圾回收器 打印日志 参数 作用 -XX：+PrintGCDetails 打印gc详细日志 -XX:HeapDumpPath=/usr/local/app/oom 内存快照存放位置 -XX:+HeapDumpOnOutOfMemoryError 在oom的时候自动dump快照出来 -XX：+PrintGC 打印gc日志 -XX: =PrintGCTimesStamps 打印gc时间戳 -XX: +PrintHeapAtGC gc之后，打印堆信息 -Xloggc 打印日志","link":"/posts/782118306.html"},{"title":"RabbitMQ","text":"文章摘要 保证消息不丢失生产者通过rabbitmq的一个confirm机制，消息发送到mq之后，将消息持久化到磁盘之后，才会返回confirm给生产者 消费者rabbitmq默认的是自动ack的机制，但是可能会发生消费者已经收到消息，但是还没有来得及处理消息就宕机的情况，这中情况下，会出现消息丢失的情况 自动ack的机制： 就是消费者只要接收到mq的消息，就会立即返回ack，不管消息是否已经处理完毕 所以采用手动ack机制来确保，消息处理完毕之后，才将ack发送给mq集群 高并发⾸先，⽤来临时存放未 ack 消息的存储需要承载⾼并发写⼊，⽽且我们不需要什么复杂的运算 操作，这种存储⾸选绝对不是 MySQL 之类的数据库，⽽建议采⽤ kv 存储。kv 存储承载⾼并发 能⼒极强，⽽且 kv 操作性能很⾼。 其次，投递消息之后等待 ack 的过程必须是异步的，也就是类似上⾯那样的代码，已经给出了 ⼀个初步的异步回调的⽅式。 消息投递出去之后，这个投递的线程其实就可以返回了，⾄于每个消息的异步回调，是通过在 channel 注册⼀个 confirm 监听器实现的。 收到⼀个消息 ack 之后，就从 kv 存储中删除这条临时消息；收到⼀个消息 nack 之后，就从 kv 存储提取这条消息然后重新投递⼀次即可；也可以⾃⼰对 kv 存储⾥的消息做监控，如果超过⼀ 定时⻓没收到 ack，就主动重发消息。 ack机制原理主要是通过delivery tag delivery tag是一次消息的唯一标识，delivery tag是在一次channle中传递的 消息积压这个消息积压，主要是当你开启批量处理ack消息的时候，很多消息目前处于unack的情况 RabbitMQ基于一个prefetch count来控制这个unack message的数量。 如果消息的数量小于这个prefetch count，会继续将消息放入这个channel中，如果大于，必须要等待已经投递过去的消息被ack了，此时才能继续投递下一个消息。 prefetch count的这个数量非常重要 如果设置过大，会导致mq中存储了海量的数据，会导致消费者服务直接被击垮了，内存溢出，OOM，服务宕机，然后大量unack的消息会被重新投递给其他的消费者服务，此时其他消费者服务一样的情况，直接宕机，最后造成雪崩效应。 如果设置过小，此时就必然会导致消费者服务的吞吐量极低。因为你即使处理完一条消息，执行ack了也是异步的。 所以鉴于上面两种极端情况，RabbitMQ官方给出的建议是prefetch count一般设置在100~300之间。 集群普通集群模式 这种模式严格来说不算是分布式的结构，因为它所有的数据都是在一台机器上的，消费者消费的时候可以在任意一个rabbitmq中进行消费，如果没有实际数据，就从有实际数据上的mq上进行拉取元数据、真实数据的描述如具体位置等 优点 提高吞吐量，可以从多个节点来消费信息 缺点 如果放实际数据的mq宕机了，基本上这个架构就失效了 集群内部有大量数据传输 可用性几乎没有保障 镜像集群模式 生产者生产一条消息，将消息发送到一个mq中，mq会自动将信息同步到其他的mq上，每个mq上的数 据都是一样的，所以称之为镜像集群模式 开启方式rabbitmq有个很好的控制台，新增一条策略、这个策略就是开启开启镜像集群模式策略、指定的时候可以指定数据同步到所有的节点，也可以要求同步到指定的节点数量，之后你在创建queue时使用这个策略、就会在动降数据同步到其它节点上去了。 缺点 性能开销太大，消息同步到所有的节点服务器会导致网络带宽压力和消耗很严重。 这种模式没有扩展性可言，如果你某个queue的负载很高，你加机器，新增的机器也包含了这个queue的所有数据，并没有办法线性扩展你的queue 不是分布式的 为什么不是分布式的那？ 因为所有的数据还是单独存在在每一个机器上，而分布式应该是将数据存储在不同的的机器上，几个机器上合起来的数据才是一个完整的数据 性能优化批量发送ack消息","link":"/posts/3041119952.html"},{"title":"Rocketmq中的NameServer源码分析","text":"创建NamesrvController 我这里都是截取的部分代码，梳理大概逻辑，具体细节部分需要大家自行去阅读源码 NamesrvStartup12345public static NamesrvController main0(String[] args) { try { NamesrvController controller = createNamesrvController(args); start(controller); } 12345public static NamesrvController start(final NamesrvController controller) throws Exception { boolean initResult = controller.initialize(); controller.start(); return controller;} 1234567public void start() throws Exception { //真正地启动nameserver，这个start的是调用了remotingServer接口的start this.remotingServer.start(); if (this.fileWatchService != null) { this.fileWatchService.start(); }} RemotingService12345678public interface RemotingService { //这个start的实现类分两种，一个是客户端的，一个是服务端的，都是借助了netty来完成的 void start(); void shutdown(); void registerRPCHook(RPCHook rpcHook);} 初始化NameServerController12345678910111213141516171819202122232425262728public boolean initialize() { this.kvConfigManager.load(); this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService); this.remotingExecutor = Executors.newFixedThreadPool(nettyServerConfig.getServerWorkerThreads(), new ThreadFactoryImpl(&quot;RemotingExecutorThread_&quot;)); this.registerProcessor(); this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { //每隔10秒就要进行一次扫描，扫描出来所有已经掉线的broker，第一次延迟五秒 @Override public void run() { NamesrvController.this.routeInfoManager.scanNotActiveBroker(); } }, 5, 10, TimeUnit.SECONDS); this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { NamesrvController.this.kvConfigManager.printAllPeriodically(); } }, 1, 10, TimeUnit.MINUTES); return true;} RouteInfoManager12345678910111213141516//过期时间是两分钟private final static long BROKER_CHANNEL_EXPIRED_TIME = 1000 * 60 * 2;//扫描出所有已经掉线的brokerpublic void scanNotActiveBroker() { Iterator&lt;Entry&lt;String, BrokerLiveInfo&gt;&gt; it = this.brokerLiveTable.entrySet().iterator(); while (it.hasNext()) { Entry&lt;String, BrokerLiveInfo&gt; next = it.next(); long last = next.getValue().getLastUpdateTimestamp(); if ((last + BROKER_CHANNEL_EXPIRED_TIME) &lt; System.currentTimeMillis()) { RemotingUtil.closeChannel(next.getValue().getChannel()); it.remove(); log.warn(&quot;The broker channel expired, {} {}ms&quot;, next.getKey(), BROKER_CHANNEL_EXPIRED_TIME); this.onChannelDestroy(next.getKey(), next.getValue().getChannel()); } }} broker注册到nameserverNamesrvController12345678910private void registerProcessor() { if (namesrvConfig.isClusterTest()) { this.remotingServer.registerDefaultProcessor(new ClusterTestRequestProcessor(this, namesrvConfig.getProductEnvName()), this.remotingExecutor); } else { //nameservier默认的请求注册进入了，都交给nettyserver来进行处理 this.remotingServer.registerDefaultProcessor(new DefaultRequestProcessor(this), this.remotingExecutor); }} DefaultRequestProcessor123456789101112131415161718switch (request.getCode()) { case RequestCode.PUT_KV_CONFIG: return this.putKVConfig(ctx, request); case RequestCode.GET_KV_CONFIG: return this.getKVConfig(ctx, request); case RequestCode.DELETE_KV_CONFIG: return this.deleteKVConfig(ctx, request); case RequestCode.QUERY_DATA_VERSION: return queryBrokerTopicConfig(ctx, request); //将broker的请求注册到nameserver中 case RequestCode.REGISTER_BROKER: Version brokerVersion = MQVersion.value2Version(request.getVersion()); if (brokerVersion.ordinal() &gt;= MQVersion.Version.V3_0_11.ordinal()) { return this.registerBrokerWithFilterServer(ctx, request); } else { //注册broker到nameserver return this.registerBroker(ctx, request); } 1234567891011//调用了namesrvController.getRouteInfoManager().registerBroker方法真正将broker注册到nameserver中了 RegisterBrokerResult result = this.namesrvController.getRouteInfoManager().registerBroker( requestHeader.getClusterName(), requestHeader.getBrokerAddr(), requestHeader.getBrokerName(), requestHeader.getBrokerId(), requestHeader.getHaServerAddr(), topicConfigWrapper, null, ctx.channel() ); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100 public RegisterBrokerResult registerBroker( final String clusterName, final String brokerAddr, final String brokerName, final long brokerId, final String haServerAddr, final TopicConfigSerializeWrapper topicConfigWrapper, final List&lt;String&gt; filterServerList, final Channel channel) { RegisterBrokerResult result = new RegisterBrokerResult(); try { try { //加锁，同一时间只能有一个线程访问 this.lock.writeLock().lockInterruptibly();//根据clusterName获取到了一个set，之后每隔30秒发送的请求是没有影响的，set会自动去重的 Set&lt;String&gt; brokerNames = this.clusterAddrTable.get(clusterName); if (null == brokerNames) { brokerNames = new HashSet&lt;String&gt;(); this.clusterAddrTable.put(clusterName, brokerNames); } //将broker添加到一个集群里面 brokerNames.add(brokerName); boolean registerFirst = false;//根据brokername获取到brokerdata//brokerAddrTable存放了所有broker的详细路由信息 BrokerData brokerData = this.brokerAddrTable.get(brokerName); //如果broker第第一次进行注册，brokerDate会是null，会new一个BrokerData，将路由信息放入brokerAddrTable中 if (null == brokerData) { registerFirst = true; brokerData = new BrokerData(clusterName, brokerName, new HashMap&lt;Long, String&gt;()); this.brokerAddrTable.put(brokerName, brokerData); } Map&lt;Long, String&gt; brokerAddrsMap = brokerData.getBrokerAddrs(); //Switch slave to master: first remove &lt;1, IP:PORT&gt; in namesrv, then add &lt;0, IP:PORT&gt; //The same IP:PORT must only have one record in brokerAddrTable Iterator&lt;Entry&lt;Long, String&gt;&gt; it = brokerAddrsMap.entrySet().iterator(); while (it.hasNext()) { Entry&lt;Long, String&gt; item = it.next(); if (null != brokerAddr &amp;&amp; brokerAddr.equals(item.getValue()) &amp;&amp; brokerId != item.getKey()) { it.remove(); } } String oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr); registerFirst = registerFirst || (null == oldAddr); if (null != topicConfigWrapper &amp;&amp; MixAll.MASTER_ID == brokerId) { if (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion()) || registerFirst) { ConcurrentMap&lt;String, TopicConfig&gt; tcTable = topicConfigWrapper.getTopicConfigTable(); if (tcTable != null) { for (Map.Entry&lt;String, TopicConfig&gt; entry : tcTable.entrySet()) { this.createAndUpdateQueueData(brokerName, entry.getValue()); } } } }//这里是broker心跳的核心处理逻辑//默认每隔30秒就会有一个新的BrokerLiveInfo被put到brokerLiveTable，覆盖上一次的心跳时间//BrokerLiveInfo里面的这个 System.currentTimeMillis(),当前时间戳就是broker最新的心跳时间 BrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr, new BrokerLiveInfo( System.currentTimeMillis(), topicConfigWrapper.getDataVersion(), channel, haServerAddr)); if (null == prevBrokerLiveInfo) { log.info(&quot;new broker registered, {} HAServer: {}&quot;, brokerAddr, haServerAddr); } if (filterServerList != null) { if (filterServerList.isEmpty()) { this.filterServerTable.remove(brokerAddr); } else { this.filterServerTable.put(brokerAddr, filterServerList); } } if (MixAll.MASTER_ID != brokerId) { String masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID); if (masterAddr != null) { BrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr); if (brokerLiveInfo != null) { result.setHaServerAddr(brokerLiveInfo.getHaServerAddr()); result.setMasterAddr(masterAddr); } } } } finally { this.lock.writeLock().unlock(); } } catch (Exception e) { log.error(&quot;registerBroker Exception&quot;, e); } return result; }","link":"/posts/1645673060.html"},{"title":"SpringBoot自动装配","text":"自动装配原理介绍 启动类注解众所周知，这是springboot的启动类的注解 12345678@SpringBootApplicationpublic class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }} @SpringBootApplication12345678910@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })public @interface SpringBootApplication {} 大概可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。根据 SpringBoot 官网，这三个注解的作用分别是： @EnableAutoConfiguration：启用 SpringBoot 的自动配置机制 @Configuration：允许在上下文中注册额外的 bean 或导入其他配置类 @ComponentScan： 扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean。如下图所示，容器中将排除TypeExcludeFilter和AutoConfigurationExcludeFilter。 @EnableAutoConfiguration` 是实现自动装配的重要注解，我们以这个注解入手。 @EnableAutoConfiguration12345678910111213@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage//将main包下的所欲组件注册到容器中@Import({AutoConfigurationImportSelector.class})//加载自动装配类 xxxAutoconfigurationpublic @interface EnableAutoConfiguration { String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;; Class&lt;?&gt;[] exclude() default {}; String[] excludeName() default {};} 这个注解最重要的就是 AutoConfigurationImportSelector这个类了 AutoConfigurationImportSelectorAutoConfigurationImportSelector中重要的方法就是这个，主要负责加载自动配置类的。 123456789101112131415protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } //默认情况下返回的是@EnableAutoConfiguration中的两个属性 AnnotationAttributes attributes = getAttributes(annotationMetadata); List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = getConfigurationClassFilter().filter(configurations); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions);} getAttributes这个方法返回了@EnableAutoConfiguration中的两个属性 getCandidateConfigurations12345678//返回应考虑的自动配置类名称。 默认情况下，此方法将使用SpringFactoriesLoader和getSpringFactoriesLoaderFactoryClass()来加载候选SpringFactoriesLoader 。protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you &quot; + &quot;are using a custom packaging, make sure that file is correct.&quot;); return configurations;} filter这个方法主要是扫描到所有的 去除一些","link":"/posts/3571835313.html"},{"title":"SpringCloud1","text":"文章摘要 SpringCloudNetFlix 核心问题微服务架构核心问题 服务很多，客户端如何访问 这么多服务，服务之间如何通信 如何治理服务 服务挂了怎么办 解决方案 Spring Cloud 生态 Spring Cloud NetFlix 一站式解决方案！ api网关：zuul组件 服务调用 Feign–HttpClinet http通信方式。同步 阻塞 服务注册发现 Eureka 熔断机制 Hystrix 负载均衡 ribbon Apache Dubbo Zookeeper 半自动，需要整合别人的 api：没有，用第三方插件，或者自己实现 Dubbo Zookeeper 没有，借助 Hystrix Spring Cloud Alibaba 最新的 一站式解决方案 更简单 认识Spring Cloudspringboot专注于快速、方便的开发单个个体微服务，springCloud关注全局协调的微服务框架 现在大型网站的架构图 Spring Cloud NetFlixEureka服务注册和服务发现 必须要进行参数优化，否则速度太慢 nacos zookeeper rockemq中的nameserver也是这个作用 consul 为什么要搞两级缓存 目的就是为了避免并发冲突 集群模式 Eureka优化 Eureka和Zookeeper区别CAP理论： C 一致性 A 可用性 P 容错性 由于分区容错P在分布式系统中是必须要保证的，因此，偶问你只能在A和P之间进行权衡 Zookeeper保证的是CP Eureka保证的是AP Feign服务调用 整合eureka和ribbon Feign是面向接口的 Ribbon负载均衡 服务第一次被调用的时候，他会初始话一个ribbon组件，初始化这些组件可能会耗费一定时间，所以很容易导致服务超时。开启下面参数，让每个服务启动的时候就直接初始化ribbon相关的组件，避免第一次请求的时候初始化 Hystrix服务熔断在服务端 服务降级在客户端 服务熔断 定义：某个服务超时或者异常，引起熔断 类似于保险丝的作用 一般发生在服务端 服务降级 定义：从整体网站请求负载考虑，当前某些服务访问量比较大，会暂时关闭一些访问量比较小的服务，将整个网站的资源倾斜在访问量比较大的服务上，等到访问量下来，就重新开启那些服务；此时在客户端上，我们可以准备一个FallbackFactory，返回默认的值，提示用户当前的服务不可用 一般发生在客户端 Zuul路由网关，用户一进来接触的组件就是路由网关 主要功能： 路由 过滤 zuul服务最终还是会注册到Eureka中 zookeeper 网关核心功能 动态路由 灰度发布 授权认证 性能监控 系统日志 数据缓存 限流熔断 kong zuul ngnix+lua（openresty） 自研网关 大厂基本上都是基于netty做的自有网关 zuul：基于java开发，功能比较简单，但是比如灰度发布，限流，动态路由等没有这些功能 kong：依托于ngnix实现，openresty，lua实现的模块，现成的一些插件，可以直接使用 zuul一般来讲一台8核16g的zuul每秒抗1000+不成问题","link":"/posts/4079009569.html"},{"title":"Spring之IOC","text":"详细介绍Spring的IOC IOCbeans和context两个包是IOC的基础，BeanFactory接口提供了管理bean的机制，而 ApplicationContext是BeanFactory的一个子接口，它增加了AOP的整合，资源国际化，事件发布，以及应用层的context，比如WebApplicationContext。 简单点的说，BeanFactory提供了配置框架和基本功能，ApplicationContext增加了企业开发需要的特性，Spring的IOC容器一般也就是指ApplicationContext。 Beanbean的标识符必须唯一，一般情况下只有一个标识符，但可以有多个名称 在xml配置中，id, name 都是指的标识符，bean可以定义多个名称，在name属性中指定(逗号，分号或者空格分隔多个别名) 如果一个bean没有定义ID，则将会以它的simple name作为名字(首字母小写，如果多个大写字母开头，则保持原样) 1&lt;alias name=&quot;fromName&quot; alias=&quot;toName&quot;/&gt; sdlkfj 容器的启动流程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Overridepublic void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } }} 加载过程加载过程分为三个步骤 资源定位 解析DefaultBeanDefinitionDocumentReader 注册 资源定位 就如上图所示，一直是在调用父类的方法，直到ResourceLoader，这个类中有个getResource方法，可以将外部的资源，读取为Resource类。 解析注册","link":"/posts/171657660.html"},{"title":"Spring循环依赖","text":"文章摘要 参考链接： B站视频讲解","link":"/posts/2782423641.html"},{"title":"Spring注解","text":"深入理解spring注解的构成 自定义注解12public @interface zhujie {} 其实最简单注解就是这样的，直接使用@interface后面跟上你想自定义的注解的名字即可 当然了，有一些本身在注解上常用的注解，下面一一来介绍。 原生注解@target123456789101112@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target { /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to */ ElementType[] value();} elementpye 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public enum ElementType { /** Class, interface (including annotation type), or enum declaration */ //类，接口或者枚举类 TYPE, /** Field declaration (includes enum constants) */ //变量声明 FIELD, /** Method declaration */ //方法声明 METHOD, /** Formal parameter declaration */ //格式化声明 PARAMETER, /** Constructor declaration */ //构造声明 CONSTRUCTOR, /** Local variable declaration */ //本地变量声明 LOCAL_VARIABLE, /** Annotation type declaration */ //注解类型声明 ANNOTATION_TYPE, /** Package declaration */ //打包 PACKAGE, /** * Type parameter declaration * * @since 1.8 */ TYPE_PARAMETER, /** * Use of a type * * @since 1.8 */ //用户类型 TYPE_USE} @Retention12345678910@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention { /** * Returns the retention policy. * @return the retention policy */ RetentionPolicy value();} 123456789101112131415161718192021222324public enum RetentionPolicy { /** * Annotations are to be discarded by the compiler. * 批注将被编译器丢弃 */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. *注释将由编译器记录在类文件中，但VM不必在运行时保留它们。 这是默认行为。 */ // CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. *注释将由编译器记录在类文件中，并在运行时由VM保留，因此可以通过反射方式读取它们。 * @see java.lang.reflect.AnnotatedElement */ RUNTIME} @Documented12345@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Documented {}","link":"/posts/1372806157.html"},{"title":"Synchronized","text":"三种形式synchronized锁对象有三种形式，普通同步方法，静态同步方法，同步方法块 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的Class对象 对于同步方法块，锁是Synchonized括号里配置的对象 实现细节代码块同步是使用monitorenter和monitorexit指令实现的，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是，方法的同步同样可以使用这两个指令来实现 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁 Java对象头synchronized用的锁是存在Java对象头里的。如果对象是数组类型，则虚拟机用3个字宽（Word）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，1字宽等于4字节，即32bitJava对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位 在运行期间，Mark Word里存储的数据会随着锁标志位的变化而变化。MarkWord可能变化为存储以下4种数据在64位虚拟机下，Mark Word是64bit大小的 锁的升级和对比目前锁是有四种状态，无锁、偏向锁、轻量级锁、重量级锁，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率 偏向锁why为什么会出现整个锁那，是因为HotSpot 的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁 what当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程 偏向锁的撤销偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程 偏向锁的关闭偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用JVM参数来关闭延迟：-XX:BiasedLockingStartupDelay=0。如果你确定应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking=false，那么程序默认会进入轻量级锁状态 轻量级锁what线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced MarkWord。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁 自旋就是不断的去使用CAS将对象头中的Mark Word替换为指向锁记录的指针 轻量级锁解锁轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁 因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争 锁的对比 参考链接： [syn]（）","link":"/posts/4124822084.html"},{"title":"ThreadLocal","text":"主要是来介绍threadlocal的使用和注意点 使用场景在任何一个类里面想用的时候直接拿出来使用 基础认识threadlocal其实主要是使用threadlocalmap来存储数据的，key是线程id，value是对应的值，value值默认为null 主要方法set12345678public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);} get12345678910111213public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; } } return setInitialValue();} remove12345public void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);} 总结 threadlocal常用的set get remove最终都调用了expungeStaleEntry方法，这个方法可以将无用entry(脏数据)回收清理掉 回收垃圾数据的这方式和redis的过期淘汰策略有点像，过期淘汰策略中的定期删除所采用的思想与cleanSomeSlots如出一辙：都是选取一批而不是全部的Key来进行删除，以此来权衡内存占用与CPU占用之间的关系 注意事项 每次使用过threadlocal之后一定要使用remove方法来避免内存泄漏 参考文章链接： https://www.jianshu.com/p/c2cea2285f67 https://www.jianshu.com/p/9cc71c6a694a https://www.jianshu.com/p/f135c24a4114","link":"/posts/354370734.html"},{"title":"ThreadPoolExecutor","text":"从源码层面来详细了解 why为什么要使用线程池？ 线程过多会带来额外的开销，其中包括创建销毁线程的开销、调度线程的开销等等，同时也降低了计算机的整体性能。线程池维护多个线程，等待监督管理者分配可并发执行的任务。这种做法，一方面避免了处理任务时创建销毁线程开销的代价，另一方面避免了线程数量膨胀导致的过分调度问题，保证了对内核的充分利用。 what12345678910111213141516171819202122232425262728293031323334353637//ctl就是把线程的运行状态和工作线程数进行统一管理的//AtomicInteger这个类可以通过CAS达到无锁并发，效率比较高,这个变量有双重身份，它的高三位表示线程池的状态，低29位表示线程池中现有的线程数，这也是Doug Lea一个天才的设计，用最少的变量来减少锁竞争，提高并发效率。private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));///表示线程池线程数的bit数private static final int COUNT_BITS = Integer.SIZE - 3;//最大的线程数量，数量是完全够用了private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctl//获取线程池的状态private static int runStateOf(int c) { return c &amp; ~CAPACITY; }//获取线程的数量private static int workerCountOf(int c) { return c &amp; CAPACITY; }private static int ctlOf(int rs, int wc) { return rs | wc; } /* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */ private static boolean runStateLessThan(int c, int s) { return c &lt; s; } private static boolean runStateAtLeast(int c, int s) { return c &gt;= s; } private static boolean isRunning(int c) { return c &lt; SHUTDOWN; } 状态 RUNNING, 运行状态，值也是最小的，刚创建的线程池就是此状态，能接受新提交的任务，并且也能处理阻塞队列中的任务 SHUTDOWN，关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务。在线程池处于 RUNNING 状态时，调用 shutdown()方法会使线程池进入到该状态。（finalize() 方法在执行过程中也会调用shutdown()方法进入该状态） STOP，不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。在线程池处于 RUNNING 或 SHUTDOWN 状态时，调用 shutdownNow() 方法会使线程池进入到该状态； TIDYING，如果所有的任务都已终止了，workerCount (有效线程数) 为0，线程池进入该状态后会调用 terminated() 方法进入TERMINATED 状态。 TERMINATED，在terminated() 方法执行完后进入该状态，默认terminated()方法中什么也没有做。 构造方法12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);} corePoolSize：核心线程数量，当有新任务在execute()方法提交时，会执行以下判断： 如果运行的线程少于 corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的； 如果线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务； 如果设置的corePoolSize 和 maximumPoolSize相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理； 如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所指定的策略来处理任务； 所以，任务提交时，判断的顺序为 corePoolSize –&gt; workQueue –&gt; maximumPoolSize。 maximumPoolSize：最大线程数量； workQueue：等待队列，当任务提交时，如果线程池中的线程数量大于等于corePoolSize的时候，把该任务封装成一个Worker对象放入等待队列； workQueue ：保存等待执行的任务的阻塞队列，当提交一个新的任务到线程池以后, 线程池会根据当前线程池中正在运行着的线程的数量来决定对该任务的处理方式，主要有以下几种处理方式: 直接切换：这种方式常用的队列是SynchronousQueue，但现在还没有研究过该队列，这里暂时还没法介绍； 使用无界队列：一般使用基于链表的阻塞队列LinkedBlockingQueue。如果使用这种方式，那么线程池中能够创建的最大线程数就是corePoolSize，而maximumPoolSize就不会起作用了（后面也会说到）。当线程池中所有的核心线程都是RUNNING状态时，这时一个新的任务提交就会放入等待队列中。 使用有界队列 ：一般使用ArrayBlockingQueue。使用该方式可以将线程池的最大线程数量限制为maximumPoolSize，这样能够降低资源的消耗，但同时这种方式也使得线程池对线程的调度变得更困难，因为线程池和队列的容量都是有限的值，所以要想使线程池处理任务的吞吐率达到一个相对合理的范围，又想使线程调度相对简单，并且还要尽可能的降低线程池对资源的消耗，就需要合理的设置这两个数量。 如果要想降低系统资源的消耗（包括CPU的使用率，操作系统资源的消耗，上下文环境切换的开销等）, 可以设置较大的队列容量和较小的线程池容量, 但这样也会降低线程处理任务的吞吐量。 如果提交的任务经常发生阻塞，那么可以考虑通过调用 setMaximumPoolSize() 方法来重新设定线程池的容量。 如果队列的容量设置的较小，通常需要将线程池的容量设置大一点，这样CPU的使用率会相对的高一些。但如果线程池的容量设置的过大，则在提交的任务数量太多的情况下，并发量会增加，那么线程之间的调度就是一个要考虑的问题，因为这样反而有可能降低处理任务的吞吐量。 keepAliveTime：线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime； threadFactory：它是ThreadFactory类型的变量，用来创建新线程。默认使用Executors.defaultThreadFactory() 来创建线程。使用默认的ThreadFactory来创建线程时，会使新创建的线程具有相同的NORM_PRIORITY优先级并且是非守护线程，同时也设置了线程的名称。 handler ：它是RejectedExecutionHandler类型的变量，表示线程池的饱和策略。如果阻塞队列满了并且没有空闲的线程，这时如果继续提交任务，就需要采取一种策略处理该任务。线程池提供了4种策略： AbortPolicy：直接抛出异常，这是默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy：直接丢弃任务； 12345678910111213141516171819202122232425//上面的this调用的就是这个方法，这个方法中会进行一些异常情况的判断public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;} execute方法1234567891011121314151617181920212223public void execute(Runnable command) { if (command == null) throw new NullPointerException(); //ctl的低29位表示线程数，高三位表示线程的状态 int c = ctl.get(); //如果正在工作的线程数小于核心线程数，就需要增加一个线程 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; //如果增加线程失败，就会重新获取ctl c = ctl.get(); } if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) reject(command);} 参考文章 https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html http://www.ideabuffer.cn/2017/04/04/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%EF%BC%9AThreadPoolExecutor/****","link":"/posts/2504375410.html"},{"title":"b树和b+树的区别","text":"B树是为了提高磁盘或外部存储设备查找效率而产生的一种多路平衡查找树。 B+树为B树的变形结构，用于大多数数据库或文件系统的存储而设计。 B树其实又被称为B-树 B树相对于红黑树的区别在大规模数据存储的时候，红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况。为什么会出现这样的情况，我们知道要获取磁盘上数据，必须先通过磁盘移动臂移动到数据所在的柱面，然后找到指定盘面，接着旋转盘面找到数据所在的磁道，最后对数据进行读写。磁盘IO代价主要花费在查找所需的柱面上，树的深度过大会造成磁盘IO频繁读写。根据磁盘查找存取的次数往往由树的高度所决定，所以，只要我们通过某种较好的树结构减少树的结构尽量减少树的高度，B树可以有多个子女，从几十到上千，可以降低树的高度。 B树和B+树的区别 B树则所有节点都带有带有指向记录（数据）的指针（ROWID），B+树中只有叶子节点会带有指向记录(数据)的指针（ROWID）。因为B+树它把所有的卫星数据(或指向数据的指针)都存储在叶节点中，内部节点只存放关键字和孩子指针，不会带上指向记录的指针（ROWID），这样，一个块中可以容纳更多的索引项，一是可以降低树的高度。二是一个内部节点可以定位更多的叶子节点(优点1)。 B+树中每个叶子节点都包含指向下一个叶子节点的指针。所有叶子节点都是通过指针连接在一起，而B树不会。 叶子节点之间通过指针来连接，范围扫描将十分简单(优点2)，而对于B树来说，则需要在叶子节点和内部节点不停的往返移动。 B+树还有一个最大的好处，遍历更加高效，方便扫库（优点2），B树必须用中序遍历的方法按序扫库，而B+树直接从叶子结点挨个扫一遍就完了，B+树支持range-query非常方便，而B树不支持。这是数据库选用B+树的最主要原因。（B+树的遍历更加高效，B树需要以中序的方式遍历节点，而B+树只需把所有叶子节点串成链表就可以从头到尾遍历）。 B+树每个节点的指针和key一样多，B树每个节点指针比key多1。 为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？ B+的磁盘读写代价更低 B+的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。 B+tree的查询效率更加稳定 由于非叶子结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 数据库索引采用B+树的主要原因是 B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低） B+树的优点 非叶子节点不会带上指向记录的指针（ROWID），这样，一个块中可以容纳更多的索引项，一是可以降低树的高度。二是一个内部节点可以定位更多的叶子节点。 叶子节点之间通过指针来连接，范围扫描将十分简单，而对于B树来说，则需要在叶子节点和内部节点不停的往返移动。 B树的优点：对于在内部节点的数据，可直接得到，不必根据叶子节点来定位。 应用mysql采用B+树来作为索引的数据结构 MongoDB采用的是B树","link":"/posts/1580086704.html"},{"title":"git","text":"学习链接 链接2 官方地址 git在线练习网站 原理 常用命令特殊符号^一个^代表往前一个commit 比如git rebase HEAD^ ，就是当前的commit往前一个commit git rebase HEAD^^ ，就是当前的commit往前两个commit ~~5 就是当前的commit往前五个 HEADhead就是指向当前的commit pullgit pull 将数据从远程仓库拉取到本地仓库 pull的实质是先把代码从远程仓库拉取下来，然后merge pushgit push 将本地仓库的数据推送到远程仓库中 branch123456#branch 就是分支的意思#当前分支切换到a分支git branch a #f是强制的意思，强制将master强制指向head的父节点git branch -f master HEAD~1 commitgit commit —amend会在分支上真的提交，是一个新的commit，而不是将刚才的commit替换掉，不会操作原commit 只有最新的commit出错才可以amend，其他的要用rebase checkout作用：切换分支 1git checkout -b newBranchName addadd的不是整个文件，是改动的内容 loggit log 查看日志git log -p 查看详细日志详细地查看每一行都修改了什么 git log –stat 查看简要统计查看的是大概的修改内容 showgit show 查看当前的commitmergemerge其实就是合并的意思，比如a和b都从远端仓库拉取代码下来，然后修改不同的文件或者同一文件的不同内容，那么就会将不同的地方直接合并，但是如果修改的是同一个文件的相同内容，那么当b或者a当对方先提交的时候，就会发生冲突conflict ，冲突的原因是，你现在的代码和远端仓库的代码不一样，需要保持一样才可以进行合并 12#当前分支和branchName分支合并git merge branchName diff使用 git diff （不加选项参数）可以显示工作目录和暂存区之间的不同。换句话说，这条指令可以让你看到「如果你现在把所有文件都 add，你会向暂存区中增加哪些内容」 git diff –staged 比对暂存区和上一条的提交换句话说，这条指令可以让你看到「如果你立即输入 git commit，你将会提交什么」： –cached 和–staged一模一样，可以直接替换使用 git diff headrebasegit rebase -i HEAD^reset本质移动head以及它所指向的branch git reset不加参数，默认是—mixed 工作目录保留，清空暂存区 12#回退三个版本 包括当前版本git rebase -i HEAD~3 上图执行的命令是git reset -i HEAD^3 当前分支为main，结点为c5 向上回退三个版本就是c3，所以c3到c5整体移动，然后通过可视化界面来调整c5和c4的顺序，最后就呈现出来的效果如上图所示 git reset —hard HEAD^刚才提交的commit觉得太烂了，不想修改了，直接放弃，就可以使用这个，退回到上一个commit就可以了 hard的意思是重置工作目录，意思就是直接将head新指向的commit的内容全部放置到当前目录，就是你之前没有提交的代码都被覆盖掉了 git reset —soft HEAD^切换到上一级的commit，将当前工作目录的改动保存到暂存区中，这也是与har的最大的区别 revert作用：撤销上一步的提交，并且将其推动到远程仓库 cherry-pick作用：将其他分支节点直接复制到当前分支的当前节点 reflog查看 Git 仓库中的引用的移动记录。如果不指定引用，它会显示 HEAD 的移动记录 当你误删一个branch的时候，看下branch所对应的 git reflog master 显示master的移动记录","link":"/posts/1368285564.html"},{"title":"hashCode和equals关系","text":"详细对比hashCode和equals hashCode()和equals()是什么？hashCode()方法和equals()方法的作用其实一样，在Java里都是用来对比两个对象是否相等一致。 hashCode()和equals()区别是什么？性能就性能来说，肯定是hashcode更快一下，毕竟只需要比较hashcode值就好 equals的话，比如String类型，如果是字符串比较大的话，比较起来就比较慢，如果字符串比较小的话，就比较快 可靠性 equals()相等的两个对象他们的hashCode()肯定相等，也就是用equals()对比是绝对可靠的。 hashCode()相等的两个对象他们的equals()不一定相等，也就是hashCode()不是绝对可靠的。 为何重写equals也要重写hashcode如果你重写了equals，比如说是基于对象的内容实现的，而保留hashCode的实现不变，那么很可能某两个对象明明是“相等”，而hashCode却不一样。 这样，当你用其中的一个作为键保存到hashMap、hasoTable或hashSet中，再以“相等的”找另一个作为键值去查找他们的时候，则根本找不到。 为什么equals()相等，hashCode就一定要相等，而hashCode相等，却不要求equals相等? 因为是按照hashCode来访问小内存块，所以hashCode必须相等。 HashMap获取一个对象是比较key的hashCode相等和equals为true。 之所以hashCode相等，却可以equal不等，就比如ObjectA和ObjectB他们都有属性name，那么hashCode都以name计算，所以hashCode一样，但是两个对象属于不同类型，所以equals为false。 阿里相关约束 只要重写 equals，就必须重写 hashCode； 因为 Set 存储的是不重复的对象，依据 hashCode 和 equals 进行判断，所以 Set 存储的对象必须重写这两个方法； 如果自定义对象做为 Map 的键，那么必须重写 hashCode 和 equals； String 重写了 hashCode 和 equals 方法，所以我们可以非常愉快地使用 String 对象作为 key 来使用； 参考链接： https://zhuanlan.zhihu.com/p/58337357","link":"/posts/4257409245.html"},{"title":"hashmap","text":"java八股文之hashmap 基本认识12345678910111213//默认初始容量 - 必须是2的幂次方static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; //hashmap的最大容量 2的30次方static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//负载因数static final float DEFAULT_LOAD_FACTOR = 0.75f;//链表数量达到八开始向红黑树转换static final int TREEIFY_THRESHOLD = 8;//当红黑树的节点少于6时，则转换为单链表存储static final int UNTREEIFY_THRESHOLD = 6;//虽然在hash冲突发生的时候，默认使用单链表存储，当单链表节点个数大于8时，会转换为红黑树存储//但是有一个前提（很多文章都没说）：要求数组长度大于64，否则不会进行转换，而是进行扩容。static final int MIN_TREEIFY_CAPACITY = 64; 最大容量为什么是不超过1&lt;&lt;30？int类型的数据所占空间大小为32位，所以如果超过这个范围之后，会出现溢出。所以，1&lt;&lt;30是在int类型取值范围中2次幂的最大值，即为HashMap的容量最大值。 为什么要将链表中转红黑树的阈值设为8？HashMap不直接使用红黑树，是因为树节点所占空间是普通节点的两倍，所以只有当节点足够的时候，才会使用树节点。也就是说，尽管时间复杂度上，红黑树比链表好一点，但是红黑树所占的空间比较大，所以综合考虑之下，只有在链表节点数太多的时候，红黑树占空间大这一劣势不太明显的时候，才会舍弃链表，使用红黑树。 所以说阈值设置为8是一个将内存和性能折中的一个方案 1.8和1.7的区别 hashmap1.8之后，结构为 数组+链表+红黑树，而1.7只是数据+链表 1.7扩容时需要重新计算哈希值和索引位置，1.8并不重新计算哈希值，巧妙地采用和扩容后容量进行&amp;操作来计算新的索引位置。 1.7插入元素到单链表中采用头插入法，1.8采用的是尾插入法。 解决hash冲突的方法： 开发定址法：所谓开放定址法，即是由关键码得到的哈希地址一旦产生了冲突，也就是说，该地址已经存放了数据元素，就去寻找下一个空的哈希地址，只要哈希表足够大，空的哈希地址总能找到，并将数据元素存入。 链地址法：hash值一样的key，会在这个数据后面新建一个链表，每次增加就直接在链表后面加节点即可 链表为什么用尾插法为了安全，因为头插法多线程情况下会导致链表成环 链表成环视频讲解 那么为什么到了1.8版本才进行修改那？ 其实是因为多线程下的hashmap本就不安全，在多线程场景下如果要使用map，也不会使用hashmap，因此等开发人员想到了，才进行修改 为什么扩容时两倍扩容文章 主要是与扩容时候的额resize方法有关，resize方法中(n - 1) &amp; hash的计算方法有关 其中n是集合的容量，hash是添加的元素进过hash函数计算出来的hash值。 容量是2的n次幂，主要原因是可以使得添加的元素均匀分布在HashMap中的数组上，减少hash碰撞，避免形成链表的结构，使得查询效率降低！ 方法hashhashcodehashcode就是通过hash函数得来的，通俗的说，就是通过某一种算法得到的，hashcode就是在hash表中有对应的位置。 通过对象的内部地址(也就是物理地址)转换成一个整数，然后该整数通过hash函数的算法就得到了hashcode(不同jvm的实现不同, hotspot的实现贴在了最后)，所以，hashcode是什么呢？就是在hash表中对应的位置。这里如果还不是很清楚的话，举个例子，hash表中有 hashcode为1、hashcode为2、(…)3、4、5、6、7、8这样八个位置，有一个对象A，A的物理地址转换为一个整数17(这是假如)，就通过直接取余算法，17%8=1，那么A的hashcode就为1，且A就在hash表中1的位置。 为什么使用hashcode很简单，因为hashcode快啊。 HashCode是用来在散列存储结构中确定对象的存储地址的 HashMap 之所以速度快，因为他使用的是散列表，根据 key 的 hashcode 值生成数组下标（通过内存地址直接查找，不需要判断, 但是需要多出很多内存，相当于以空间换时间） 代码实现1234static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 1public native int hashCode(); h是调用c写的hashcode的方法获取到的哈希值 h与h向右移动16位进行异或 异或的规则是： 相同为0，不同为1 hashmap12345678910111213141516171819202122232425262728293031public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);}/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR);}/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted} hashmap的构造方法，三种初始化hashmap的形式 resize12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485 /** * The next size value at which to resize (capacity * load factor). *下一个要调整大小的大小值（容量负载因子） * @serial */ int threshold;final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //如果超过最大的容量则不允许扩容，直接返回原数组 if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } //如果hashmap数量的两倍小于2的32次方并且 大于等于16 //那么新的hashmap的大小就是原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; //判断e是不是最后一个节点 if (e.next == null) //重新hash之后放入对应的位置 newTab[e.hash &amp; (newCap - 1)] = e; //判断这个节点是不是在红黑树中 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else { Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab;} hashmap的key只能有一个为null，value可以有多个null HashTable中，无论是key还是value，都不能为null put123public V put(K key, V value) { return putVal(hash(key), key, value, false, true);} putVal1234567891011121314151617181920212223242526272829303132333435363738394041424344454647final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果table数组中没有值，就要初始化一个长度 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果数组的这个下标位置没有数据，就将这个数据插入进去 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果检测到是树节点，就要使用下面这个方法来进行插入元素 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); //如果链表的数量大于8，就转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } //记录修改的次数 ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;} treeifyBin12345678910111213141516171819202122final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; //虽然上面判断链表长度大于8,到了操作树的环节，但是还是判断数据长度大于64才可以将 //链表转换为红黑树 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); }} 遍历方式lambda123456Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();map.put(2, 10);map.put(1, 20);map.put(4,40);map.put(3,30);map.forEach((k, v) -&gt; System.out.println(&quot;key: &quot; + k + &quot; value:&quot; + v)); for each123for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) { System.out.println(&quot;Key = &quot; + entry.getKey() + &quot;, Value = &quot; + entry.getValue()); } 迭代键值对123456789// 迭代键 for (Integer key : map.keySet()) { System.out.println(&quot;Key = &quot; + key); } // 迭代值 for (Integer value : map.values()) { System.out.println(&quot;Value = &quot; + value); } Iterator12345Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; entries = map.entrySet().iterator(); while (entries.hasNext()) { Map.Entry&lt;Integer, Integer&gt; entry = entries.next(); System.out.println(&quot;Key = &quot; + entry.getKey() + &quot;, Value = &quot; + entry.getValue()); } 原理虽然hashmap的插入数据是无序的，但是它遍历出来的结果都是有序的,并且每次遍历的结果都一样 原因就是因为hashmap的数组下标是 hashcode和hashmap的容量大小 按位与出来的结果 线程安全hashmap是线程不安全的，如果需要保证线程安全，推荐使用ConcurrentHashMap 参考文章美团技术文章 知乎文章","link":"/posts/3398514521.html"},{"title":"icarus Theme use","text":"文章摘要自己在md文件中随意一处加入 1&lt;!-- more --&gt; 就可以将这个语句之上的作为摘要了","link":"/posts/3750237992.html"},{"title":"idea使用技巧","text":"常用快捷键 psvm 主函数 sout 输出 f7 可以进入被嵌套方法的内部 f8 不可以进入被嵌套方法的内部 f9 可以直接跳到下一个断点 Alt+insert 出现问题的解决方案 ctrl+o 构造方法 ent+shift 光标跳到新的一行 ctrf+f 搜索 shift+f10 run shift+f9 debug run ctrl+alt+L 格式化代码 shift + F6 批量修改对象 ctrl+alt+t 环绕方法（try） alt+8 出现服务窗口 ctrl+alt+左键 返回上一个查看代码的位置 ctrl+g 跳到指定行数","link":"/posts/2692604028.html"},{"title":"java_String","text":"string类型的介绍 string不可变性123456789//由于是final，所以string是不可继承的public final class String//final修饰的char[]代表了被存储的数据不可更改性//是final和privateprivate final char value[];//下面这个例子说明，是final和private一起来使得String的数据是不可改变的final int[] array={1,2,3,4};array[2]=100;System.out.println(array[2]); 原因 只有String是不可改变的，字符串池才有可能实现 如果字符串是可变的，会引发线程安全的问题 长度限制由于stirng其实就是一个char数组 char数组的下标是整型，integer https://segmentfault.com/a/1190000020381075 三种常量池区分全局常量池全局字符串池里的内容是在类加载完成，经过验证，准备阶段之后在堆中生成字符串对象实例，然后将该字符串对象实例的引用值存到string pool中（记住：string pool中存的是引用值而不是具体的实例对象，具体的实例对象是在堆中开辟的一块空间存放的。）。 在HotSpot VM里实现的string pool功能的是一个StringTable类，它是一个哈希表，里面存的是驻留字符串(也就是我们常说的用双引号括起来的)的引用（而不是驻留字符串实例本身），也就是说在堆中的某些字符串实例被这个StringTable引用之后就等同被赋予了”驻留字符串”的身份。这个StringTable在每个HotSpot VM的实例只有一份，被所有的类共享。 class文件常量池我们都知道，class文件中除了包含类的版本、字段、方法、接口等描述信息外，还有一项信息就是常量池(constant pool table)，用于存放编译器生成的**各种字面量(Literal)和符号引用(Symbolic References)**。 字面量就是我们所说的常量概念，如文本字符串、被声明为final的常量值等。 符号引用是一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可（它与直接引用区分一下，直接引用一般是指向方法区的本地指针，相对偏移量或是一个能间接定位到目标的句柄）。一般包括下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 运行时常量池当java文件被编译成class文件之后，也就是会生成我上面所说的class常量池，那么运行时常量池又是什么时候产生的呢？ jvm在执行某个类的时候，必须经过加载、连接、初始化，而连接又包括验证、准备、解析三个阶段。而当类加载到内存中后，jvm就会将class常量池中的内容存放到运行时常量池中，由此可知，运行时常量池也是每个类都有一个。在上面我也说了，class常量池中存的是字面量和符号引用，也就是说他们存的并不是对象的实例，而是对象的符号引用值。而经过解析（resolve）之后，也就是把符号引用替换为直接引用，解析的过程会去查询全局字符串池，也就是我们上面所说的StringTable，以保证运行时常量池所引用的字符串与全局字符串池中所引用的是一致的。 总结1.全局常量池在每个VM中只有一份，存放的是字符串常量的引用值。 2.class常量池是在编译的时候每个class都有的，在编译阶段，存放的是常量的符号引用。 3.运行时常量池是在类加载完成之后，将每个class常量池中的符号引用值转存到运行时常量池中，也就是说，每个class都有一个运行时常量池，类在解析之后，将符号引用替换成直接引用，与全局常量池中的引用值保持一致。 substringhttps://www.hollischuang.com/archives/1232 jdk6和jdk7之后的差别 jdk6jdk6的时候，当截取字符串的时候，会在堆中new 一个新的string对象，但是这个string对象使用的char数组还是之前的数组，如果你只是在很长的字符串中引用了很小的一块数据，但是由于这个char数组是有引用的，所以无法进行垃圾回收，但是由于你所使用的字符串只是很小的一部分，但是你却用了这么大的char数组，会导致好像那么一大空间不存在似的，这就产生了内存泄露的问题。 内存泄露：在计算机科学中，内存泄漏指由于疏忽或错误造成程序未能释放已经不再使用的内存。 内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，导致在释放该段内存之前就失去了对该段内存的控制，从而造成了内存的浪费。 jdk7既然已经知道了上述问题所在，那么只需要new一个新的string的时候，让这个string指向自己的包含的char数组即可 1234567891011121314151617181920 public String(char value[], int offset, int count) { if (offset &lt; 0) { throw new StringIndexOutOfBoundsException(offset); } if (count &lt;= 0) { if (count &lt; 0) { throw new StringIndexOutOfBoundsException(count); } if (offset &lt;= value.length) { this.value = &quot;&quot;.value; return; } } // Note: offset or count might be near -1&gt;&gt;&gt;1. if (offset &gt; value.length - count) { throw new StringIndexOutOfBoundsException(offset + count); } //就是在这里将char数组拷贝过来，截取的offset和截取字符串的值是一样的 this.value = Arrays.copyOfRange(value, offset, offset+count);} replaceFirst、replaceAll、replace区别12345678910public String replaceFirst(String regex, String replacement) { return Pattern.compile(regex).matcher(this).replaceFirst(replacement);} public String replaceAll(String regex, String replacement) { return Pattern.compile(regex).matcher(this).replaceAll(replacement);} public String replace(CharSequence target, CharSequence replacement) { return Pattern.compile(target.toString(), Pattern.LITERAL).matcher( this).replaceAll(Matcher.quoteReplacement(replacement.toString()));} replaceFirst的作用是让regex去replacement替换原有string的第一个字符 replaceAll的作用是replacement替换regex replace的作用是将原有字符串的所有target替换为repalcement String、StringBuilder和StingBuffer之间的区别与联系StringBuilder123456789101112131415161718192021public StringBuilder() { super(16);}public StringBuilder(int capacity) { super(capacity);} public StringBuilder(String str) { super(str.length() + 16); append(str);} @Overridepublic StringBuilder append(Object obj) { return append(String.valueOf(obj));}@Overridepublic StringBuilder append(String str) { super.append(str); return this;} 从源码可以看出，默认的stringbuilder是16个字节，如果指定了大小就用指定的大小，如果直接给了个参数就是字符串的长度加上16个字节 方法中没有synchronized，所以这是线程不安全的 StingBuffer123456789101112131415@Overridepublic synchronized int length() { return count;}@Overridepublic synchronized int capacity() { return value.length;}@Overridepublic synchronized void ensureCapacity(int minimumCapacity) { super.ensureCapacity(minimumCapacity);} 方法都是synchronized修饰的，所以是线程安全的 String对“+”的重载https://juejin.im/post/6844903960608784392 123456789package com.test;public class demo { public static void main(String[] args) { String a = &quot;1&quot;; String b = &quot;2&quot;; System.out.println(a+b); }} 从反编译的过程可以看出，是调用了stringbuilder的append方法， String.valueOf和Integer.toString的区别stirng 12345678 //stirng不能为null，为null会报NullPointerException public String toString() { return this;}//这个方法的功能就是如果形参是null，那么返回null字符串，而不是直接报NullPointerException异常public static String valueOf(Object obj) { return (obj == null) ? &quot;null&quot; : obj.toString();} integer 1234567891011121314151617181920212223242526272829public static String toString(int i, int radix) { if (radix &lt; Character.MIN_RADIX || radix &gt; Character.MAX_RADIX) radix = 10; /* Use the faster version */ if (radix == 10) { return toString(i); } char buf[] = new char[33]; boolean negative = (i &lt; 0); int charPos = 32; if (!negative) { i = -i; } while (i &lt;= -radix) { buf[charPos--] = digits[-(i % radix)]; i = i / radix; } buf[charPos] = digits[-i]; if (negative) { buf[--charPos] = '-'; } return new String(buf, charPos, (33 - charPos));} switch对String的支持1234567891011121314151617181920package com.test;public class demo { public static void main(String[] args) { String str = &quot;world&quot;; switch (str) { case &quot;hello&quot;: System.out.println(&quot;hello&quot;); break; case &quot;world&quot;: System.out.println(&quot;world&quot;); break; default: break; } }} 反编译 由反编译的代码可以看出来，switch中的case是通过hashCode来进行匹配的，使用equals方法来进行值的比较 字符串池intern美团对于这个关键字的分析 String比较特别的地方 直接中双引号引起来的，是放在常量池中，如代码所示String s2=”1”; 如果不是用双引号声明的String对象，可以使用String提供的intern方法。intern 方法会从字符串常量池中查询当前字符串是否存在，若不存在就会将当前字符串放入常量池中 实际例子验证： 123456789101112package com.test;public class demo02 { public static void main(String[] args) { String s=&quot;1&quot;; String s1=new String(&quot;1&quot;); String s2=s.intern(); System.out.println(s==s1); System.out.println(s1==s2); System.out.println(s2==s); }} 运行结果：false false true jdk7之后，intern方法对 intern 操作和常量池都做了一定的修改。主要包括2点： 将String常量池 从 Perm 区移动到了 Java Heap区 String#intern 方法时，如果存在堆中的对象，会直接保存对象的引用，而不会重新创建对象。 由上图代码及运行结果可以看出，双引号的的String是直接在常量池中的，而new出来的对象是在堆中的，当s2调用intern的方法之后，会去常量池中查找是否有这变量，如果有的话会直接引用常量池中的对象 转换为String的三种方式 (string) toString String.valueOf() 三种方式的区别 推荐使用 String.valueOf()的方法，这个方法可以避免强转时候对象不能转换为String的错误，也可以避免toString的时候对象为null的情况","link":"/posts/2238481670.html"},{"title":"java的构造函数","text":"主要介绍java的构造函数 定义首先我们肯定要知道什么是构造函数？ 简单来说，就是与类名相同，无返回类型的方法，参数可以为空，一般称之为无参构造，也可以有参数，一般称之为有参构造。 实例化比如说有个test类，你想要实例化它，一般都是new test（）； 这种new 一个类的名称，后面加括号的就是调用的是这个类的无参构造，当然里面如果有参数，调用的就是有参构造了。 注意编译报错在一个类中，当没有在子类里加super（），也没有在父类里构造无参函数 编译器看到我们在父类里写了有参构造法方法，它就会认为，我们不想让子类在回溯的时候走默认的无参构造方法这条路【super(); 此路不通】，但是我们又没告诉给子类，它应该走哪条路（你给了有参就是告诉我不要走无参，但也没告诉我走有参，那走哪？），所以会报错。 所以，若父类没有提供无参构造方法，但是提供了有参构造方法，就要在子类的无参构造方法里，显示的加上调用父类的有参构造方法语句。 如：super（）；（这里就告诉了去走有参了） 总结 无参构造运行是需要往上一级一级找无参，甚至找到object 如果其中一级只有有参，没有无参，会发生错误。但只有无参，没有有参一定不会出错。 在以后的Java开发中，若一个类要有带参构造方法，要记得先写无参构造方法。防止类追溯时找不到上级，报错。","link":"/posts/3513846333.html"},{"title":"java继承","text":"彻底搞懂java的父类与子类的关系 缘由java出现继承的原因其实很简单，就是为了代码复用，减少编写很多无用的代码 单继承众所周知，java是单继承的，为什么那？ 主要是因为如果a继承了b和c的方法，但是b和c中有同名的方法，那就无法确定到底是调用的是哪个父类的方法 注意事项 除了object类，一个类只有一个父类，并且在没有明确声明继承与哪个类的时候，默认继承object类 子类无法访问父类的private字段或者private方法 子类不会继承任何父类的构造方法。子类默认的构造方法是编译器自动生成的，不是继承的。 如果父类没有默认的构造方法，子类就必须显式调用super()并给出参数以便让编译器定位到父类的一个合适的构造方法，否则就会报错 正常情况下，只要某个class没有final修饰符，那么任何类都可以从该class继承。 推荐使用向上转型，使用向下转型的时候，可能会报错，推荐使用instanceof 判断之后再进行向下转型 java中静态属性和静态方法可以被继承，但是没有被重写(overwrite)而是被隐藏。 初始化顺序 父类静态成员和静态初始化快，按在代码中出现的顺序依次执行。 子类静态成员和静态初始化块，按在代码中出现的顺序依次执行。 父类的实例成员和实例初始化块，按在代码中出现的顺序依次执行。 执行父类的构造方法。 子类实例成员和实例初始化块，按在代码中出现的顺序依次执行。 执行子类的构造方法。 测试12345678910111213141516171819202122232425262728293031package test;public class demo02 { public static void main(String[] args) { Father a = new Father(); Chilren b = new Chilren(); Father c = new Chilren(); a.getAge(); System.out.println(a.age); b.getAge(); System.out.println(b.age); c.getAge(); System.out.println(c.age); }}class Father { int age = 40; public void getAge() { System.out.println(age); }}class Chilren extends Father { int age = 18; public void getAge() { System.out.println(age); }} 上面这段代码的输出结果是： 40 40 18 18 18 40 重点看倒数第二个结果，也就是 Father c = new Chilren(); 从上面程序的运行结果可以看出 访问变量看声明，访问方法看实际对象类型（new出来的类型），也就是说如果你使用的声明是new出来的这个类的父类，那么这个实例对象的变量使用的是父类的变量，方法是使用的子类的方法，如果要是使用父类的方法需要使用super关键字 在实例化一个子类的同时，系统会给子类所有实例变量分配内存，也会给他的父类的实例变量分配内存，及时父子类中存在重名的实例变量，也会两个都分配内存的，这个时候子类只是隐藏了父类的这个变量，但还是会给它分配内存，然后可以用super来访问属于父类的变量。 this：是一个真真实实对象，代表的就是当前对象，可以用 return this; 去返回一个对象。 super：不能一个对象，不是指向父类对象的意思，super只是修饰了他后边的内容，告诉JVM，后面这部分内容不是当前对象所属类的内容而已，若用return super，JVM是不允许的，是一种错误的语法.","link":"/posts/2310251744.html"},{"title":"java面试题","text":"常见的java面试题总结 八大基础类型整形： byte short int long 浮点型： float double 布尔型： boolen 字符型： char 访问修饰符区别 类型转换float f=3.4;是否正确？答:不正确。3.4是双精度数，将双精度型（double）赋值给浮点型（float）属于下转型（down-casting，也称为窄化）会造成精度损失，因此需要强制类型转换float f =(float)3.4; 或者写成float f =3.4F;。 short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗？对于short s1 = 1; s1 = s1 + 1;由于1是int类型，因此s1+1运算结果也是int 型，需要强制转换类型才能赋值给short型。而short s1 = 1; s1 += 1;可以正确编译，因为s1+= 1;相当于s1 = (short)(s1 + 1);其中有隐含的强制类型转换。","link":"/posts/345102264.html"},{"title":"kafka-1","text":"主要介绍kafka的基本知识 架构 组成 Producer：生产者，也就是发送消息的一方。生产者负责创建消息，然后将其发送到 Kafka。 Consumer：消费者，也就是接受消息的一方。消费者连接到 Kafka 上并接收消息，进而进行相应的业务逻辑处理。 Consumer Group：一个消费者组可以包含一个或多个消费者。使用多分区 + 多消费者方式可以极大提高数据下游的处理速度，同一消费组中的消费者不会重复消费消息，同样的，不同消费组中的消费者消息消息时互不影响。Kafka 就是通过消费组的方式来实现消息 P2P 模式和广播模式。 Broker：服务代理节点。Broker 是 Kafka 的服务节点，即 Kafka 的服务器。 Topic：Kafka 中的消息以 Topic 为单位进行划分，生产者将消息发送到特定的 Topic，而消费者负责订阅 Topic 的消息并进行消费。 Partition：Topic 是一个逻辑的概念，它可以细分为多个分区，每个分区只属于单个主题。同一个主题下不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）。 Offset：offset 是消息在分区中的唯一标识，Kafka 通过它来保证消息在分区内的顺序性，不过 offset 并不跨越分区，也就是说，Kafka 保证的是分区有序性而不是主题有序性。 Replication：副本，是 Kafka 保证数据高可用的方式，Kafka 同一 Partition 的数据可以在多 Broker 上存在多个副本，通常只有主副本对外提供读写服务，当主副本所在 broker 崩溃或发生网络一场，Kafka 会在 Controller 的管理下会重新选择新的 Leader 副本对外提供读写服务。 Record：实际写入 Kafka 中并可以被读取的消息记录。每个 record 包含了 key、value 和 timestamp。","link":"/posts/364974771.html"},{"title":"linux结构","text":"基本结构 VFS层就是看你对哪个目录中的文件执行的磁盘io操作，把io请求交给具体的文件系统 Page Cache在这个page cache 基于内存的缓存里找你要的数据在不在里面，如果有就基于内存缓存来执行读写，如果没有就继续往下一层走，此时这个请求会交给通用的block层，在这一层会把你对文件的io请求转换为Block IO请求 IO调度层这一层里默认的是用CFQ公平调度算法，也就是说，可能有两个sql请求同时过来，一个比较简单，只是更新磁盘的一个block的数据就可以了，另外一个是读取一个表中的所有数据，如果基于这个默认的CFQ算法，就会让读全表的这个sql先执行，更新一个条数据的sql等到读全表的sql全部执行结束之后才能执行。这样会导致明明应该先执行简单地操作，结果复杂的反而先执行了，不符合我们的预期 于是，在mysql的生产环境，建议采用deadline iO调用算法。它的一个核心思想就是，任何一个IO操作都能不能一直不停的等待，在指定范围内，都必须让他去执行 Block 设备驱动层Io调度层决定了哪个io请求先执行，哪个io请求后执行，此时可以执行的io请求就会交给Block设备驱动层，经过驱动把iO请求发送给真正的存储硬件，也就是Block设备层 RAID第一种、项目的数据读写十分频繁，然后对可靠性要求很高，那这时候毫无疑问肯定选择RAID10了，但是付出的代价也就高了，不过一般这么选择的用户眼都不会眨一下，类似银行，你懂的。 第二种、项目的数据读十分频繁，写则较少一些，然后可靠性有一定要求但不是很高，那么可以选择RAID5，这应该是一种存储性能、数据安全和存储成本兼顾的存储解决方案了，也可以理解为是RAID 0和RAID 1的折中方案。 第三种、项目的读写都十分频繁，但是可靠性要求不高，主要用于内部这种，可以选择RAID0。 充放电服务器使用功能多快磁盘组成的RAID阵列的时候，一般会有一个RAID卡，这个RAID卡是带有一个缓存，这个缓存不是直接用我们的服务器的那种模式，他是一种跟内存类似的SDRAM，当然，你大致就认为它也是基于内存来存储的吧 然后我们可以把RAID的缓存模式设置为write back，这样 的话，所有写入到磁盘阵列的数据，都先会缓存在RAID卡的缓存里，后续慢慢再写入到磁盘阵列里去，这种写缓冲机制，可以大幅度提升我们的数据库磁盘写的性能 有了缓存我们很容易就会想到一个问题，就是比如数据写到了RAID卡的缓存中，机器突然宕机了，那么数据不就丢失了吗？其实正是因为如此，RAID卡一般都有配置自己独立的锂电池或者电容，如果服务器突然掉电了，无法接通电源了，RAID卡自己是基于锂电池供电运行的，然后它会赶紧把缓存里面的数据写入到阵列的磁盘上去 但是锂电池是存在性能衰减的问题的，所以一般来说锂电池都是要配置定时充放电的，也就是说每隔30天-90天（不同的锂电池厂商是不一样的），就会自动对锂电池进行放电一次，这样可以延长锂电池的寿命和校验电池容量。 如果不这样做，可能锂电池中存储的电量不够，在机器突然掉电的情况，锂电池中的电量不足以支撑将缓存中的数据全部刷新到磁盘上，就还是会造成数据丢失的现象 所以在锂电池充放电的过程中，RAID的缓存界别会从wirte back变成 write through，我们通过RAID写数据的时候，IO就直接写磁盘了，如果写内存和写磁盘相比的话，写内存的效率会比写磁盘的效率高上数十倍 生产建议：开启RAID卡的缓存，需要对锂电池的自动充放电的问题进行思考，防止由于锂电池定时的充放电造成系统的数据库每隔一段时间就会造成性能几十倍的抖动","link":"/posts/3640054253.html"},{"title":"mysql事务","text":"事务 常见问题脏写现在有两个事务A和B，A和B同时在修改一个数据，A先更新，B再更新，但是B回滚了，那么A就是没有写进去，这个数据还是原来的值，这样的情况 称之为脏写 脏读有两个事务A和B，A读取了B已经修改了，但是还没有提交事务的数据，之后B事务回滚了，就会造成A读取的数据和数据库中存储的数据不一样，这样的情况 称之为脏读 不可重复读就是每次读取的数据，都和上次读取的数据不一样 比如一个事务A已经开始事务，读取到了一个数据的值，然后同时一个事务B修改了这数据并且事务提交，那么事务A再次过来查询，就是发现刚才的数据值发生变化了，变成数据B了，如果此时，事务A还没有提交，又过来一个事务C修改了这个数据并且提交，那么事务A再次来查询这个数据就变成了数据C，这种 在一个事务多次查询同一个值，但是查询结果每次都有变化的现象被 称为 不可重复读。 不可重复默认就避免了脏读的问题，因为只有在别的事务已经提交之后，当前在查询的事务才能看到新的值 幻读当一个事务，用一个一样的sql进行多次查询之后，结果每次都会看到比上次多一些数据，这种现象被称为 幻读 长事务尽量不要使用长事务长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。 查询尝事务1select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 级别未提交read uncommitted 这个级别的事务，是不允许发生脏写的，也就是说不可能两个数据在没有提交的情况下去更新同一行数据的值，但是在这种隔离的级别下，可能发生脏读，不可重复读，幻读 读提交（RC）read commited 很明显，意思就是一个事务在没有提交的情况下，其他事务是读取不到这个事务的修改之后的值的。 可重复读（RR） mysql默认的事务级别 这个级别下，不会发生脏写、脏读和不可重复读的问题，因为你一个事务多次查询一个数据的值，那么别的事务修改了这个值并且提交了，但是你还是不会读到其他事务修改过的值，你的事务一旦开始，多次查询一个值，会一直读到同一个值 串行化这个级别的意思就是不会有事务并发执行了，一个一个执行 一般生产不会选择这个级别，因为这样数据库的性能太差了 MVCCundolog版本链 上图看起来很简单，无非就是一个新的事务来了，更新这个值，就在undolog版本链上增加一行数据，值更新，txr_id更新为书屋id，roll_pointer指向上一条数据的值 ReadView执行一个事务的时候，就会生成一个ReadView 四个参数 m_ids 这个就是说此时有那些事务在mysql里执行还没有提交 min_trx_id 就是m_ids里最小的值 max_trx_id 就是myslq下一个要生成的事务di，就是最大的事务id creator_trx_id 就是当前事务的事务id 执行过程数据库里有一行数据 有两个并发的事务过来了，一个是A，事务id=45，一个B，事务id=59，B是去更新这行数据的，A是去查询这个行数据的 A事务生成一个ReadView， m_ids 45,59 min_id 45 max_id 59 creator_trx_id 45 A开始查询数据，A要进行判断，判断当前这行的数据的txr_id是否小于ReadView中的min_trx_id,小于最min_trx_id说明你事务开始之前这个事务已经执行完成了，所以可以看到这个事务的结果，也就是这行数数据 B把这个数据更新了，于是就把B的事务id作为txr_id，B的值也放上去，然后roll_pointer指向原始值 此时A再过来查，还是先和当前txr_id比较，发现trx_id在m_ids之中，就知道这个事务是和自己的事务并发执行的，所以这个结果是读不到的， 于是就顺着链表往下查找，发现32是小于min_trx_id的，知道这行数据是在自己事务之前执行完成的，所以可以读到这行数据 最终读到的数据就是原始值 RCRC的实现其实就是根据undolog版本链和ReadView来构成的，其实每次查询的时候都生成一个ReadView，这样就可以保证，虽然两个事务同时开启，其中一个写的数据先执行成功，另外一个读的事务可以读到修改后的值 重点在于：m_ids中代表这还活跃的时候，当发现这个事务的id在min_trx_id和max_trx_id之间，但是不在m_ids中的时候，表示的意思是 两个事务虽然同时开始执行，但是其中一个先提交了，所以另外一个事务可以看到这个事务已经提交的值 RRrr通过undolog版本链和ReadView解决了不可重复读和幻读的问题 解决不可重复读和幻读问题，主要是靠 只 生成一次的ReadView来解决的 还用上面的那个例子来说明 刚开始是有一条原始数据 trx_id为32，然后事务A和事务B同时启动，事务A是查询，事务B是更新数据，在事务A第一次查询的时候，生成一次ReadView m_ids 45,59 min_id 45 max_id 59 creator_trx_id 45 事务A第一次查询，在B事务还没有提交的时候，读取到的是原始值，因为32小于min_id 事务A第二次查询，在B事务已经提交的时候，此时trx_id变为了59，59在 在min_ids中，也在min_id和max_id之间，表示事务A和事务B是同时启动的，所以即使事务B提交了，但是A还是读取不到B修改后的值，读取到的还是原始值 重点在于：事务id 在min_ids中，也在min_id和max_id之间，表示事务A和事务B是同时启动的，并发启动，无论你事务是否提交，都只能读取到之前已经提交的数据 幻读，也是靠只 生成一次的ReadView来解决的，无论你修改多少回，事务A查询的这个ReadView永远不变，在B之后修改的时候id，肯定要大于B的事务id59，而大于max_id的意思是在之后才执行的事务，所以是读取不到的，所以就往前面找，往前找就到了上面不可重复读的情况，再往上就读取到了 原始数据 启动方式两种 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 事务传播事务传播 - Propagation REQUIRED使用当前的事务，如果当前没有事务，则自己新建一个事务，子方法是必须运行在一个事务中的； 如果当前存在事务，则加入这个事务，成为一个整体。 举例：领导没饭吃，我有钱，我会自己买了自己吃；领导有的吃，会分给你一起吃。 SUPPORTS如果当前有事务，则使用事务；如果当前没有事务，则不使用事务。 举例：领导没饭吃，我也没饭吃；领导有饭吃，我也有饭吃。 MANDATORY该传播属性强制必须存在一个事务，如果不存在，则抛出异常 举例：领导必须管饭，不管饭没饭吃，我就不乐意了，就不干了（抛出异常） REQUIRES_NEW如果当前有事务，则挂起该事务，并且自己创建一个新的事务给自己使用； 如果当前没有事务，则同 REQUIRED 举例：领导有饭吃，我偏不要，我自己买了自己吃 NOT_SUPPORTED如果当前有事务，则把事务挂起，自己不适用事务去运行数据库操作 举例：领导有饭吃，分一点给你，我太忙了，放一边，我不吃 NEVER如果当前有事务存在，则抛出异常 举例：领导有饭给你吃，我不想吃，我热爱工作，我抛出异常 NESTED如果当前有事务，则开启子事务（嵌套事务），嵌套事务是独立提交或者回滚； 如果当前没有事务，则同 REQUIRED。 但是如果主事务提交，则会携带子事务一起提交。 如果主事务回滚，则子事务会一起回滚。相反，子事务异常，则父事务可以回滚或不回滚。 举例：领导决策不对，老板怪罪，领导带着小弟一同受罪。小弟出了差错，领导可以推卸责任。","link":"/posts/1027016161.html"},{"title":"mysql命令","text":"sql语句 database12345678#显示所有数据库show databases;#切换数据库use 数据库名;#DDL 数据定义语言，DDL语言是隐性提交的，不能rollbackcreate#DML 数据操纵语言insert update delete select123select 字段 from 表名select * from 表名 #查询所有docker exec -it mysql bash #进入到docker的mysql中 delete1DELETE FROM 表名称 WHERE 列名称 = 值 删除delete1update tableName set key1=value1,key2=value2 where delete不建议在生产上删除大规模数据，因为这会导致数据库性能下降。 原因是 使用delete删除数据会形成一个大事务，非常影响数据库的服务 如果使用delete误删除数据了，可以通过Flashback工具来进行恢复 Flashback 恢复数据的原理，是修改 binlog 的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保 binlog_format=row 和 binlog_row_image=FULL。 事前保证： 把 sql_safe_updates 参数设置为 on。这样一来，如果我们忘记在 delete 或者 update 语句中写 where 条件，或者 where 条件里面没有包含索引字段的话，这条语句的执行就会报错 代码上线前，必须经过 SQL 审计。 adddrop1drop table 表名字 index12#显示一个表的所有索引show index from 表名; order by排序，默认是升序排列，想要降序的话使用desc where order by limit执行顺序：先where 然后order by 最后limit show12345678910111213#可以列出MySQL服务器运行各种状态值，另外，查询MySQL服务器配置信息语句：show global status;#show variables like ‘%slow%‘；show global status like ‘%slow%‘;#查看当前正在执行的语句show processlist;#查看表的统计信息#rows是个估计值，data_length是表的聚簇索引的字节大小show table status like “表名”; #查看从库状态 show slave status； 里面有个seconds_behind_master 意思是从库和主库的延迟是多少秒 joininner join普通的联表查询就是内连接，就是两个表的交集 outer join一般连接条件放在on里面 1234select e.name,e.department,ps.product_name,ps.saled_amountfrom employee e left out join product_saled paon e.id=pa.employee_id nest-loop join嵌套循环关联 左外连接如果左表的数据在右表里没有任何匹配的也要全部返回 右外连接如果右表的数据在左表里没有任何匹配的也要全部返回 limit主要作用就是限制回表到聚簇索引的次数，比如你select * from student ，会进行全表扫描，加个limit就会减少从联合索引查询的数据，然后再从主键索引中查找，性能会好一些 12#查询的是从6-15行limit 5,10","link":"/posts/584054310.html"},{"title":"mysql常见生产问题","text":"线上数据库时不时的发生性能抖动的问题 当buffer pool中没有缓存页的时候，来了一个查询大量数据的sql，大量脏页刷盘 redolog buffer的所有文件都已经写满的时候，来了一个大量更新的sql，需要将第一个redolog涉及的缓存页更新到磁盘中 解决办法： 由于无法控制缓存页刷新到磁盘的频率，所以只能是加大内存，给buffre pool分配的内存大一点大小 就是加快缓存页到磁盘的速度，那就是才去SSD固态硬盘，众所周知，固态硬盘比机械硬盘快","link":"/posts/2789195984.html"},{"title":"mysql执行流程","text":"基本流程 上图就是mysql的基本流程 连接器账号密码输入之后即可登录 连接的方式分为两种： 长连接：连接成功后，如果客户端持续有请求，则一直使用同一连接， 短连接：指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接 但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 怎么解决这个问题呢？你可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 解析器这个解析器的功能，其实很好理解，就是用来识别关键字的，比如常见的select delete create等等，同时也要检查你写的sql语句语法是否有问题，有问题的就是直接返回 优化器多索引的时候，一般会选择在索引里扫描行数比比较少的那个条件 或者join连接的时候，先连接哪个表 成本计算 sql改写就是会对你的sql语句进行一些优化，更加明确sql语句的语义 执行器经过前面两个步骤之后，到这里mysql才真正开始执行你写的sql语句，但是它会判断当前这个用户是否有这个操作的权限，如果有，则执行sql语句，如果没有，则返回没有权限的错误 引擎扫描行数跟 rows_examined 并不是完全相同的 innodb引擎引擎执行sql语句的时候，也并不是直接去磁盘中查找的，而是要先在缓存中进行查找，在缓存中找不到的话，才会从磁盘中将数据加载到缓存中，然后从缓存中读取 这个缓存层，在mysql中人们习惯称之为buffer pool Buffer Pool 上图其实就是buffer pool的执行流程 首先是判断数据是否在buffer pool中，如果在的话，直接更新数据，如果不在，则直接去磁盘中请求相应数据，然后刷入到buffer pool中，再从buffer pool中读取 更新undolog到磁盘中，undolog的作用其实非常清楚，它主要是用于回滚事务的，比如你这个事务没有执行成功，它就依靠undolog来进行回滚 将更新数据的操作写入到redolog buffer中 将更新数据的操作写入到binlog中 提交redolog的commit，将redolog写入到os cache中，然后根据redolog的参数来决定os cache的刷盘策略 redolog作用：主要用来出现意外情况，数据还在内存当中，但是机器宕机的场景 redo log里本质上记录的就是在对某个表空间的某个数据页的某个偏移量的地方修改了 redolog与binlog的区别 redolog 在缓存中叫做redolog buffer，在磁盘中叫做 redolog file。 redologbuffer 默认为16MB 两段式提交，数据刷新到内存，然后刷新到redolog buffer上，此时redolog是处于prepare阶段，然后mysql将数据刷新到binlog上，binlog写入成功之后，提交给redolog一个commit，此时redolog才算刚刚结束，接着要把redolog buffer刷入到os cache中，然后根据redolog的参数来决定os cache的刷盘策略 redolog buffer刷新到磁盘中是有方法的，在redolog buffer没有写满的时候，采用追加写的方式，当redolog buffer 写满的时候， 了解了redo log的写入方式之后，我们发现主要完成的操作是redo log buffer 到磁盘的redo log file的写入过程，其中需要经过OS buffer进行中转。关于redo log buffer写入redo log file的时机，可以通过 参数innodb_flush_log_at_trx_commit 进行配置，各参数值含义如下： l参数为0的时候，称为“延迟写”。事务提交时不会将redo log buffer中日志写入到OS buffer，而是每秒写入OS buffer并调用写入到redo log file中。换句话说，这种方式每秒会发起写入磁盘的操作，假设系统崩溃，只会丢失1秒钟的数据。 l参数为1 的时候，称为“实时写，实时刷”。事务每次提交都会将redo log buffer中的日志写入OS buffer并保存到redo log file中。其有点是，即使系统崩溃也不会丢失任何数据，缺点也很明显就是每次事务提交都要进行磁盘操作，性能较差。 l参数为2的时候，称为“实时写，延迟刷”。每次事务提交写入到OS buffer，然后是每秒将日志写入到redo log file。这样性能会好点，缺点是在系统崩溃的时候会丢失1秒中的事务数据。 结构日志类型（就是类似MLOG_1BYTE之类的），表空间ID，数据页号，数据页中的偏移量，具体修改的数据 redo log就划分为了不同的类型，MLOG_1BYTE类型的日志指的就是修改了1个字节的值，MLOG_2BYTE类型的日志指的就是修改了2个字节的值，以此类推，还有修改了4个字节的值的日志类型，修改了8个字节的值的日志类型。 当然，如果你要是一下子修改了一大串的值，类型就是MLOG_WRITE_STRING，就是代表你一下子在那个数据页的某个偏移量的位置插入或者修改了一大串的值。 MLOG_WRITE_STRING类型的日志，因为不知道具体修改了多少字节的数据，所以其实会多一个修改数据长度，就告诉你他这次修改了多少字节的数据，如下所示他的格式： 日志类型（就是类似MLOG_1BYTE之类的），表空间ID，数据页号，数据页中的偏移量，修改数据长度，具体修改的数据 组成redolog buffer里面有很多条数据，那它刷新到磁盘的时候，总不可能一条数据一条数据刷吧，那样性能就太差了，所以说有了redolog block这个数据结构 redolog block中存放了许多个单行日志，刷新到磁盘按照redolog block来刷新 一个block最多放496个自己的redo log日志 ，一个redolog block 是512字节，这个redolog block的512字节分成三个部分 12字节的header快头 4个字节的block no，块的唯一编号 2个字节的data length，就是block里面写入了多少字节数据 2个字节的first record group 这个是说每个事务都会有多个redo log，就是一个redolog group，即一组redo log。那么在这个block里的第一组的redolog的偏移量，就是这个两个字节存储的 4个字节的checkpoint on 496字节的body块体 4字节的trailer块尾 事务在进行一个事务的时候，由于一个事务要进行多个增删改查的操作，所以一般都是将这些redolog先在别的地方存放，等到都转型完毕了，就把这一组redolog写入到redolog bufer中 刷盘时机 如果写入redolog bufer 的日志已经占据了redolog buffer总容量的一半，就会开始刷盘 一个事务提交的时候，必须把它那些redolog 所在的redolog block刷入到磁盘中去，只有这样，才可以保证事务提交之后，他提交的数据绝对不会丢失。因为redolog有日志记录，随时可以回复事务做的修改 后台线程定时刷新，有一个线程每个1秒就会吧redolog buffer中的redolog block刷入到磁盘文件 mysql关闭的时候，redolog buffer全部刷新到磁盘中 参数InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值： 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘； 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。 一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。 组提交日志逻辑序列号（log sequence number，LSN）的概念。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。 LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。 比如说有三个事务并发提交了，对应的 LSN 分别是 50、120 和 160,如果50的lsn对应的事务先到达os cache之后，它就会成为leader，等到它开始要进行刷盘的时候，此时这个组里面已经有三个事务了，lsn变为了160，所以这个事务写盘的时候，带的lsn是160，因此等 这个事务返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘； 命令123456#查看redolog目录show variables like 'datadir'#设置redolog目录innodb_log_group_home_dir#redolog默认为48MB，默认有两个日志文件innodb_log_files_in_groupinnodb_log_file_size binlog作用：主要用来进行主从备份的 刷盘方式 sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。 组提交如果你想提升 binlog 组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 来实现。 binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync; binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。、 这两个条件是或的关系，也就是说只要有一个满足条件就会调用 fsync。所以，当 binlog_group_commit_sync_delay 设置为 0 的时候，binlog_group_commit_sync_no_delay_count 也无效了。 格式主要分为三种格式，statement row和mixed statement 格式下，记录到 binlog 里的是语句原文，就是你在mysql中执行的是什么语句，在binlog也是同样的语句，但是这样可能会导致出现索引不通的情况出现，当索引不通的时候，你在执行delete的时候还使用了limit，那么就会出现错误删除的情况 rpw 格式下，记录的是哪个表，删除的主键id是什么，所以是不会执行错误的，但是它同样有一个缺点，就是当数据量大的时候，非常消耗空间 ，mixed其实就是前两种格式的融合 mixed的来源 因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。 但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。 所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。 undologundolog这个日志主要是用来进行事务回滚的，一般只有进行数据变动的时候才会有undolog，比如update insert delete，但是select 是没有的，因为select只是获取数据，并没有对数据进行变更 比如你插入一条数据，undolog中记录的是 删除一条数据，是和你进行操作的行为是相反的 结构 这条日志开始的问题只 主键的各列长度和值，主键可能是你设置的表的主键，也可能是三个字段组成的联合主键，也有可能是myslq默认添加的row_id作为主键 表id undolog日志编号 undolog日志类型 ，比如 insert语句的undolog的日志类型是 TRX_UNDO_INSERT_REC 这条日志的结束位置 区别 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ” redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？ MySQL 这么设计的主要原因是，binlog 是不能“被打断的”。一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。 非双1一般情况下，把生产库改成“非双 1”配置，是设置 innodb_flush_logs_at_trx_commit=2、sync_binlog=1000。 crash-safe即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚 这个能力依赖的就是redo log和unod log两个日志。 实际上数据库的 crash-safe 保证的是： 如果客户端收到事务成功的消息，事务就一定持久化了； 如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了； 如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了。 磁盘文件磁盘文件有三个层级 一组数据组，是256个数据区 一个数据区，是64个数据页 一个数据页，是16kb 在磁盘中把一页的数据叫做数据页，在缓存中，称之为缓存页","link":"/posts/3948463409.html"},{"title":"mysql执行计划","text":"Explain 模样 使用1explain select * from user possible_keys所有的可能使用的索引 key实际使用的索引 select_typesimply 单表或者多表连接查询 primary 主查询 subquery 子查询 union_result 根据union去重 depend subquery 内层的子查询 depend union 内层的union derived 根据临时表执行的查询 materialized 物化为临时表，就是生成一个临时表存储到磁盘上 id一个select对应一个id，如果是多个select就会对应多个id keys_len索引的长度 extrausing index直接在二级索引中查到结果，不需要回表 using index condition using where 没有用到索引 除了用到了索引，还用到了其他没有索引的字段 using firesort没有用到索引，会基于内存或者磁盘进行排序，大部分情况下都是基于内存进行排序的，性能非常差 using temporary使用了临时表进行操作 using join buffer内存优化的手段，减少表的全表扫描次数 filtered经过搜索之后剩余数据的百分比,也就是你想要的结果 rows预估读取的行数 typeconst直接通过主键索引或者唯一二级索引列进行等值匹配的时候可以查找到数据，type就是const 但是这个二级索引必须是唯一的，如果只是二级索引不是唯一索引，type就是ref ref 如果是普通索引进行等值比较，就是ref 执行连接查询，被驱动表中的某个普通二级索引列与驱动表中的某个列进行等值匹配，那么对被驱动表也可能是使用ref的访问方法 eq_ref执行连接查询的时候，如果被驱动表是通过主键或不允许存储null值的唯一二级索引列等值匹配的方式进行访问的（如果该主键或者不允许存储null的唯一二级索引是联合索引，则所有的索引都必须进行等值比较），则对该驱动表的访问方式是eq_ref ref_or_null如果是多个普通索引，连续多个都是等值比较且索引列的值可以null值的时候，访问方法可能是ref_or_null 不是某个列的值为null，而是在sql语句中专门加上is null 这种情况 fulltext全文索引 index_merge一般情况下只会为单个索引生成扫描区间，但是我们在唠叨单标访问方式的时候，特意强调了在某些场景下也可以使用Intersection union sort-union这三种索引列合并的方式来执行查询 unique_subquery类似于两表连接的eq_ref访问方法，unique_subquery针对的是一些包含in子查询的查询语句。如果查询优化器决定将in子查询转换为exists子查询，而且子查询在转换之后可以使用爪机或者不允许存储null值的唯一二级索引进行等值匹配，那么该子查询的type列的值就是unique_subquery index_subquery和unique_subquery类似，只不过在访问子查询中的表时使用的是普通索引 range一旦使用了二级索引作为范围查找，就是range index 只要遍历二级索引就可以拿到想要的数据，不需要进行回表的，就是index 比如 select x，y，z from user where x2=xxx（联合索引字段为x，y，z），这个时候发现，x，y，z都是联合索引的字段，所以直接根据联合索引的索引树，将数据取出来即可，这个比遍历主键索引块多了，毕竟二级索引的叶子节点数据比主键索引的叶子节点少的多 对于innodb存储引擎来说，当我们需要执行全表扫描，并且需要对主键进行排序的时候，此时访问方法就是index all全表扫描 refconst使用了你在sql语句中的值进行等值匹配 table_db.t1.id1select * from t1 inner join t2 on t1.id=t2.id 这样的情况，type列一般为eq_ref show wanging执行完执行计划，可以执行这个语句，如果看到semi join可能这就是sql之索引慢的原因了 mysql在生成执行计划的时候，会自动的把一个普通的in语句，优化成了in+子查询","link":"/posts/2486957536.html"},{"title":"mysql索引","text":"想要了解mysql的索引，首先要了解mysql的物理存储结构 物理存储结构 mysql的物理存储结构其实就是上图所示，是一个双向链表，每个结点的都是一个数据页， 一行一行的数据就放在这个数据页中 索引为什么会产生索引这么一个东西，很简单，就是加快查询速度啊！ 其实索引就和书籍的目录一样，都是为了更快的可以找到所需要的数据 那数据库的索引究竟是什么样子那？ 其实就和书籍的目录一样，就是将每一个数据页的最小数据放在目录上，根据这个数据来进行多次的二分查找 页分裂目的：就是进行主键排序，保证所有的数据页的主键都是从小到大排序的 页分裂这个其实也很简单，数据库中那么多数据，一个数据页肯定存储不下，当存储的数据大于一个数据页的时候，就会将新的数据和这个数据页的整体数据进行排序，主键比较大的数据放在新建的数据页中 构成其实我们从索引的作用就可以自己猜想出来索引的组成了 首先，肯定是有对应数据页的主键的最小值，目的很简单，就是用于二分查找，提高查询的速度 根据上一步拿到了想要找的数据所在的数据页，但是具体是那个不知道啊，所以就必须有 数据页页号 B+树众所周知myslq数据库采用的B+树的数据结构，那究竟是为什那？ 有以下三个原因 单一节点存储的元素更多，使得查询的IO次数更少，所以也就使得它更适合做为数据库MySQL的底层数据结构了。 所有的查询都要查找到叶子节点，查询性能是稳定的，而B树，每个节点都可以查找到数据，所以不稳定。 所有的叶子节点形成了一个有序链表，更加便于查找。 原则最左匹配原则比如说你建立了一个（a,b）的索引，就没有必要再建立a索引了，因为（a，b）这个所以已经包括了a索引了，所有没有必要再单独建立b索引，但是b索引还是要单独建立的，因为（a,b）是满足a且满足b的情况，与只索引b不是一个意思 当然，a和b你要考虑这两个索引的大小，尽量选择较小的那个，单独作为索引，因为这样速度相对来说会快一点 第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 等值原则查询的字段和数据库的字段一样，而且是=进行匹配的，百分之一百会使用索引 最左前缀匹配原则如果要用like来进行模糊查询，1%可以用上索引，但是%1就不行了 范围查找原则如果where语句中有范围查询，那么只有联合索引的最左侧的索引进行范围查找才能用到索引 等值匹配+范围匹配原则类型主键索引主键索引又被称为聚簇索引 主键索引的叶子节点存的是完整的数据页 在Key-Value的场景下，只有一个索引且是唯一索引，则适合直接使用业务字段作为主键索引。 主键索引首先要去查询主键目录，主键目录维护的就是每个数据页的页号和最小的主键值 非主键索引别名：二级索引、辅助索引、普通索引 非主键索引的叶子节点内容是主键的值 叶子结点存储的是主键+字段值 在不影响排序结果的情况下，在取出主键后，回表之前，会在对所有获取到的主键排序 如果一个主键被定义了，那么这个主键就是作为聚集索引。 如果没有主键被定义，那么该表的第一个唯一非空索引被作为聚集索引。 如果没有主键也没有合适的唯一索引，那么innodb内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键是一个6个字节的列row_id，该列的值会随着数据的插入自增。 减少回表次数的方法 覆盖索引 一般来说，一个sql语句中只能用到一个二级索引，但是也有可能同时查多个索引树取个交集，再回表到主键索引，这个可能性是有的 区别主键索引和普通索引的区别：主键索引只要搜索ID这个B+Tree即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表) 那么我们是选择用业务字段来当做主键，还是一个自增字段来当做主键？ 首先自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择 一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。 覆盖索引如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，那么就不用回表操作，直接在联合索引 索引树就可以查找到数据，可以直接返回结果 必须索引包含了where条件部分和select返回部分的所有字段，才能实现覆盖索引 最好是用上limit 或者where之类的语句，来限定回表到聚簇索引的次数 唯一索引唯一索引就是字面意思，一个表中只有一个索引，如果这个索引不是主键索引，那么它就只可能是非主键索引（即我们常说的二级索引或普通索引） 联合索引就是依次按照每个字段进行二分查找，先定位到第一个字段的值在哪个页里，然后如果一个页里有很多条数据都一样，就会根据第二个字段来找，以此类推 联合索引的叶子节点放的是也是页，但是不是整个页数据，而是联合索引中的那些字段 设计索引 尽量让使用基数比较大的字段，就是值比较多的字段用上索引，充分发挥B+树的优势 基础比较大的字段是说，比如一个字段，虽然在所有的行中都有值，但是其实都是0和1，那么就没有必要为这个字段设置索引 因为你的索引字段中如果只有1和0两个数字，也就没有办法利用二分查找，所以设置这个字段为索引也就没有什么意义 尽量对字段的类型比较小的字段进行索引 尽量不要让查询语句中有函数或者计算，这样会导致用不上索引 尽量主键是自增的，不要用uuid之类的，因为如果主键是自增的，就会自然进行页分裂，但是如果不是，就会导致频繁的页分裂 索引不要太多，一般两三个联合索引就可以覆盖一个表的全部查询了 一个表中的索引太多的话，会导致虽然查询非常方便，但是增删改每次都要维护巨大的索引树，性能会急速下降 尽量范围查询放在最后面，保证索引都可以用 where 筛选和order by排序实际上大部分情况下，是没有办法同时用到索引的 这个时候，推荐让where用上索引，因为where用上索引，会在数据量小的情况下，先将符合条件的where的数据加载到内存中，然后根据order by条件进行排序，在内存中排序会比直接在磁盘排序要快得多 重点尽量利用一两个复杂的多字段的联合索引，支撑下80%以上的查询，然后利用一两个辅助索引支撑下剩余20%的非典型索引，保证99%以上的查询都能充分利用索引，就能保证你的查询和性能！ 技巧 比如你经常需要查询在七天之内登录的用户，完全可以将这个情况当做一个字段，新增这个字段，比如1就是七天之内登录，0就是七天之内没有登录，直接将其转换为枚举值了 注意问题对索引字段做函数操作，优化器会放弃走树搜索功能 如果一个字段做了范围索引，那么之后的索引都会失效，所以一般建议范围索引放在最后面 排序使用经常用的多字段排序，可以直接按照顺序建立一个联合索引，由于索引本身就有排序，所以速度很快，直接根据索引树查找即可 要求必须要都是升序或者降序，mysql默认为升序，降序的话需要在order by后面+desc where和orderby当where和orderby只能选择一个使用索引的时候，如果数据量小，可以选择先where查找出来数据，之后再进行排序和分页，成本不会太大 原理MySQL会为每个线程分配一个内存（sort_buffer）用于排序该内存大小为sort_buffer_size 如果排序的数据量小于sort_buffer_size，排序将会在内存中完成 如果排序数据量很大，内存中无法存下这么多数据，则会使用磁盘临时文件来辅助排序，也称外部排序 在使用外部排序时，MySQL会分成好几份单独的临时文件用来存放排序后的数据，然后在将这些文件合并成一个大文件 mysql会通过遍历索引将满足条件的数据读取到sort_buffer，并且按照排序字段进行快速排序 如果查询的字段不包含在辅助索引中，需要按照辅助索引记录的主键返回聚集索引取出所需字段 该方式会造成随机IO，在MySQL5.6提供了MRR的机制，会将辅助索引匹配记录的主键取出来在内存中进行排序，然后在回表 按照情况建立联合索引来避免排序所带来的性能损耗，允许的情况下也可以建立覆盖索引来避免回表","link":"/posts/1895910490.html"},{"title":"mysql锁","text":"表结构分类 行锁默认情况下，是不需要锁的，默认是开启MVCC机制的，所以读取数据和修改数据完全不会互相影响，直接根据undolog版本链和ReadView来进行读取即可 共享锁共享锁又被称为S锁 默认查询数据不会开启共享锁的，是走mvcc机制读快照版本，但是可以手动添加 1234#加共享锁select * from vpm_project lock in share mode#查询之后还是更新，这样就给查询语句加上了独占锁select * from vpm_project for update 生产情况下，是不会开启共享锁的，因为根据这样会造成性能下降，而且根据mvcc机制读快也可以保证数据的准确性 如果非要在查询的时候加锁，通常都是在redis/zookeeper分布式锁来控制系统的锁逻辑，因为你如果直接在数据中加上复杂业务的锁逻辑，锁逻辑会隐藏在sql语句中，这对于java来说不太好维护 lock in share mode 只锁覆盖索引 独占锁默认更新数据的时候是开启的 关系 读取数据其实就是 select 修改数据其实就是 update delete insert 间隙锁gap lock 官方称为 LOCK_GAP 作用：只是为了给这条记录前面的间隙插入数据 这个锁的设计之初就是为了防止插入幻影记录 这个锁也有S锁和X锁两种，但是其实并没有什么卵用，并不妨碍其他事务的插入 infimum：表示一个表中的最小记录 supremum：表是一个表中的最大记录 比如你要防止在id为20之后中插入新的记录，你就可以给所在页面的supremum加上一个gap锁 Next-Key Lock作用：既想要给当前记录加锁，又想要阻止其他事务在该记录前面的间隙插入新纪录 本质就是record lock和gap lock的个体 这个锁只有在可重读的情况下才会生效，在可重复读的情况下，mysql加锁的基本单位就是next-key lock next-key lock 是前开后闭区间 insert intention lock插入意向锁 官方名称为 LOCK_INSERT_INTENTION 一个事务在插入一条记录的时候，需要判断插入位置是否已经被别的事务加入了gap锁，如果有的话，插入操作需要等待，知道等待拥有gap锁的按个事务提交为止。 innodb规定，这种也需要生成一个锁，于是命名为insert intention lock 这个锁比较“鸡肋”，不会阻拦任何其他事务的操作 record lock我们自己给它起个别名叫 记录锁 官方 名称为 LOCK_REC_NOT_GAP 就是给数据库里的一条记录加锁，这中锁也有S锁和X锁两种 和基础的S锁和X锁一样 写锁和写锁 读锁都冲突 读锁和读锁不冲突，和写锁冲突 隐式锁事务id其实就是一个隐式锁 其他所有的锁，加锁之后都会在内存中创建一个锁记录关联，只有隐式锁不会 两段锁协议在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 出现死锁之后的解决办法： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 如何解决热点行更新导致的性能问题？ 如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关闭掉。一般不建议采用 控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。 将热更新的行数据拆分成逻辑上的多行来减少锁冲突，但是业务复杂度可能会大大提高。 表锁执行DDL语句的时候，默认会加上表锁，这是通过数据库的元数据锁实现的，也就是Metadata Locks DDL语句和增删改操作，确实是互斥的 共享锁12#加表级共享锁lock tables 表名 read; 独占锁12#加表级独占锁lock tables 表名 write; 意向共享锁当有事务在表里执行增删改操作的时候，会默认在行级加独占锁，同时也会在表级加一个意向独占锁 意向独占锁当有事务在表里执行查询操作的时候，会默认在表级加一个意向共享锁 关系 全局锁对整个数据库实例加锁。 MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL) 这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。 使用场景：全库逻辑备份。 风险： 1.如果在主库备份，在备份期间不能更新，业务停摆 2.如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟 官方自带的逻辑备份工具mysqldump，当mysqldump使用参数–single-transaction的时候，会启动一个事务，确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。 一致性读是好，但是前提是引擎要支持这个隔离级别。 如果要全库只读，为什么不使用set global readonly=true的方式？ 1.在有些系统中，readonly的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大。 2.在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。 加锁条件 在同一个事务中进行加锁操作 被加锁的记录在同一个页面中 加锁的类型是一样的】 等待状态是一样的 同时满足上面四个条件，就可以将多个所记录的锁放在同一个锁结构中 加锁规则总结可重复读 加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。 查找过程中访问到的对象才会加锁。 等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。如果不是唯一索引，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 范围查询，无论是否是唯一索引，范围查询都需要访问到不满足条件的第一个值为止 for update 默认的是更新之后还要进行查找，所以不仅给索引上锁，符合条件的主键索引页会上锁 唯一索引会找到对应范围就会停止，但是非唯一索引是需要多找一段 范围索引InnoDB 会往前扫描到第一个不满足条件的行为止 删除的时候limit，不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围 读提交语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交。","link":"/posts/1181292247.html"},{"title":"mysql集群","text":"主从架构 其实就是一个master服务器和一个slave服务器 master服务器主要负责平常的读和写，而slave就是要同步master的数据 同步原理 从库上有个io线程与主库进行tcp连接 这个io线程请求主库将binlog传输给自己，主库上有个io dump线程，负责通过这个tcp连接把binglog日志传输给从库的io线程 从库的io线程会把读取到的binlog日志写入到自己的本地的中转（relaylog）日志中 从库中另外一个sql线程会读取relay日志里的内容，进行日志重做，把所有主库执行过的增删改查，在从库上执行一遍，做到数据和主库的数据一致 从库可以设置为只读（readonly）模式 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作； 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致； 可以用 readonly 状态，来判断节点的角色。 你可能会问，我把备库设置成只读了，还怎么跟主库保持同步更新呢？ 因为 readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，就拥有超级权限。 建议log_slave_updates 设置为 on，表示备库执行 relay log 后生成 binlog 读写分离半同步复制 after_commit 主库写入数据到binlog，等到binlog传输给从库，主库就提交事务，接着等待从库返回给自己一个成功的响应，然后主库返回提交事务成功的响应给客户端 mysql5.7默认的 主库写入数据到binlog，将binlog传输给从库，从库写入成功，给主库返回响应，主库才提交事务，接着返回事务成功的响应给客户端 123#检查半同步复制是否正常运行shwo global status like '%semi%';如果看到了Rpl_semi_sync_master_status的状态是on，就表示开启的 主从延迟使用percona-toolkit工具集里的pt-hearbeat工具，他会在主库中创建一个hearbeat表，然后有一个线程定时更细这个表里的时间戳字段，从库上就会有一个monitor线程会负责检查从库同步古来的hearbeat表的的时间戳 把时间戳跟当前时间戳比较一个下，就知道主从同步落后了多长时间 解决办法： mysql5.7已经支持并行复制了，在从库中设置slave_parallel_workers&gt;0 然后把slave_parallel_type设置为 logical_clock 就ok了 如果想要刚写入的数据立马强制必须一定可以读到，可以利用类似mycat或者sharding-spherre之类的中间件里设置强制读写都从主库从，这样你写入主库的数据，强制从主库里读取，一定立即可以读到 三个原因 备库的压力大 备库所在机器的性能要比主库所在的机器性能差 大事务 不建议一次性地用 delete 语句删除太多数据 备库的并行复制能力 高可用一般生产环境使用MHA，Master High Availablility Magager and Tools for Mysql，日本人写的，用perl脚本写的一个工具，这个工具就是专门用于监控主库的状态，如果感觉不对，可以把从库切换为主库 这MHA也是需要单独部署的，分为manager和node。manager节点一般是单独部署一台机器的，node节点一般是部署在每台myslq机器上的额，因为node节点需要通过解析各个myslq的日志来进行一些分析 单表建议mysql的单表数据量不要超过1000万，最好是在500万之内，如果能控制在100万以内，那是最佳的选择，基本单表在100万以内的数据，性能上不会有太大的问题，前提是你做好索引 一般一亿行数据，大小在一个g到几个g之间","link":"/posts/1202068288.html"},{"title":"oracle表空间","text":"详细介绍表空间 在逻辑结构中，Oracle从大到下，分别是如下的结构：数据库实例 -&gt; 表空间 -&gt; 数据段（表） -&gt; 区 -&gt; 块。 也就是说当我们要使用Oracle作为项目的数据库时，我们需要先创建数据库实例，之后创建表空间，再创建相对应的表（也就是逻辑结构中的数据段）。 使用Oracle作为项目的数据库时，我们需要先创建数据库实例，之后创建表空间，再创建相对应的表（也就是逻辑结构中的数据段）。 常营命令12345678910# 给表空间增加大小alter tablespace HGCS1031 add datafile 'C:\\APP\\YSS\\PRODUCT\\11.2.0\\DBHOME_1\\DATABASE\\HGCS1031_1.DBF'Size 1000M Autoextend on maxsize unlimited; #查看表空间大小select tablespace_name,sum(bytes)/1024/1024 from dba_data_files group by tablespace_name;# 查看表空间大小已经使用的情况和分配情况select SEGMENT_TYPE,owner,sum(bytes)/1024/1024 from dba_segments where tablespace_name='XITONG' group by segment_type,owner; # 查看block的大小 select value/1024 as &quot;kb&quot; from v$parameter where name='db_block_size'; 由于Oracle的Rowid使用22位来代表数据块号，因此Oracle表空间数据文件每个数据文件最多只能包含2^22个数据块。 也因此数据库表空间的数据文件不是无限增长的，例如： 在数据块为8k的情况下，单个数据文件的最大容量为8K*2^22 = 32G 2K = 8G、8K = 32G、16K = 64G、32K = 128G； DB_BLOCK_SIZE作为数据库的最小操作单位，是在创建数据库的时候指定的，在创建完数据库之后便不可修改。要修改DB_BLOCK_SIZE，需要重建数据库。一般可以将数据EXP出来，然后重建数据库，指定新的DB_BLOCK_SIZE，然后再将数据IMP进数据库。 空间不足首先选择设置自增长，sql语句：Alter tablespace 表空间名 adddatafile ‘数据文件存放路径‘ autoextend on next 每次增加的大小 maxsize 数据文件大小的最大值 增加数据文件在自增长失灵了之后，需要增加数据文件，sql语句：Alter tablespace 表空间名 adddatafile ‘数据文件存放的路径’ size 数据文件大小M autoextend on next 每次自增长大小M Maxsize UNLIMITED；（后半部分为设置自增长） 这里放个例子： 1alter tablespace SDE add datafile 'E:\\app\\EmmaXu\\product\\11.1.0\\db_1\\database\\SDE_1.dbf' size 400Mautoextend off","link":"/posts/3892716853.html"},{"title":"redis cluster","text":"集群配置 12345678910111213141516mkdir -p /etc/redis-clustermkdir -p /var/log/redismkdir -p /var/redis/7001#配置文件目录/var/redis/7001.conf#redis.confport 7001cluster-enabled yescluster-conifg-file /etc/redis-cluster/node/7001.confcluster-node-timeout 15000daemonize yespidfile /var/run/redis_7001.confdir /var/redis/7001logfile /var/log/redis/7001.logbind 192.168.0.21appendonly yes 至少要3个master节点启动，每个master加一个slave节点，先选择6个节点，启动6个实例 12345678910111213141516171819yum install -y rubyyum install -y rubygemsgem install rediscp /home/redis/src/redis.trib.rm /usr/local/bin redis-trib.rb create --replicas 1 192.168.0.21:7001 192.168.0.21:7002 192.168.0.22:7003 192.168.0.22:7004 192.168.0.23:7005 192.168.0.23:7006#查看redis.cluster对于主从节点的配置信息redis-cli --cluster check 192.168.0.21:7001#测试redis-cli -h 192.168.0.22 -p 7004 #自动写入对应的机器redis-cli -h 192.168.0.22 -p 7004 -c#将节点加入到redis-cluster集群中redis-cli --cluster add-node 192.168.0.21:7007 192.168.0.22:7004#加入salve节点redis-cli --cluster add-node --slave --master-id c1dcc6197201c958fb06fa7737190e4209dc2171 192.168.0.22:7008 192.168.0.22:7004 #删除一个节点，要把它上面的slot移动到其他机器上 多master写入 每条数据上只存在于一个master上，不同master负责存储不同的数据 当你清空一个master的hashslot的时候，redis cluster会将其slace挂载到其他master上去，这个时候，你只需要删除这个节点就可以了 扩容redis-cli –cluster reshard 192.168.0.21:7001 读写分离redis-cluster可以实现物理的读写分离即一个redis专门用来读数据，一个redis专门用来写数据，但是一般不建议使用，现在用的都是master负责读写，而slave只是数据备份 默认不支持slave节点读或者写 在slave中要先执行readlonly，再执行get命令，才可以取到数据 自动化slave迁移当master的slave挂掉了，其他有冗余的slave会将自动将其slave挂在到缺失slave的master why为什么不进行读写分离那 原因是因为之前我们让物理的读写分离，是在一主多从的情况下，想要增加访问量，就是要增加slave的数量，这样才可以达到水平扩容的效果，但是用了redis.cluster之后，是多主多从的情况，因此要想增加屯库量直接增加master的数量就可以了 机器配置redis的内存不建议太大，一般是8g或者16g，如果内存比较大，redis在fork子线程的时候，可能会造成机器卡顿 原理基础通信原理各个节点之间通过gossip协议进行通信 跟集中式不通，不是讲集群元数据放在某一个节点上，而是互相之间不断通信，保持整个集群所有节点的数据是完整的 集中式存储 10000端口每个节点都有一个专门用于节点间通信的端口，就是自己提供服务端口号+10000，比如7001，那么用于通信的就是17001，每个节点每隔一段时间会往另外几个节点发送pin消息，同时其他节点接受到ping之后会返回pong 交换信息故障信息，节点的增加和移除，hashslot信息 等 gossipgossip协议包括多种信息，包括 Ping pong meet fail等 meet：某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点通信 ping： 每个节点都会频繁的给其他节点发送ping，其中包含自己的状态和自己维护的集群元数据，互相通过ping来交换元数据 每个节点每隔10秒回执行10次ping，每次会选择5个最久没有通信的其他节点，如果发现某个节点的通信延时达到了cluster_node_out/2，那么立即发送ping，避免数据交换延迟过长 每次ping，一个是带上自己的节点的信息，还有就是带上十分之一的其他节点的信息发送出去，进行数据交换 至少包含3个其他节点的信息，最多包含总节点-2个其他节点的信息 pong： 返回ping和meet，包含自己的状态和其他等信息，也可以用于信息广播和更新 fail： 某个节点判断另一个节点fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了 测试 图中出现的是moved的意思是应该到192.168.0.23的机器上 ，端口为7005的机器上进行这个命令的写入 每个机器在执行命令的时候，都会计算这key对应的crc16的值，然后对16384的hashslot取模，找到对应的hashslot，并且返回该hashslo所对应的机器及端口号 自动将命令执行在对应的机器上 redis-cli -h 192.168.0.21 -p 7001 -c 重启之后的日志","link":"/posts/63534.html"},{"title":"redis之String","text":"详细介绍redis String类型 whatString是redis的基本类型之一 12345678910111213141516171819202122232425262728293031set name shy #设置name为shy的键值对get name #获取键为name的值keys * #查看所有键值对exits key名字 #返回1，表示key存在，0表示不存在move key名字 1 #1表示当前数据库，删除keyexpire key名字 10 #设置key名字的过期时间为10秒ttl key名字 #查看key名字的过期时间还剩多少秒type key名字 #key的类型append key名字 值 #向key的值追加数据strlen key名字 #获取key的长度incr key #给key+1decr key #给key-1incrby key 步长 #key每次加多少decrby key 步长# key每次减多少getrange key 0 3 #截取字符串从0到3getrange key 0 -1 #查看所有内容setrange key 1 xx #将第1位的值替换为xxsetex（set with expire） #设置过期时间setex key3 30 “hello” #设置key3的值为hello，30秒后会过期setnx（set if not exist） #不存在设置setnx key3 “redis” #如果不存在key3，就创建mset key value #一次设置多个键值对mget key1 key2 #一次获取多个键的值msetnx 多键值对版本 #是一个原子性的操作，要么都成功，要么都失败 mset user:1:name zhangsan user:1:age 12#设置一个对象的两个属性 对象名：ID：属性名mget user：1：name user：1：age#获取一个对象的两个属性setget v1 redis#如果不存在v1，则返回空，但是将v1的值设置为redis#，如果存在，则返回v1的值，然后将v1的设置为redis whySDS采用sds结构来存储字符串，结果如下 redis中使用这样的sds的结构来构建字符串主要有以下几个原因： 当获取长度的时候，时间复杂度为0（1） 二进制安全，当比如用一个空字符串作为字符串中的一个特殊变量的时候，由于c中的字符串是通过\\0这个空字符来区分一个字符串是否结尾的，所以它只能用于保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据 杜绝缓冲区溢出（sds是通过预分配策略和惰性空间释放来减少的） 减少修改字符串时带来的内存重分配次数（sds是通过预分配策略和惰性空间释放来减少的） 可以使用c语言的一些函数，因为sds字符串的也是以\\0 作为结尾的，但是sds字符串是通过sds的len属性来确定这个字符串是不是结束的，c的字符串则是单一的通过\\0来确认一个字符串是否结束 预分配策略如果对SDS进行修改之后，SDS的长度（也即是len属性的值）将小于1MB，那么程序分配和len属性同样大小的未使用空间，这时SDS len属性的值将和free属性的值相同。举个例子，如果进行修改之后，SDS的len将变成13字节，那么程序也会分配13字节的未使用空间，SDS的buf数组的实际长度将变成13+13+1=27字节（额外的一字节用于保存空字符）。 如果对SDS进行修改之后，SDS的长度将大于等于1MB，那么程序会分配1MB的未使用空间。举个例子，如果进行修改之后，SDS的len将变成30MB，那么程序会分配1MB的未使用空间，SDS的buf数组的实际长度将为30MB+1MB+1byte。 惰性空间释放惰性空间释放用于优化SDS的字符串缩短操作：当SDS的API需要缩短SDS保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用。 sds和c字符串的区别","link":"/posts/3631682412.html"},{"title":"redis命令","text":"基础命令 1234567891011# nx 都是在key不存在的时候进行设置，通过这个可以实现最简单的分布式锁set key value nx#统计字符串数量strlen#取字符串的一个片段getrange 开始 结尾#默认每次给一个key增加1，也可以设置增加的数值#博客点赞次数incr#默认给key减一decr 审计日志 可以用到append命令，key为当前日期，value为今天对机器的所有操作 网址的长链接转短链接","link":"/posts/3560590924.html"},{"title":"redis的string类型","text":"详细redis的数据类型 Stringwhat12345678910111213141516171819202122232425262728293031set name shy #设置name为shy的键值对get name #获取键为name的值keys * #查看所有键值对exits key名字 #返回1，表示key存在，0表示不存在move key名字 1 #1表示当前数据库，删除keyexpire key名字 10 #设置key名字的过期时间为10秒ttl key名字 #查看key名字的过期时间还剩多少秒type key名字 #key的类型append key名字 值 #向key的值追加数据strlen key名字 #获取key的长度incr key #给key+1decr key #给key-1incrby key 步长 #key每次加多少decrby key 步长# key每次减多少getrange key 0 3 #截取字符串从0到3getrange key 0 -1 #查看所有内容setrange key 1 xx #将第1位的值替换为xxsetex（set with expire） #设置过期时间setex key3 30 “hello” #设置key3的值为hello，30秒后会过期setnx（set if not exist） #不存在设置setnx key3 “redis” #如果不存在key3，就创建mset key value #一次设置多个键值对mget key1 key2 #一次获取多个键的值msetnx 多键值对版本 #是一个原子性的操作，要么都成功，要么都失败 mset user:1:name zhangsan user:1:age 12#设置一个对象的两个属性 对象名：ID：属性名mget user：1：name user：1：age#获取一个对象的两个属性setget v1 redis#如果不存在v1，则返回空，但是将v1的值设置为redis#，如果存在，则返回v1的值，然后将v1的设置为redis whySDS采用sds结构来存储字符串，结果如下 redis中使用这样的sds的结构来构建字符串主要有以下几个原因： 当获取长度的时候，时间复杂度为0（1） 二进制安全，当比如用一个空字符串作为字符串中的一个特殊变量的时候，由于c中的字符串是通过\\0这个空字符来区分一个字符串是否结尾的，所以它只能用于保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据 杜绝缓冲区溢出（sds是通过预分配策略和惰性空间释放来减少的） 减少修改字符串时带来的内存重分配次数（sds是通过预分配策略和惰性空间释放来减少的） 可以使用c语言的一些函数，因为sds字符串的也是以\\0 作为结尾的，但是sds字符串是通过sds的len属性来确定这个字符串是不是结束的，c的字符串则是单一的通过\\0来确认一个字符串是否结束 预分配策略如果对SDS进行修改之后，SDS的长度（也即是len属性的值）将小于1MB，那么程序分配和len属性同样大小的未使用空间，这时SDS len属性的值将和free属性的值相同。举个例子，如果进行修改之后，SDS的len将变成13字节，那么程序也会分配13字节的未使用空间，SDS的buf数组的实际长度将变成13+13+1=27字节（额外的一字节用于保存空字符）。 如果对SDS进行修改之后，SDS的长度将大于等于1MB，那么程序会分配1MB的未使用空间。举个例子，如果进行修改之后，SDS的len将变成30MB，那么程序会分配1MB的未使用空间，SDS的buf数组的实际长度将为30MB+1MB+1byte。 惰性空间释放惰性空间释放用于优化SDS的字符串缩短操作：当SDS的API需要缩短SDS保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用。 sds和c字符串的区别 List常用语法在redis中可以将list玩成栈，队列，阻塞队列 list中所有的命令以l开头 12345678910111213lpush list one #将一个或多个值插入到列表头部（左）rpush list one # 将一个或多个值插入到列表尾部（右）lrange list 0 -1 # 获取list中的值！lpop list # 移除列表的第一个元素rpop #移除列表的最后一个元素lindex list 0 # 获取list的第0个值llen list # 获取list的长度lrem list 1 one # 移除list结合中指定个数的valueltrim list 1 2 #通过下标截取指定的长度rpoplpush list list1 #移除list列表中最后一个元素到新的list1列表中exists list #判断list是否存在lset list 0 item #将list【0】的值设为item 注意：如果list没有创建，会报错linsert list before/after ss # aa在list集合中的值ss的前面（或后面）aa why结构 特性 链表节点带有prev和next指针，获取某个节点的前置节点和后置节点的复杂度都是O（1） 表头节点的prev指针和表尾节点的next指针都指向NULL，对链表的访问以NULL为终点 带表头指针和表尾指针：通过list结构的head指针和tail指针，程序获取链表的表头节点和表尾节点的复杂度为O（1） 带链表长度计数器：程序使用list结构的len属性来对list持有的链表节点进行计数，程序获取链表中节点数量的复杂度为O（1） 链表节点使用void*指针来保存节点值，并且可以通过list结构的dup、free、match三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值 Sethash结构 zsetbitmap","link":"/posts/3588975288.html"},{"title":"redis持久化","text":"介绍redis的两种持久化方式rdb和aof rdbRDB方式，是将redis某一时刻的数据持久化到磁盘中，是一种快照式的持久化方法。 有两个Redis命令可以用于生成RDB文件，一个是SAVE，另一个是BGSAVE。 RDB文件的载入工作是在服务器启动时自动执行的，所以Redis并没有专门用于载入RDB文件的命令，只要Redis服务器在启动时检测到RDB文件存在，它就会自动载入RDB文件。 如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态。只 有在AOF持久化功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态。 服务器在载入RDB文件期间，会一直处于阻塞状态，直到载入工作完成为止。 saveSAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求： bgsaveBGSAVE命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程（父进程）继续处理命令请求： 在这个命令执行期间，服务器处理SAVE、BGSAVE、BGREWRITEAOF三个命令的方式会和平时有所不同。 客户端发送的SAVE命令会被服务器拒绝，服务器禁止SAVE命令和BGSAVE命令同时执行是为了避免父进程（服务器进程）和子进程同时执行两个rdbSave调用，防止产生竞争条件 客户端发送的BGSAVE命令会被服务器拒绝，因为同时执行两个BGSAVE命令也会产生竞争条件。 BGREWRITEAOF和BGSAVE两个命令不能同时执行 如果BGSAVE命令正在执行，那么客户端发送的BGREWRITEAOF命令会被延迟到BGSAVE命令执行完毕之后执行。 如果BGREWRITEAOF命令正在执行，那么客户端发送的BGSAVE命令会被服务器拒绝。 在redis.conf的配置文件中，就有这样的条件来触发bgsave的命令 上面的意思我那第一个来举例说明 服务器在900秒之内，对数据库进行了至少1次修改。 只要满足上面任意一个条件都会触发bgsave的命令 问题来了，怎么自动触发bgsave那？ 肯定是要有一个方法来触发的，就是服务器周期性操作函数serverCron，默认每隔100毫秒就会执行一次，其实的一项工作就是检查save选项所设置的保存条件是否已经满足，如果满足的话，就执行BGSAVE命令。 结构aofAOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态的 实现AOF持久化功能的实现可以分为命令追加（append）、文件写入、文件同步（sync）三个步骤。","link":"/posts/3707414788.html"},{"title":"redis集群配置","text":"单机基本配置 12345678910111213141516171819202122232425#将redis目录下面utils中的redis_init_script复制到/etc/init.d/目录下，并且改为为redis_端口号cp redis_init_script /etc/init.d/redis_6379mkdir /etc/redis #存放redis的配置文件mkdir /var/redis/6379 #存放redis的持久化文件修改redis.conf拷贝到/etc/redis/6379.conf中#修改redis.conf中的配置环境为生产环境daemonize yes # 让redis以后台方式启动pidfile /var/run/redis_6379.pid #设置redis的pid文件位置port 6379 #设置redis的监听端口号dir /var/redis/6379 #设置持久化文件的存储位置#在任何目录下都可以使用redis_clisudo cp src/redis-cli /usr/local/bin/ sudo cp src/redis-server /usr/local/bin/ #启动rediscd /etc/init.d/chmod 777 6379.conf./redis_6379.conf start#让redis开机自动启动，在redis_6379的最上面加入下面两行注释#chkconifg:2345 90 10#description:Redis is a persistent key-value database#--------------------------#在当前目录下执行下面这个命令chkconfig redis_6379 on#查看redis是否启动ps -ef |grep redis 主从复制配置master在redis的配置文件中，即/etc/redis/6379.conf 12bind 自己的ip地址requirepass 密钥（自己随意设置即可） slave123bind 自己的ip地址masterauth 上面master设置的密钥replicaof（比较旧的版本是slaveof） aster的IP地址 端口号（一般默认为6379） 启动redis1234567注意：先启动master再启动slave#启动redis 进入到/etc/init.d/目录下./redis.6379 start#进入redis,设置了requirepass的服务器需要输入密钥，没有设置的不需要，也就是说master是需要输入，而slave不需要输入密钥 redisl-cli -h 当前服务器的ip地址 -a 设置的密钥 #在服务器中可以查看主从配置信息 info replication master info replication slave info replication 问题ping通，但是telnet不通 如果你在从的redis中看见master_link——status 状态是down，可能是telnet到master的端口不通，执行telent master ip地址 redis设置的端口号（默认是6379） 如果发现没有telnet命令，需要执行 yum install -y telnet 如果yum出现问题，修改yum的源文件，vim /etc/yum.repos.d/ epel.repo将下图中的metalink注释起来，将baseurl取消注释，下图是修改后的结果 telnet安装之后，还是slave 可以ping通master，但是telnet不通，这种情况一般就是服务器的防火墙的问题，可以选择直接将服务器关闭123456#关闭开机自动启动防火墙systemctl disable firewalld.service #查看防火墙的状态firewall-cmd --state#临时关闭防火墙systemctl stop firewalld.service","link":"/posts/2793461167.html"},{"title":"rocketmq","text":"单机最多10万并发 阿里开发 集群部署架构图 原理 高可用主从架构及多副本策略 Borker有主和副之分，master broker 主要负责写入 master borker 收到消息之后会同步给slave broker ，保证一个机器宕机之后，另一个机器有数据 slave broker固定时间会从master broker拉取数据，这就是所谓的主从同步 每个broker启动都要向所有的nameserver进行注册。 因为如果只是向个别nameserver进行注册的话，当一台nameserver宕机之后，就会丢失broker的信息 nameserver和borker之间有个心跳机制，保证当broker宕机之后，nameserver会及时感知到 每隔固定时间，broker会向nameserver发送心跳，nameserver会将这个最新心跳时间更新 nameserver每隔固定时间来扫描所有broker的心跳时间，如果超过一个数据，将认为这个broker宕机 心跳传输的时候，还会降 消息获取来源消息获取可能来自于master broker，也可能来自于slave broker broker将消息返回给请求系统的时候，会向系统建议下一次消息请求要请求master broker或者slave broker， 重点rocketmq4.5之前不是完全的高可用模式，当master broker宕机之后，不会自动切换到slave broker，需要人手动修改 4.5之后，采用一种Dledger的机制来支持master broker宕机之后自动切换到slaver broker Dledger采用的是raft算法 下载安装1234567#下载rocketmqhttps://github.com/apache/rocketmq#下载dledger https://github.com/openmessaging/openmessaging-storage-dledger #可视化界面https://github.com/apache/rocketmq-externals 保证消息不丢失发送消息到mq零丢失 同步发送消息+反复多次重试 rocketmq事务消息机制，这个整体效果会更好一点 mq收到消息之后零丢失开启同步刷盘+主从架构同步机制 将数据写入磁盘后，并且将数据写入到slave broker的磁盘之后才返回给生产者，消息写入mq成功 消费者收到消息之后零丢失rocketmq天然就保证了，因为rocketmq默认就是当消息处理之后才会返回给mq消息发送成功，而不是在执行消息处理逻辑之前就将成功的消息返回给mq了 幂等性避免对同一 业务方法判断，当重试的时候，提前发送一条消息到mq中查询这个条消息是否已经发送过了，如果有则不再发送，没有则发送 状态查询 你写入到mq一条消息，将消息也写入到redis中，写入id和订单状态，当接口重复调用的时候，就去redis中年查询一下，根据id查询状态，成功则不再发送，失败则再次发送 redis这种方案有缺陷，还有可能会重复消费 就是你将消息发送到mq，没有来得及写redis，redis宕机了，重启之后会将重新发一次消息，这样就有两条消息了；所以一般来说推荐使用业务方法来进行判断 重复消息会有专门的重试队列，最多重试16次，16次之后进入死信队列，死信队列的处理方式是自定义，看业务需求 当业务由于某种原因故障不能消费消息时,可以返回reconsume_laster,将消息加入到延时消息consumerGroup中进行消息的重试最大15次阶梯型重试,15失败后放入到死信队列中消费者则专门开启线程进行消费 消费者消费者消费消息的方式有两种，一种是push，broker主动向consumer不定时发送消息；一种是pull，consumer不定时从broker拉取消息 push本质上也是消费者不停向broker发送消息拉取数据 消费者在处理完一批消息之后，会立马发送请求到broker拉取消息，看起来好像是broker不定时向消费者推送信息一样，其实是消费者不停向broker发送消息拉取数据 当请求发送到broker的时候，发现没有消息可以消费，就会让这个请求线程挂起，默认挂起15秒，然后后台有一个线程不停地检查broker中是否有消息，有的话，会主动将请求线程唤醒，然后消费者拿到消息 pull保证消息不丢失消息如果发送到消费者了，但是可能消费者还没有真正消费消息，就宕机了，此时消息在系统缓存中，但是却返回给mq的消息是消费者消费消息成功了 同步机制 当消费者真正执行完消息的处理逻辑之后，再将成功的消息返回给mq 不能用异步机制，因为异步机制可能导致消费者还没有消费好消息的时候，已经将成功的消息给mq了 ，如果此时消费者宕机了，那么返回给mq的消息就是假的，虽然返回的是储成功的消息，但是实际上并没有成功 生产者同步发送消息发送消息给mq，等待mq返回结果，没有返回结果的话，就会卡在这里 异步发送消息发送消息给mq，不等待mq返回结果，cpu去干别的事儿了，等到mq返回消息后，代码会继续执行下去 单向发送消息发送消息给mq就可以了，不管mq是否返回信息 保证消息发送成功 half消息对消费者不可见 一般一个消息到mq之后会写入对应的topic/messageque/consumerqueue中，但是rocketmq识别到消息为half消息后，会将消息写入rocketmq的内部topic中，所以消费者对于half来说是不可见的 比如你买东西，已经付款到了订单系统，发了一次half发现mq没有返回消息，mq挂了，于是进行资金回退。 如果本地事务失败了，会让订单系统给mq发送一个rollback，表示我这里失败了，无法接受你返回的消息 如果rollback和commit失败了，由于mq里面的消息一直处于half状态，长时间没有回应之后就知道mq出现问题，这个时候需要判断下订单的状态是“已完成”吗 是的话，再次commit请求，不是的话，再次执行rollback请求， 如何执行rollback 将rollback记录写入到op_+topic,标记某个half消息是rollback的了 假设一直没有执行rollback或者commit，mq最多会调用15次接口来判断half消息的状态，如果15次之后还是没有知道half消息的状态，就会自动将消息标记为rollback half每次发送消息之前，就要发送half消息到mq，如果mq正常工作就会返回一个ok给生产者，生产者就可以发送真正的消息了，如果返回的不是ok，就表示mq有问题，此时就会进行消息回滚 上面其实有三个步骤 producer发half给mq mq给producer返回信息 producer进行下一步处理 上面三步其实都可能出现问题，那么如何保证不出现问题那，请继续往下看 针对以上三个步骤进行下面三个回复 如果producer发送half消息失败，会调用一个本地线程来查看half消息在限定时间内有消息返回，如果没有则就按回滚处理 如果mq给producer返回信息失败，mq会调用一个本地线程来查看half消息在限定时间内有消息返回，如果没有则就按回滚处理 如何确保half消息发送成功 消息写入到RMQ_SYS_TRANS_HALF_TOPIC 技巧可以将使用阿里的cannal的技术来同步mysql的binlog 一个topic的数据放在多个messagequeue上，实现分布式存储 持久化brokerbroker收到消息后会将所有消息顺序写入到磁盘中，叫做commitlog，会有一个参数来规定commitlog的最大容量，达到最大容量后会自动创建一个新的commitlog来进行写入 磁盘顺序写+os cache写入+os异步刷盘 broker收到消息之后并不是直接写入磁盘的，是将消息写入到系统缓存中，然后系统缓存不定时将消息写入磁盘 异步刷盘会有数据丢失的风险，比如将数据写入到系统缓存之后，系统突然宕机了，生产者以为将消息已经写入了，但是实际上并没有写入到磁盘中 同步刷盘的意思就是每次必须将数据写入到磁盘中以后才叫做消息发送完成 优化文件预热 madvise系统调用，会尽可能能将数据从磁盘空间加载到内存中，减少数据从磁盘空间加载到内存的次数 mmap普通的将数据存储在磁盘的过程如下 需要进行两次拷贝 mmap只需要进行一次拷贝 原因：就是把磁盘文件地址和线程私有空间做了一个映射，一旦写入到虚拟内存后，直接拷贝到磁盘空间中即可，不用二次拷贝了（即拷贝到线程私有空间，再拷贝到磁盘空间中） messagequeue一个topic下面有多个messagequeeu，一个messagequeeu下只有一个consumerqueue messagequeue在broker的存储上是这样的 会存储在相应topic/messagequeeu0/consumerqueue0 每次broker收到消息之后，会将消息顺序写入磁盘，同时也会将这个消息的物理存储位置记录在topic/messagequeeu0/consumerqueue0中，这样方便消费者过来消费消息的时候，可以知道消息存放的位置 Dledgerwhat这个机制可以保证当leader broker失效的时候，可以自动切换到slave broker， why采用的是raft算法，简单来讲就是 所有的broker每个人都会投票给自己，第一轮所有人都会投票给自己，然后进行随机休眠，比如broker1休眠2秒，broker2休眠3秒，broker3休眠4秒，从数据来看肯定是broker1先苏醒，他投票给自己，将自己的投票发给其他人，剩下两个发现别人已经投票过了，于是跟随投票，所以broker就被选举上了，成为了leader broker 投票完成：机器数量/2+1，就表示大多数，就是说当有大多数人投票以后，不需要其他人发表意见，直接将大多人的意见作为最后的意见 broker投票机制 有人已经投票的话，会尊重他们的意见，跟随投票 会给自己投票 上面的顺序也是优先级的顺序 多副本同步分为两个阶段 uncommitted阶段 committed阶段 uncommitted阶段leader broker收到数据后，会标记为uncommitted状态，然后通过他自己的dledger server组件把这个数据发送给slave broker的dledger server committed阶段slave broker的dledger server收到消息之后，会回复一个ack给leader broker的dledger server，当leader broker收到一半以上的slave broker之后，会将数据标记为committed的状态 然后leader broker的slave broker将committed的状态发送给slave broker的dledger server消息同步完成 网络通信架构模型rocketmq的网络通信架构模型。首先生产者和服务端通过reactor主线程建立tcp长连接，客户端与服务端采用socketchannel进行通信，通过socketschannel发送消息，通过reactor线程池去监听socketchannel的消息到达。reactor线程池只负责把消息取出来，在消息被正式处理前需要加密验证，编码解码，网络连接管理通过worker线程池去做这些准备工作。再通过sendmessage线程池去发送消息。reactor主线程负责建立长连接reactor多线程并发监听消息请求达到。再通过worker多线程去处理消息，读写磁盘通过业务线程池处理，各个线程池执行时，不会影响其他线程池在其他环节处理请求。 reactor线程池采用aio多路复用 消息丢失三种情况 生产者发送消息的时候，由于网络故障或者master broker宕机导致broker没有收到消息，可以通过重试机制和备忘录机制多次发送失败后进行消息补偿 消息到达mq,rocketmq丢消息,当使用异步刷盘时可能消息对于的commit log还在page cache中未刷新到磁盘此时broker的物理机宕机了重启导致page cache中数据丢失,如果选择了同步刷盘消息存储到磁盘后也可能存在丢失当磁盘故障时,此时我们可以通过冗余备份磁盘的方式保证尽量丢少的消息 消息保存到mq,消费者消费消息时未进行ack让mq以为消息消费成功了跳到了下一个offset此时通过ack机制来保证消息不丢失","link":"/posts/1048239581.html"},{"title":"Spring之AOP","text":"文章摘要 目的AOP能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 概念 切面(Aspect)： Aspect 声明类似于 Java 中的类声明，事务管理是AOP一个最典型的应用。在AOP中，切面一般使用 @Aspect 注解来使用，在XML 中，可以使用 &lt;aop:aspect&gt; 来定义一个切面。 连接点(Join Point): 一个在程序执行期间的某一个操作，就像是执行一个方法或者处理一个异常。在Spring AOP中，一个连接点就代表了一个方法的执行。 通知(Advice):在切面中(类)的某个连接点(方法出)采取的动作，会有四种不同的通知方式： **around(环绕通知)，before(前置通知)，after(后置通知)， exception(异常通知)，return(返回通知)**。许多AOP框架（包括Spring）将建议把通知作为为拦截器，并在连接点周围维护一系列拦截器。 切入点(Pointcut):表示一组连接点，通知与切入点表达式有关，并在切入点匹配的任何连接点处运行(例如执行具有特定名称的方法)。由切入点表达式匹配的连接点的概念是AOP的核心，Spring默认使用AspectJ切入点表达式语言。 介绍(Introduction): introduction可以为原有的对象增加新的属性和方法。例如，你可以使用introduction使bean实现IsModified接口，以简化缓存。 目标对象(Target Object): 由一个或者多个切面代理的对象。也被称为”切面对象”。由于Spring AOP是使用运行时代理实现的，因此该对象始终是代理对象。 AOP代理(AOP proxy): 由AOP框架创建的对象，在Spring框架中，AOP代理对象有两种：JDK动态代理和CGLIB代理 织入(Weaving): 是指把增强应用到目标对象来创建新的代理对象的过程，它(例如 AspectJ 编译器)可以在编译时期，加载时期或者运行时期完成。与其他纯Java AOP框架一样，Spring AOP在运行时进行织入。 通知分类 前置通知(Before Advice): 在目标方法被调用前调用通知功能；相关的类org.springframework.aop.MethodBeforeAdvice 后置通知(After Advice): 在目标方法被调用之后调用通知功能；相关的类org.springframework.aop.AfterReturningAdvice 返回通知(After-returning): 在目标方法成功执行之后调用通知功能； 异常通知(After-throwing): 在目标方法抛出异常之后调用通知功能；相关的类org.springframework.aop.ThrowsAdvice 环绕通知(Around): 把整个目标方法包裹起来，在被调用前和调用之后分别调用通知功能相关的类org.aopalliance.intercept.MethodInterceptor 时期 编译期: 切面在目标类编译时被织入，这种方式需要特殊的编译器。AspectJ 的织入编译器就是以这种方式织入切面的。 类加载期: 切面在目标类加载到 JVM 时被织入，这种方式需要特殊的类加载器( ClassLoader )，它可以在目标类引入应用之前增强目标类的字节码。 运行期: 切面在应用运行的某个时期被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态创建一个代理对象，Spring AOP 采用的就是这种织入方式。 代理分类静态织入(AspectJ 实现)和动态代理(Spring AOP实现) AspectJApectJ 主要采用的是编译期静态织入的方式。在这个期间使用 AspectJ 的 acj 编译器(类似 javac)把 aspect 类编译成 class 字节码后，在 java 目标类编译时织入，即先编译 aspect 类再编译目标类。 不足 如果接口改了，代理的也要跟着改，很烦！ 因为代理对象，需要与目标对象实现一样的接口。所以会有很多代理类，类太多。 动态代理 JDK动态代理：Spring AOP的首选方法。 每当目标对象实现一个接口时，就会使用JDK动态代理。目标对象必须实现接口 CGLIB代理：如果目标对象没有实现接口，则可以使用CGLIB代理。 原理代理主要是使用JDK中的proxy类中的newProxyInstance方法来使用的 newProxyInstance方法的三个参数 参数一：生成代理对象使用哪个类装载器【一般我们使用的是代理类的装载器】 参数二：生成哪个对象的代理对象，通过接口指定【指定要代理类的接口】 参数三：生成的代理对象的方法里干什么事【实现handler接口，我们想怎么实现就怎么实现】 JDK动态代理CGLIB代理代码示例注解aop实现 123456789101112131415161718192021222324252627282930package cn.shiyujun.test;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;//将这个类注入到Spring容器中@Component//使用aOP@Aspectpublic class broker { @Before(&quot;execution(* cn.shiyujun.test.test01.service())&quot;) public void before(){ System.out.println(&quot;带租客看房&quot;); System.out.println(&quot;谈价格&quot;); } @After(&quot;execution(* cn.shiyujun.test.test01.service())&quot;) public void after(){ System.out.println(&quot;交钥匙&quot;); } @Around(&quot;execution(* cn.shiyujun.test.test01.service())&quot;) public void sayAround(ProceedingJoinPoint pjp) throws Throwable { System.out.println(&quot;注解类型环绕通知..环绕前&quot;); pjp.proceed();//执行方法 System.out.println(&quot;注解类型环绕通知..环绕后&quot;); }} 接口 12345package cn.shiyujun.test;public interface hexin { void service();} 接口实现类 123456789101112131415package cn.shiyujun.test;import org.springframework.stereotype.Component;//spring中bean的名字@Component(&quot;helloAOP&quot;)public class test01 implements hexin { @Override public void service() { // 仅仅只是实现了核心的业务功能 System.out.println(&quot;签合同&quot;); System.out.println(&quot;收房租&quot;); }} xml 123456789101112131415161718192021&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.shiyujun.aop_cglib&quot;/&gt; &lt;!-- 开启aop注解方式，此步骤s不能少，这样java类中的aop注解才会生效 --&gt; &lt;aop:aspectj-autoproxy/&gt; &lt;!-- 强制使用cglib代理，如果不设置，将默认使用jdk的代理，但是jdk的代理是基于接口的 --&gt; &lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt;&lt;/beans&gt; main类 123456789101112131415161718package cn.shiyujun.test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class main { public static void main(String[] args) { //这个是application容器，所以就会去所有的已经加载的xml文件里面去找，包括jar包里面的xml文件 ApplicationContext context=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); //通过ApplicationContext.getBean(beanName)动态加载数据（类）【获取Spring容器中已初始化的bean】。 test01 helloWorld=(test01) context.getBean(&quot;helloAOP&quot;); //执行动态加载到的类的方法 helloWorld.service(); }} 运行结果 xml","link":"/posts/2157337050.html"},{"title":"spring事务","text":"了解spring事务机制 what简单来讲，就是操作要么一起成功，要么一起失败；主要的目的是为了保证数据的一致性 事务传播机制 required 如果当前没有事务，就自动创建一个新的事务，如果当前存在事务，就加入该事务 supports 支持当前事务，如果当前存在事务，就加入该事务，如果没有，就以非事务的方式来执行 mandatory 如果当前有事务执行事务，如果没有事务会报异常 request_new 无论当前有没有事务都要创建一个新的事务 not_supported 以非事务的方式执行，如果存在事务，将该事务挂 never 以非事务的方式执行，如果存在事务，就抛出异常 nested 如果当前存在事务，会嵌套在事务内部执行（如果外层事务回滚，会导致内存层事务回滚，如果内层事务回滚，只会回滚自己的事务），相当于加入已经存在的事务当中，如果当前没有事务，则按required属性执行 how日常使用最多的应该就是@transactional 注解 注意有一个使用！@transcational失效的情况 失效情况链接","link":"/posts/2101472463.html"},{"title":"spring源码的gradle配置","text":"主要是修改init.gradle文件配置 全部内容如下 12345678910111213141516171819allprojects { repositories { maven { url &quot;https://maven.aliyun.com/repository/public&quot; } //有一个依赖，public中没有，所以需要spring maven { url &quot;https://maven.aliyun.com/repository/spring&quot; } maven { name &quot;ALIYUN_CENTRAL_URL&quot; // name 可以不需要 url 'https://maven.aliyun.com/nexus/content/repositories/central' } maven { name &quot;ALIYUN_JCENTER_URL&quot; url 'https://maven.aliyun.com/nexus/content/repositories/jcenter' } maven { name &quot;ALIYUN_GOOGLE_URL&quot; url 'https://maven.aliyun.com/nexus/content/repositories/google' } }}","link":"/posts/1798854186.html"},{"title":"sql语句","text":"DDL多表 尽量让多个表中都有索引，否则查询速度会很慢 inner join1select * from a,b where a.id=b.id 要求两个表的数据可以完全关联起来 连接可以放在where条件中 outer join 一般把连接条件放在on的后面 left join左侧的全部数据返回，无论右侧表中是否有 right join连接的全部数据返回，无论左侧表中是否有 nest-loop join嵌套循环关联 删除","link":"/posts/19189241.html"},{"title":"vagrant使用","text":"具体步骤 从这个地址下载centos7 https://github.com/tommy-muehle/puppet-vagrant-boxes/releases/download/1.1.0/centos-7.0-x86_64.box cmd，运行以下命令 1234#CentOs7是自定义的名字#E:/config/centos-7.0-x86_64.box是下载到本地的地址vagrant box add CentOs7 E:/config/centos-7.0-x86_64.box 运行以下命令 12#CentOs7是上面自定义的名字vagrant init CentOs7 注意：如果你之间已经执行过vagrant init这个命令，会报错，需要把相应的文件删除即可，一般文件会在你操作的目录下，我的是C:\\Users\\lenovo，删除这个Vagrantfile，再次执行vagrant init CentOs7即可 至此，已经安装成功，直接vagrant up启动即可 打开xshell输入相应信息 123127.0.0.1root vagrant端口：2222 使用xshell连接即可 若是要多个虚拟机做集群，需要在第4部之前修改Vagrantfile,然后下面vagrant up就可以了，下面是我的Vagrantfile配置，是三个虚拟机 123456789101112131415161718192021222324252627282930313233Vagrant.configure(&quot;2&quot;) do |config| config.vm.define &quot;vagrant1&quot; do |vb| config.vm.provider &quot;virtualbox&quot; do |v| v.memory = 1024 v.cpus = 1 end vb.vm.host_name = &quot;vagrant1&quot; vb.vm.network :public_network, ip: &quot;192.168.1.21&quot; vb.vm.box = &quot;Centos7&quot; end config.vm.define &quot;vagrant2&quot; do |vb| config.vm.provider &quot;virtualbox&quot; do |v| v.memory = 1024 v.cpus = 1 end vb.vm.host_name = &quot;vagrant2&quot; vb.vm.network :public_network, ip: &quot;192.168.1.22&quot; vb.vm.box = &quot;Centos7&quot; end config.vm.define &quot;vagrant3&quot; do |vb| config.vm.provider &quot;virtualbox&quot; do |v| v.memory = 1024 v.cpus = 1 end vb.vm.host_name = &quot;vagrant3&quot; vb.vm.network :public_network, ip: &quot;192.168.1.23&quot; vb.vm.box = &quot;Centos7&quot; endend 账号是root 密码是 vagrant 端口号每个不一样，非常好辨认 vagrant常用命令123456789101112131415161718#初始化配置；vagrant init#启动全部虚拟机；vagrant up：#启动单个虚拟机vagrant up 虚拟机名字#登录虚拟机；vagrant ssh：#挂起虚拟机；vagrant suspend：#：重启虚拟机；vagrant reload#关闭虚拟机；vagrant halt：#查看虚拟机状态；vagrant status：#删除虚拟机。vagrant destroy：","link":"/posts/4202801640.html"},{"title":"volatile","text":"三大特性：保证可见性，不保证原子性，禁止指令重排 不保证原子性测试123456789101112131415161718192021public class demo2 { private volatile static int num=0; public static void add(){ num++; } public static void main(String[] args) { for (int i = 0; i &lt; 100; i++) { new Thread(()-&gt;{ for (int j = 0; j &lt;1000 ; j++) { add(); } }).start(); } while (Thread.activeCount()&gt;2) { Thread.yield(); } System.out.println(Thread.currentThread().getName()+&quot;---&quot;+num); }} 不加lock和synchronized如何保证原子性使用原子类 代码示例 1234567891011121314151617181920public class demo2 { private static AtomicInteger num=new AtomicInteger(); public static void add(){ num.getAndIncrement(); } public static void main(String[] args) { for (int i = 0; i &lt; 100; i++) { new Thread(()-&gt;{ for (int j = 0; j &lt;1000 ; j++) { add(); } }).start(); } while (Thread.activeCount()&gt;2) { Thread.yield(); } System.out.println(Thread.currentThread().getName()+&quot;---&quot;+num); }} 这些类的底层都和操作系统挂钩！在内存中修改值！unsafe类是一个很特殊的存在！ 指令重排什么是指令重排指令重排：你写的程序，并不是按照你写的那样去执行的 处理器在进行指令重排的时候，考虑：数据之间的依赖性 源代码到代码执行的过程 源代码-&gt;编译器优化重排-&gt;指令也可能重排-&gt;内部系统也会重排 指令重排原理 内存屏障，Cpu指令、作用： 保证特定的操作的执行顺序！ 可以保证某些变量的内存可见性（利用这些特性volatile实现了可见性） 可见性作用下面会涉及几个CPU的术语 内存屏障：用于实现用户操作排列顺序的CPU指令 缓冲行：缓存的最小单位 有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，通过查IA-32架构软件开发者手册可知，Lock前缀的指令在多核处理器下会引发了两件事。 将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效 其实很简单，各个线程会有一个共享的主内存，读取数据都要从主内存读取，每个线程都有自己的内存，线程读取数据的时候就是把主内存的数据读取到线程私有的内存里面，当线程修改自己内存中的变量之后，会将修改的值更新到主内存中，此时其他线程私有内存中所保留的值全部失效，必须重新从主内存读取该变量 volatile中使用的是地址的引用，而非值的复制，因为如果是值的复制的话，当volatile中所修饰的值变得非常大之后，复制也是非常耗时的，可能就无法保证及时将修改后的数据及时协会到内存中 线程 JMM是个虚拟概念，在实际中其实就是放在cache中 那其他线程是如何知道自己缓存的数据有变化了那？ 这是通过硬件（处理器的嗅探机制）来实现的，不难猜出，所谓的嗅探机制就是当共享内存的变量被修改的时候，所有缓存该变量的线程中的值都将失效，必须要重新从共享内存读取才行 volatile优化缓存行数据为64字节，但是如果结点的数据不足64字节的话，自动填充到64字节可以提升效率 不能使用volatile变量是都应该追加到64字节吗？两种情况不允许 缓存行非64字节宽的处理器 共享变量不会被频繁的写","link":"/posts/4266433718.html"},{"title":"session，cookie和token","text":"三种的区别 http什么是无状态呢？就是说这一次请求和上一次请求是没有任何关系的，互不认识的，没有关联的。这种无状态的的好处是快速。坏处是假如我们想要把www.zhihu.com/login.html和www.zhihu.com/index.html关联起来，必须使用某些手段和工具 cookie和sessioncookie是session的一种实现方案 客户端访问服务器的流程如下： 首先，客户端会发送一个http请求到服务器端。 服务器端接受客户端请求后，建立一个session，并发送一个http响应到客户端，这个响应头，其中就包含Set-Cookie头部。该头部包含了sessionId。Set-Cookie格式如下，具体请看Cookie详解Set-Cookie: value[; expires=date][; domain=domain][; path=path][; secure] 在客户端发起的第二次请求，假如服务器给了set-Cookie，浏览器会自动在请求头中添加cookie 服务器接收请求，分解cookie，验证信息，核对成功后返回response给客户端 注意 cookie只是实现session的其中一种方案。虽然是最常用的，但并不是唯一的方法。禁用cookie后还有其他方法存储，比如放在url中 现在大多都是Session + Cookie，但是只用session不用cookie，或是只用cookie，不用session在理论上都可以保持会话状态。可是实际中因为多种原因，一般不会单独使用 用session只需要在客户端保存一个id，实际上大量数据都是保存在服务端。如果全部用cookie，数据量大的时候客户端是没有那么多空间的。 如果只用cookie不用session，那么账户信息全部保存在客户端，一旦被劫持，全部信息都会泄露。并且客户端数据量变大，网络传输的数据量也会变大 区别 session是保存在服务端的，cookie是保存在客户端的 tokentoken 也称作令牌，由uid+time+sign[+固定参数]token 的认证方式类似于临时的证书签名, 并且是一种服务端无状态的认证方式, 非常适合于 REST API 的场景. 所谓无状态就是服务端并不会保存身份认证相关的数据。 组成 uid: 用户唯一身份标识 time: 当前时间的时间戳 sign: 签名, 使用 hash/encrypt 压缩成定长的十六进制字符串，以防止第三方恶意拼接 固定参数(可选): 将一些常用的固定参数加入到 token 中是为了避免重复查库 存放token在客户端一般存放于localStorage，cookie，或sessionStorage中。在服务器一般存于数据库中 参考链接： https://segmentfault.com/a/1190000017831088","link":"/posts/3324693535.html"},{"title":"一个有序且元素重复的数组来进行去重排序","text":"一个有序数组，比如1 1 2 3 4 5 6 ，要拿到前五个不重复的长度 思路代码实现1234567891011121314151617public static int [] get(int [] res){ if (res.length &lt; 1){ return res; } int i = 0, r = i+1; while (r &lt; res.length){ if (res[i]==res[r]){ r++; continue; } i++; res[i]=res[r]; r++; } int[] ints = Arrays.copyOf(res, i + 1); return ints;}","link":"/posts/3653065266.html"},{"title":"一致性hash","text":"文章摘要 介绍一致性hash的来源和实现 来源为什么会出现这个东西那？ 主要是有了将数据均匀的分散到各个节点中，并且尽量的在加减节点时能使受影响的数据最少的这个需求，传统的hash满足不了，所以出现这个东西 实现范围首先是将所有的哈希值构成了一个环，其范围在 0 ~ 2^32-1，然后各个节点分布在这个环上 容错性 比如在n1服务器宕机的时候，依然根据顺时针方向，k2 和 k3 保持不变，只有 k1 被重新映射到了 N3。这样就很好的保证了容错性，当一个节点宕机时只会影响到少少部分的数据。 拓展性 在 N2 和 N3 之间新增了一个节点 N4 ，这时会发现受印象的数据只有 k3，其余数据也是保持不变，所以这样也很好的保证了拓展性。 虚拟节点出现这个东西的原因是因为，当服务器数量比较少的时候，会出现数据分布不均匀的情况 于是引入了虚拟节点 虚拟节点的意思其实就是一个key进行多次hash，得到的值，都在hash环上，但是这几个都是虚拟的机器，实际上还是存储在实际的机器上 参考文章： https://crossoverjie.top/2018/01/08/Consistent-Hash/ https://blog.csdn.net/suifeng629/article/details/81567777","link":"/posts/1557219409.html"},{"title":"为什么我要一个自己的博客？","text":"由来 &emsp;&emsp;其实我年初的时候就已经开启了一个博客，但是由于我在原来的那个电脑上胡乱折腾，导致用不了了，一度很伤心，当时就想着说有一天等我换电脑了，就重新运作起我的博客，这不一换电脑就立马折腾起来了。 &emsp;&emsp;对于博客，不知道为什么自己有一种执念，虽然目前市面上的博客平台很多，比如博客园、掘进等等，但是我还是想要有自己的博客，我仔细想了想自己到底是为什么对拥有一个自己的博客这么有执念那?结果就是想要在网络平台上有一块自己的小地方，它也许不需要很大的流量，也不会有很多人关注，但它可能是我心里的那一道光，一个记录我自己成长的地方。","link":"/posts/3919390837.html"},{"title":"互联网公司开发流程","text":"部署方案滚动发布这是最常见的部署模式,一般就是说你一个服务/系统都会部署在多台机器上,部署的时候，要不然是手动依次部署，最low 的比如就是每台服务器上放一个tomcat，每台机器依次停布机 tomcat，然后把新的代码放进去，再重新启动tomcat，各个机器逐渐重启，这就是最low的的滚动发布 手动部署 备份原来的代码 将新的代码部署上去 自动化部署一些自动化工具可以帮助我们自动化部署到机器上，比如Jenkins，它会从你指定的git仓库中拉取相应的代码，然后部署到指定的机器上 无论是手动部署，还是自动化部署，都需要考虑如果部署失败，如何进行版本回退 那么如何来看是否部署成功那？ 人工的话，就是去各个机器上看对应的日志就可以 使用jenkins的话，也会有日志打印，部署失败，会有相应提示 灰度发布灰度发布，指的就是说，不要上线就滚动全部发布到所有机器，一般就是会部署在比如1台机器上，采用新版本，然后切比如10%的流量过去，观察那10%的流量在1台机器上运行-段时间，比如运行个几天时间，观察日志、异常、数据，是否一切正常，如果验证发现全部正常，那么此时就可以全量发布了 全量发布还是采用的滚动发布的模式 蓝绿发布蓝绿部署的意思是说，你得同时准备两个集群，一个集群放新版本代码，一个集群放老版本版权，然后新版本代码的集群准备好了过后，直接线上流量切到新版本集群上去，跑一段时间来验证，如果发现有问题，回滚就是立马把流量切回老的集群，回滚速度是很快的 总结在进行一些bug修复，或是改动不大的小版本的话，建议使用灰度发布； 如果是大版本的发版，建议使用蓝绿发布，防止部署失败，大规模服务失效的情况","link":"/posts/2964921422.html"},{"title":"几种通信方式的区别和联系","text":"http websocket socket webservice这四种方式的区别和联系 httpHTTP 是基于请求响应式的，即通信只能由客户端发起，服务端做出响应，无状态，无连接的协议 http是最初的产生的系统间进行通信的协议 websocketwhy为什么会出现这个东西？ 是因为在一些特定场景下，需要让服务端主动来发送请求，而不是只能让客户端进行轮询，比如我们想了解今天的天气，只能是客户端向服务器发出请求，服务器返回查询结果。HTTP 协议做不到服务器主动向客户端推送信息。 这种单向请求的特点，注定了如果服务器有连续的状态变化，客户端要获知就非常麻烦。我们只能使用”轮询”：每隔一段时候，就发出一个询问，了解服务器有没有新的信息。最典型的场景就是聊天室。 socketservice 参考文章： https://blog.csdn.net/miao_9/article/details/85286025","link":"/posts/2321636764.html"},{"title":"分布式锁","text":"分析分布式锁的实现 来源为什么需要分布式锁？ 主要是由于在单服务器系统我们常用本地锁来避免并发带来的问题，但是，当服务采用集群方式部署时，本地锁无法在多个服务器之间生效，这时候保证数据的一致性就需要分布式锁来实现。 特征一个相对安全的分布式锁具备什么特征？ 互斥性。互斥是锁的基本特征，同一时刻锁只能被一个线程持有，执行临界区操作。 超时释放。通过超时释放，可以避免死锁，防止不必要的线程等待和资源浪费，类似于 MySQL 的 InnoDB 引擎中的 innodblockwait_timeout 参数配置。 可重入性。一个线程在持有锁的情况可以对其再次请求加锁，防止锁在线程执行完临界区操作之前释放。 高性能和高可用。加锁和释放锁的过程性能开销要尽可能的低，同时也要保证高可用，防止分布式锁意外失效。 实现方式 Memcached 分布式锁 利用 Memcached 的 add 命令。此命令是原子性操作，只有在 key 不存在的情况下，才能 add 成功，也就意味着线程得到了锁。 Zookeeper 分布式锁 利用 Zookeeper 的顺序临时节点，来实现分布式锁和等待队列。ZooKeeper 作为一个专门为分布式应用提供方案的框架，它提供了一些非常好的特性，如 ephemeral 类型的 znode 自动删除的功能，同时 ZooKeeper 还提供 watch 机制，可以让分布式锁在客户端用起来就像一个本地的锁一样：加锁失败就阻塞住，直到获取到锁为止。 Chubby Google 公司实现的粗粒度分布式锁服务，有点类似于 ZooKeeper，但也存在很多差异。Chubby 通过 sequencer 机制解决了请求延迟造成的锁失效的问题。 Redis 分布式锁 基于 Redis 单机实现的分布式锁，其方式和 Memcached 的实现方式类似，利用 Redis 的 SETNX 命令，此命令同样是原子性操作，只有在 key 不存在的情况下，才能 set 成功。而基于 Redis 多机实现的分布式锁 Redlock，是 Redis 的作者 antirez 为了规范 Redis 分布式锁的实现，提出的一个更安全有效的实现机制。 redis分布式锁setnx最简单的加锁方式就是直接使用 Redis 的 SETNX 指令，该指令只在 key 不存在的情况下，将 key 的值设置为 value，若 key 已经存在，则 SETNX 命令不做任何动作。key 是锁的唯一标识，可以按照业务需要锁定的资源来命名。 为了防止资源被长期占用，所以需要设置一个过期时间 由于setnx和expire不是原子性的操作，所以还是存在这个过期时间没有加上的原因，资源还是被长期占用了 所以可以用redis的扩展命令 NX 表示只有当 lock_resource_id 对应的 key 值不存在的时候才能 SET 成功。保证了只有第一个请求的客户端才能获得锁，而其它客户端在锁被释放之前都无法获得锁。 EX 10 表示这个锁 10 秒钟后会自动过期，业务可以根据实际情况设置这个时间的大小。 但是这种方式仍然不能彻底解决分布式锁超时问题： 锁被提前释放。假如线程 A 在加锁和释放锁之间的逻辑执行的时间过长（或者线程 A 执行过程中被堵塞），以至于超出了锁的过期时间后进行了释放，但线程 A 在临界区的逻辑还没有执行完，那么这时候线程 B 就可以提前重新获取这把锁，导致临界区代码不能严格的串行执行。 锁被误删。假如以上情形中的线程 A 执行完后，它并不知道此时的锁持有者是线程 B，线程 A 会继续执行 DEL 指令来释放锁，如果线程 B 在临界区的逻辑还没有执行完，线程 A 实际上释放了线程 B 的锁。 锁被误删除的解决办法： 就是在加锁时将 value 设置为一个唯一的随机数（或者线程 ID ），释放锁时先判断随机数是否一致，然后再执行释放操作，确保不会错误地释放其它线程持有的锁，除非是锁过期了被服务器自动释放。 但是由于判断 value 和删除 key 是两个独立的操作，并不是原子性的，所以这个地方需要使用 Lua 脚本进行处理，因为 Lua 脚本可以保证连续多个指令的原子性执行。 但是，上面这个处理方法还是没有保证锁可能被提前释放的问题，所以有了下面的这种redisson的处理方式 Redisson怎么能解决锁被提前释放这个问题呢？ 可以利用锁的可重入特性，让获得锁的线程开启一个定时器的守护线程，每 expireTime/3 执行一次，去检查该线程的锁是否存在，如果存在则对锁的过期时间重新设置为 expireTime，即利用守护线程对锁进行“续命”，防止锁由于过期提前释放。 当然业务要实现这个守护进程的逻辑还是比较复杂的，可能还会出现一些未知的问题。 目前互联网公司在生产环境用的比较广泛的开源框架 Redisson 很好地解决了这个问题，非常的简便易用，且支持 Redis 单实例、Redis M-S、Redis Sentinel、Redis Cluster 等多种部署架构。 Redlock出现这个锁的原因是，上面两种的分布式锁都只是作用在了是加锁时只作用在一个 Redis 节点上，即使 Redis 通过 Sentinel 保证了高可用，但由于 Redis 的复制是异步的，Master 节点获取到锁后在未完成数据同步的情况下发生故障转移，此时其他客户端上的线程依然可以获取到锁，因此会丧失锁的安全性。 参考文章： https://www.infoq.cn/article/dvaaj71f4fbqsxmgvdce","link":"/posts/1320368156.html"},{"title":"动态规划","text":"文章摘要 零钱兑换题目 答案1234567891011public static int change(int amount, int[] coins) { // int[] dp = new int[amount+1]; dp[0] = 1; for (int coin : coins) { for (int i = coin; i &lt;= amount; i++) { dp[i] += dp[i - coin]; } } return dp[amount];} 如何列出正确的状态转移方程 确定基础的例子 确定【状态】，也就是原问题和子问题中会变化的变量 确定【选择】，也就是导致【状态】产生变化的行为 明确dp函数/数组的定义。自定向下 备忘录","link":"/posts/2819424305.html"},{"title":"单例模式","text":"详细介绍单利模式 核心作用保证一个类只有一个实例，并且提供一个访问该实例的全局访问点 常见的五种单例模式的是实现方式 饿汉式（线程安全，调用效率高，不能延时加载） 懒汉式（线程安全，调用效率不高，可以延时加载） DCL懒汉式（由于JVM底层内部模型原因，偶尔会出现问题，不推荐使用） 饿汉式改进（静态内部类，线程安全，调用效率高，可以延时加载） 枚举单例（线程安全，调用效率高，不能延时加载） 饿汉式12345678910111213141516171819202122//饿汉式单例模式public class demo1 { //1. 私有化构造器 private demo1(){ }// 2. 类初始化的时候，立即加载该对象；由于用了static关键字，在类装载的时候就初始化对象，// 不存在并发问题，因为一加载就出来了 private static demo1 instance=new demo1(); //3. 提供获取该对象的方法;由于不存在并发问题，所以没有加synchronized关键字，效率高；如果考虑到并发情况，是要加synchronized关键字的； public static demo1 getInstance(){ return instance; }}//问题：如果在这个类中，加了一些开辟空间的方法，那么不管我用不用这个类，这个块空间都被占用了，//浪费了资源，理想情况应该是在调用getInstance方法的时候，再去开辟空间；class demo1Test{ public static void main(String[] args) { demo1 instance=demo1.getInstance(); demo1 instance1=demo1.getInstance(); System.out.println(instance==instance1); }} 懒汉式1234567891011121314151617181920212223public class demo2 { //1. 私有化构造器 private demo2(){ } //2. 类初始化的时候，不立即加载该对象 private static demo2 instance; //3. 提供获取该对象的方法，由于可能会出现有多个线程来进来的话，需要让他们排队，否则都有问题 // 所以有synchronized同步这个关键字，效率低！ public static synchronized demo2 getInstance(){ if(instance==null) { instance=new demo2(); } return instance; }}class demo2Test{ public static void main(String[] args) { demo2 instance=demo2.getInstance(); demo2 instance1=demo2.getInstance(); System.out.println(instance==instance1); }} DCL懒汉式123456789101112131415161718192021222324252627282930313233343536373839404142//DCL懒汉式public class demo3 { private demo3(){ } //2. 类初始化的时候，不立即加载该对象 private volatile static demo3 instance; //volatile是后来加的 //3. 提供获取该对象的方法，由于可能会出现有多个线程来进来的话，需要让他们排队，否则都有问题 // 所以有synchronized同步这个关键字，效率低！ //不用synchronized可以用synchronized代码块，锁demo3这个类本身；双重检测 //分析：现在不需要对整个方法进行同步了，将锁的范围变得更精细了，如果有个进程进来了，发现 //这个instance对象没有被创建，有一个锁，他首先要和其他进程竞争本类的锁，获得锁之后，再次检查， //如果还是null，说明他是第一个竞争到这个锁的，于是他这个线程就负责创建这个对象，其他的线程 //进来之后，直接调用即可 public static demo3 getInstance(){ if(instance==null) { synchronized (demo3.class){ if(instance==null) { instance=new demo3(); } } } return instance; }}//由于这个操作不是原子性操作，所以他会经过下面几个步骤//1.分配内存//2. 执行构造方法//3. 执行地址//可能会出现的问题：极端情况，一个线程进来了，走到instance=new demo3()，还没有出去方法，//结果另一个线程进来了，就会直接走到 return instance;，这样instance就是一个新的对象，//破环了单例模式，可能会发生一些意想不到的问题；在这种情况下加volatile关键字；//volatile可以保证一个线程在对这个变量进行修改的时候，另一个线程，该变量的缓存就失效了，直接读内存中的值class demo3Test{ public static void main(String[] args) { demo3 instance=demo3.getInstance(); demo3 instance1=demo3.getInstance(); System.out.println(instance==instance1); }} 静态内部类实现123456789101112131415161718192021222324//静态内部类实现public class demo4 { private demo4(){ } private static class InnerClass{ private static final demo4 instance=new demo4(); } public static demo4 getInstance(){ return InnerClass.instance; }}//反射机制可以破坏privateclass demo4Test{ public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException { demo4 instance=demo4.getInstance(); //通过反射拿到instance Constructor&lt;demo4&gt; demo4Constructor=demo4.class.getDeclaredConstructor(null); demo4Constructor.setAccessible(true); demo4 instance1=demo4Constructor.newInstance(); System.out.println(instance==instance1); System.out.println(instance.hashCode()); System.out.println(instance1.hashCode()); }} 优化静态内部类123456789101112131415161718192021222324252627282930313233343536public class demo5 { private demo5(){ synchronized (demo5.class){ if(instance!=null) { throw new RuntimeException(&quot;不要试图用反射破坏单例模式！&quot;); } } } //2. 类初始化的时候，不立即加载该对象 private volatile static demo5 instance; public static demo5 getInstance(){ if(instance==null) { synchronized (demo5.class){ if(instance==null) { instance=new demo5(); } } } return instance; }}class demo5Test{ public static void main(String[] args) throws Exception { demo5 instance=demo5.getInstance(); //通过反射拿到instance Constructor&lt;demo5&gt; demo4Constructor=demo5.class.getDeclaredConstructor(null); demo4Constructor.setAccessible(true); demo5 instance1=demo4Constructor.newInstance(); System.out.println(instance==instance1); System.out.println(instance.hashCode()); System.out.println(instance1.hashCode()); }} 枚举123456789101112131415//枚举//反射不能够破坏枚举public enum demo6 { INTERFACE; public demo6 getInstance(){ return INTERFACE; }}class demo6Test{ public static void main(String[] args) { demo6 anInterface = demo6.INTERFACE; demo6 anInterface2 = demo6.INTERFACE; System.out.println(anInterface==anInterface2); }}","link":"/posts/4049607742.html"},{"title":"实现一个简单的计算器","text":"文章摘要 两个栈1234567891011121314151617181920212223242526272829303132333435public int calculate(String s) { Deque&lt;Integer&gt; stack = new LinkedList&lt;Integer&gt;(); char preSign = '+'; int num = 0; int n = s.length(); for (int i = 0; i &lt; n; ++i) { //判断这个字符是否是数字 if (Character.isDigit(s.charAt(i))) { //字符和字符的加减法都是用的对应的ASCII来进行的，由于字符对应的ASCII码也是按照数字的大小来的，所以直接就相当于字符直接相加减 num = num * 10 + s.charAt(i) - '0'; } if (!Character.isDigit(s.charAt(i)) &amp;&amp; s.charAt(i) != ' ' || i == n - 1) { switch (preSign) { case '+': stack.push(num); break; case '-': stack.push(-num); break; case '*': stack.push(stack.pop() * num); break; default: stack.push(stack.pop() / num); } preSign = s.charAt(i); num = 0; } } int ans = 0; while (!stack.isEmpty()) { ans += stack.pop(); } return ans;} 一个栈12345678910111213141516171819202122232425262728public static double calculate(String s){ int len=s.length(); char[] str=s.toCharArray(); Stack&lt;Integer&gt; st_num=new Stack&lt;&gt;(); char op='#'; //记录乘除符号 int ans=0,sign=1; //记录加减符号 for(int i=0;i&lt;len;i++){ if(str[i]==' ') continue; if(str[i]&gt;='0'&amp;&amp;str[i]&lt;='9'){ int num=str[i]-'0'; while(i&lt;len-1&amp;&amp;str[i+1]&gt;='0'&amp;&amp;str[i+1]&lt;='9') //找完这个数 num=num*10+(str[++i]-'0'); if(op!='#'){ //如果之前有乘除符号 if(op=='*')num*=st_num.pop(); //则将这个数运算之后，再重新进栈 else num=st_num.pop()/num; op='#'; //重置乘除符号 } st_num.push(num); } else if(str[i]=='*'||str[i]=='/') op=str[i]; //更新乘除符号 else{ //遇到加减符号则可以直接更新ans了 ans+=st_num.pop()*sign; sign=str[i]=='+'?1:-1; //更新加减符号 } } return ans+st_num.pop()*sign; //sign更新后，还有一次未计算}","link":"/posts/4113357341.html"},{"title":"对于学历造假问题的思考","text":"今天在地铁上刷脉脉的时候，发现脉脉热点第一的话题就是这个，看了下面的各种评论，我有些认同，有些不认同，各种思维在我的脑海中不停的碰撞，所以我决定写下这篇文章来总结我的观点。 下面我从下面几点进行讨论。 学历首先我们来思考一下 学历是什么？ 学历是我们从哪所学校毕业的证明，是我们入职公司所必须要的一个东西。 学历可以证明什么？ 我认为学历是你过去在上学阶段的重要成果，可以证明你在过去的二十几年里，学习能力非常出色，理解能力很到位，从而可以在优秀的学校毕业。 那么问题来了，学历低或者甚至没有学历的同学，是否所有的人都是智商不行？是否所有的人都是学习能力不行？是否所有的人理解能力都很差，高学历就可以学会，而低学历却需要两三遍。 不是的，事实上，只是从概率来说，是从好学校毕业的学生整体素质比其他学校的高而已，仅此而已。 企业首先要思考的是，企业招聘人员的目的在哪里？毫无疑问，肯定是要帮助企业成长的人才。那么什么才算是人才那？在社会普遍认知来说，在学历高的人群中，优秀的人才相对来说概率更大。事实上也确实是如此，那么企业是否要把高学历作为招聘人才时候的唯一标准那？ 这个当然不对，因为这样的话，就表示了你传递了一个价值观，就是我只招聘那些在学生时代付出过努力并取得优秀结果的同学，这样就否认了人在工作时代的努力。 但是这就存在一个问题，就是如果企业我放开学历要求，那么招聘成本会急剧上升，一个企业招收的人也不多，可能一年光985毕业的学生都招收不过来，又怎么会要你低学历的人那？ 其实我想了一个折中方案，就是你如果确实对学校有要求，那么你可以写 比如 985,211学校优先，这个是一个加分点，但是不是一个必要条件，这样就既减少了招聘成本，又尊重了个人工作之后的努力。 所以，最终企业招人是有以下三个选择的 将高学历作为招聘人才的最低要求，硬性条件，不可跨越。这个选择相对来说招聘成本最低 将高学历作为招聘人才的加分项，而不是必要条件 不限制学历，一切以面试结果为准 造假其实我在脉脉上还看到一个种言论，就是说 他如果简历都能造假，那他工作之后那，如果还有许多类似的行为，要如果处理？ 其实我想说，这种言论，上刚上线，下面我们一条一条来分析。 首先，这个简历造假肯定是个不诚信的行为，这没有什么好说的，但是这没有必要上升到攻击人的层面 这个其实也是社会形式所逼迫的，因为很多公司你如果不学历造假，就没有面试的机会 能力有人说，面试的并不能证明能力，只能证明他背得多，学习的多，但是工作中不一定可以。 针对这种言论，我想说，那他起码愿意花时间在这个上面，对于这个面试有足够的重视，那么我就有理由相信他的实践能力。 通过面试一般来说是可以看出能力的，但是这也考验面试管的水平，因为如果面试官只是对理论知识进行提问，而忽视了实践环节，那么确实有很大可能会出现 面试可以，但是工作的时候却很差劲。 人性在脉脉上还有一种言论是 那些成天把985 211挂在嘴上的人，其实都是为了夯实一道围墙，为了让自己的行业加上门槛，好让自己活的更加舒服，不用那么卷？ 看到这个言论的时候，我很赞同。原因如下： 经常把自己的学校挂在嘴边的，一般都是不太自信的人，或者说是想要以此来形成自己的护城河的人，他和普通学校毕业的人的关系有点像资本家和打工人的关系。 资本家总是想榨干打工人的剩余价值，让打工人辛辛苦苦的加班，熬夜，就是为了给资本家买车买房，成为一个亿万富翁。资本家这种行为的其实也是想要为自己增加护城河，最终目的就是不要让我的员工之后和我一样有钱，优秀。但是这其实并不是资本家的主要目的，资本家的主要目的就是盈利，也就是俗称的赚钱。 所以说这种资本家给自己添加护城河的言论，并不真实，而是一种资本家主要目的的一个附加的结果。 为什么说是一个附件的结果那？ 很简单， 因为如果你每天的时间都在工作上面，那么你就不会时间来提升自己，那也就是说 你的市场竞争力在不断的下降，从而将来会面对，一旦一离开这家公司，可能就比较难找工作，当然如果你的业务有非常多优秀的地方，也是可以的，但是这仍然是需要你去总结和消化的，而不是做过了，就忘记了 还有人说，我要是一个二本毕业的，去了一个都是985毕业的公司，我一定会好好努力，要是去了都是专科毕业的公司，就直接走了。 这其实也是有问题的，就是说你的思维定性了，认为学校好的就是什么都好，而学校不好的则都不好。","link":"/posts/2367700560.html"},{"title":"开闭原则","text":"开闭原则 定义软件实体应该对扩展开放，对修改关闭，其含义是说一个软件实体应该通过扩展来实现变化，而不是通过修改已有的代码来实现变化 原则","link":"/posts/4110889468.html"},{"title":"情绪自由","text":"主要是来聊一下我对于情绪自由的看法。 今日唠叨 今天是周六，无聊的我又在家自己待了一天，三餐外卖，说是要学习spring源码的，但是总是看了一小会，就去看视频了，看看b站，看看LPL比赛，看看向往的生活，看看爱奇艺的自制节目《姐妹俱乐部》。 其实我感觉我和其他的男生有点不太一样，因为他们好像是都不太喜欢综艺节目，我还是比较喜欢综艺节目的，比如我最喜欢的一些语言类节目《脱口秀大会》、《奇葩说》、《吐槽大会》等。 这个《姐妹俱乐部》是我最近追的综艺之一，主要就是来讲述一些关于女性在生活中的困境的。从节目的名字来讲，明显是针对于女性用户来开放的，但是也并没有限制男性观众观看，一开始时出于好奇心，看到这个节目，后来发现，挺有意思的，小小小小情景剧一些场景非常的夸张，最后也可以有一个小小的思考，于是我就开始追这个综艺了，几乎每期都看。 主要内容这周是第七期，讲的主题是“她的笑，她的哭”，注意了，朋友们，女字旁的她，强调女性，内容主要还是以三位老板为主，讲解了在生活中一些想要自由的哭和笑的时候，却遭到了他人的阻拦，或者说在这个社会 的压力或者这个场景的压力下，而不得不压抑自己的情绪。 如果想要了解这个视频的朋友，可以自己去看下，有些东西可能是需要看视频来自己体会的 下面主要是来谈谈我的理解。 情绪自由这个主题我认为还是挺有意思的，有句话说的好，既然我们都很难达到财富自由了，难道连情绪自由都要放弃吗？ 其实这个问题恒久存在，一直是在对我们自己的情绪进行限制，尤其是我们中国人，饱受传统思想的熏陶，经常性的压抑自己的情绪，比如说在家庭中，父母看起来总是很坚强，似乎没有见过父母哭的样子，或者说是父母不想把自己脆弱的一面留给孩子。 母亲的哭泣写到这里，突然想起来了，我似乎见过我母亲哭泣的样子，如果我记忆没有错的话，应该还是我把我的母亲弄哭的。 本来想写，我妈的，但是感觉太过口语化，于是改成了母亲 在我记忆不太清晰的样子中，似乎是在我初中回家的一个下午，我当时饥肠辘辘的回到家中，急忙需要食物来填满我的肚子，但是我的母亲，不太记得是因为什么原因了，没有做我想吃的饭，又或者是根本没有做饭，在我请求了我的母亲很长的时间之后，她还是不去为我做饭。 而我直到现在都还清晰的记得，那时的我，经历了饿的不同阶段，一开始是有点饿，慢慢越来越饿，直到这个饿的感觉达到一个顶峰，那个顶峰是真的难受，确实是有种饿到肚子疼的感觉，而那个时候，我的饭还迟迟没有着落，我也只能硬生生扛过那个时间，过了那个顶峰之后，慢慢的就越来越感觉到，似乎自己没有那么饿了。 当我终于辛苦的扛过自己的最饿的时候，我的母亲又过了一会才做好饭，端上来给我，而那个时候的我傲娇的很，由于自己比较恨母亲没有早点做好饭，于是就一直在拒绝母亲的喂饭，我已经记不清楚母亲是喂了我自己饭，而当时的我却是一直在拒绝，可能是因为担心我没有吃饭饿着，又或者是因为其他，那是我人生中第一次看到我的母亲在我的面前流泪了，我突然就慌了，不知道该如何自处，只是记得我最后好像流着泪吃完了那个饭，记忆非常的深刻。 这段母亲的哭泣，是在写文章的时候，临时起意，希望大家不要介意。 正文情绪自由，我认为对于很多人还是有思考的意义的，比如说我自己吧，由于小的时候，不知道因为一次什么原因，就给自己定了一个小的目标，就是以后不再哭泣。当时可能是觉得哭没有用吧，所以就不想要自己遇到事情再哭了，那并不能解决问题啊。虽然是不知道什么时候制定的目标，但是我自己回想过去的几年中，似乎确实不太有哭泣，不知道是不会了，还是说已经习惯了这种没有哭泣的生活。说起来，还有点小悲伤，一个正常的人竟然不会哭泣了，哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈。我仔细的回想，发现我好像虽然不会因为生活中的事情而哭泣了，但是不知道是因为年龄大了(但是一想，我97年，今天2021年，算一下也就24岁)，变得更加感性了，还是说其他什么原因，我竟然有一次看的一个动作戏哭了，真是发现自己确实比以前感性了，看待事情也确实比以前更加柔和了，不知道这是好是坏。 从那个时候，貌似我给自己的座右铭就定位了，用微笑面对生活 我对于情绪自由的理解其实很简单，就是接纳我们自己，接纳自己的这些正常的情绪，如果你可以允许自己笑，当然也可以允许自己哭泣，这都是我们身为一个正常的人类所拥有的一个正常的反应。 说起来很简单，接纳我们自己的情绪，但是我们常常因为社会的环境，或者我们的传统思想而不自觉地去限制自己的情绪，这是需要我们意识到的，慢慢的来学会接纳自己，学会释放自己的情绪，而不是过分的压抑。 朋友有一个非常经典的俗语是“爱笑的女孩运气都不会太差”，这个我是听过非常多次了，本来以为就是觉得大家都喜欢爱笑的女孩，但是看了今天的视频才知道，原来这也有另外一层意思啊，就是说爱哭的女孩运气比较差喽，我也是醉了，在如今思想解放的今天，女性的思想也越来越开放，这本应该是一个好的事情，但是什么事情都有一个度，这个度没有把握好，就出现了所谓的女权了。来继续爱笑的女孩这个话题，在朋友的关系中，我们可以喜欢看到女孩的笑容，但是不应该去强迫女孩露出笑容，这其实干涉到了女孩的情绪自由，比较过分，而且当女孩意识过来的时候，可能已经没有做朋友了，因为她不想再压抑自己的情绪了。 父母按照我们中国传统思想来说，作为子女的我们，都是认为父母已经为我们操心的很多了，当我们长大的时候，遇到困难或者说是困难的时候，总是喜欢报喜不报忧。不想要将自己的压力转移到父母的身上，而且他们可能对于这个问题的解决也并不能提供什么帮助。这种情况怎么说那，比较复杂，请听我分析一下。 这种情况对于我们子女自身而言，一方面不用给父母增加压力，报的都是开心的事儿，但是父母也不是傻子，不会那么容易就相信你生活中不会遇到什么困境，只是觉得你不给他们分享，另一方面，这种压力过大的时候，你自己可能控制不住，无意间泄漏或者父母从你的举止行为间看出来了，但是这种时候，父母通常都会问一下，你要真是不说，父母也没办法，其实我感觉遇到这种情况，讲一下，比较好，有句俗话话说的好，堵不如疏，将事情将讲清楚之后，父母才没有那么担心，虽然他们可能并不能提供一些具体的解决办法。","link":"/posts/3668139362.html"},{"title":"我的2020","text":"写文章的原因一方面是由于加入的技术群很多人都在写年终总结，另一方面也是自己觉得确实应该写一篇文章来记录一下这一年，记录这无法忽视的疫情，记录这已经逝去的校园美好时光 疫情 犹记得就在上一年的12月份学校放假回家过年了，那时谁能预想到竟然会有一场席卷全球的风暴来了，竟然会在家连续待了几个月，都不带出门的，当然我也庆幸自己12月份回家了，否则学校放假迟一点，就赶上疫情爆发，就回不去家了。 回顾我这2020年，由于疫情，在家连续待了几个月，考研也没有考上，于是自学java几个月，使用java做了一个毕设，然后就开始找工作了。 当时，都说2020年是最难的一年，都说这一年是很难找工作的，我考虑到自己的情况和自己又想转型做java，非常焦虑，害怕自己找不到工作，害怕毕业了还是要留在家里靠父母养活，感觉那样有点丢人。于是就开始我的找工作之旅，当时稚嫩的自己找了差不多一周吧，投了很多份简历在各种求职平台上，但是都没有什么回复，于是我就慌了，然后没几天之后，终于接到了一个电话，简单聊了几句之后，就可以去了，由于害怕自己找不到工作，于是就接受了这份工作，虽然不是做java开发的。这份工作很神奇，非常紧急，我记得我大概周五接的电话，周一就让到，到郑州之后，直接去广州出差，一入职就让去出差的公司，真的是闻所未闻，甚至我家人都以为这是个皮包公司，是骗人的，不是正经公司，结果到了，才发现，只是项目紧张而已。 不知不觉从在广州出差了两个月，这两个月几乎都是996，没想到我一毕业就体验了996的生活，不知道是该高兴还是该难过，反正那段时间是挺难熬的，而且在那个项目组，由于项目紧张，没有老同志教我，都是靠自己学习的，关键是不是目前使用的主流技术，技术问题不会了，在互联网上也搜索不到，非常难受。终于在出差了两个月后，回到了郑州，当天就赶紧去租了房（要不没有地方住），在郑州的生活现在回想起来还是挺惬意的，早上九点上班，由于住的地方离公司非常近，所以可以八点多起床，路上买个早餐，慢慢悠悠的地去公司。不好的地方在于 总是不能够6点准时下班，往往要拖时间，这让我很不爽，而且干的工作也不是我想要的，于是开始往北京的公司投简历，不知道是因为我没有来过北京，还是说因为它是祖国的首都，虽然其他城市我同样想去，但是感觉自己对北京有一种执念，所以投的北京的公司。由于是在职然后求职，所以也不是很着急，但是由于当时任务比较繁重，感觉自己搞不了了，也是想早点跳槽，正好在这个契机，一家北京的公司打来电话，又是简单的聊了聊，说他们不是做java开发的，是图形化开发，java作为底层，python作为上层，不用接触代码，但是自己想看也可以，于是我傻傻的急急忙忙的同意了，然后也是非常紧急，三天后就要在北京入职了，于是当天我就在公司提了离职，由于公司比较坑，按理来说我来实习的日子就应该算是在实习期内的，但是公司说必须拿到毕业证之后才能算做实习期，所以按照公司的方式来说，我还在实习期，所以三天之内就可以离职，所以我当天给公司提了离职在之后，一个在外出差的领导在逃避我，不跟我谈论这个话题，我很着急，所以第二天直接找了另外一个领导，交了电脑和相关资料后，就又匆匆忙忙的来北京了。 ​ 哎，怎么说那，我也不是非常有事业心的那种人，但是总是我觉得我还年轻，应该去外面闯荡一下，等到老了，有些东西了，会害怕失去了，就不会再有勇气想要去什么地方就去了。 在北京的工作平常是在银行里面驻场开发，整天就是画图，每一天都感觉很漫长，和上一家一样，技术问题都无法在网上查阅到，更惨的是银行是内网开发，根本都上不了外网，感觉每天都是在煎熬，都在期待着下班。本来想着工作之余，自己多学习学习，提升自己的技术，因为自己并不想在这家公司长待，只是一个过渡期，没想到十一月份，自己还没有主动辞职，反倒是被公司主动辞退了。那个时候非常难受，虽然我认为自己工作的还不错，但是由于那个小领导不满意，只能被迫辞职了，比较怂的我也没有什么保证金。 在被公司辞职找工作的时候，我明白了许多东西，懂的了家人的重要性，一生之中只有家人是不离不弃的，是我们永远的陪伴；真实且现实的认识到了金钱的重要性，哪里都需要钱，没有钱真的是寸步难行。非常感谢在我辞职期间，老妈老爸对我的支持，虽然他们工资不多，但是仍旧要帮我支付在北京的生活成本。 12月份真的就是一边学习，一边找工作，每天都在紧张的学习和面试中度过，一开始面试的时候还非常紧张，不知道该如何回答，后来就慢慢熟悉和了解了，知道了一些面试套路，由于面试的都是一些小公司，所以面试的问题并不是非常难，都是网上一些常见的面试题，对于项目和java基础问的比较多，我准备好的jvm都没有怎么问，最终我是拿到了两家的offre，虽然也没有多少钱，但是对于我这紧急找工作的人来说，算还可以了，总算是有份工作了，不需要再问爸妈要钱了。 目前是在新公司工作了一个月了，已经初步了解了公司的产品和开发流程，十来个人的小团队，用的说是springcloud，其实也就是用了个gateway，前端用的react，对于这一个月的开发，我分享一些收获。 深刻理解了大部分公司都是技术服务于业务，作为一名专业的程序开发人员，你不仅要非常熟悉开发工具和语言，同时还要熟悉业务，因为业务往往提的需求是非常模糊的，需要你从开发的角度来思考，思考需求应该提供一些别的信息，以此来避免开发过程中需求不明确的情况。 我们开发人员并不需要拘泥于某一种开发语言，而是要博采众长，需要什么，学什么，一种开发语言只是实现开发目的一种选择，并不是必要条件，只是相对来说更符合性能需求而已 自己平时需要学习一点技术东西，深入原理的学习，这样才可以保证自己不被后浪拍死在沙滩上 之前以为不就是日常学习吗？有什么时候难的，工作了之后才发现，工作了一天之后已经累的不行了，会会到家里只想躺着，根本没有什心思学习。所以我决定给自己降低期待，每天问问自己，今天有没有学到一些新的知识点，每天睡前都要问自己，“你是不是比昨天更博学了”","link":"/posts/1823279601.html"},{"title":"排序算法","text":"排序的基本操作：比较和移动 直接插入排序分为有序区和无序区，每一次从无序区里找一个最小的放在有序区 希尔排序先分组，两个数一组，比较交换，小的放在前面，一直这样比较 冒泡排序两个数比较最小的放在前面，第一轮排序结束，第一个数一定是最小的，每一次从无序区中选取一个最小的 快速排序随机选择一个数进行比较，一般情况下选择第一数为关键字，经过第一轮排序后，关键字前面都是比他小的，关键字后面都是比他大的 先从最后面开始比较，如果比关键字小，关键字和他换位置，然后从最前面往后扫描，比关键字大的，和关键字换位置，然后再从最后面开始扫描 简单选择排序每一次都从无序区中选取一个最小的放在最前面 堆排序每一次都是全部比较，最大的放在前面即根节点，然后输出根节点 堆排序的插入：插入一个数放在最后面，保证父节点的数比子节点的数大即可 堆排序的删除：删除一个数后，保证父节点的数比子节点大即可 归并排序先分组，组内排序 一开始是2个一组，四个一组，八个一组 基数排序不用进行关键字的比较，第一次比较按照个位数排列，第二趟按照十位数排列，第三位按照百位排列 基数排序的移动次数与关键字的排列次序无关 排列趟数和序列的初始状态无关的排序方法是 直接插入排序、简单选择排序、基数排序 每趟排列结束之后都至少能确定一个元素的最终位置的方法是 简单选择排序、快速排序、堆排序 最后一趟结束前，所有元素不一定归位：直接插入排序、希尔排序、 快速排序在原始序列无序的时候，速度最快","link":"/posts/1935562632.html"},{"title":"接口隔离原则","text":"接口隔离原则 接口分类 java中普通的interface是接口的一种 new 一个新类的时候，可以将其成为 实例化接口 定义 客户端不应该依赖它不需要的接口 类间的依赖关系应该建立在最小的接口上 我们可以把这两个定义概括为一句话：建立单一接口，不要建立臃肿庞大的接口。再通俗一点讲：接口尽量细化，同时接口中的方法尽量少 规则接口尽量小首先不能违反单一职责原则 接口要高内聚要求在接口中尽量少公布public方法，接口是对外的承诺，承诺越少对系统的开发越有利，变更的风险也就越少，同时也有利于降低成本 定制服务在进行系统设计的时候，要不同的系统间或者系统内不同的模块一定会产生耦合的部分，在这样的情况下，需要为各个访问者提供专用的接口，以确保一个类或者一个接口中提供多个不同访问者的方法的时候，权限没有控制好，导致一个访问者可以查询本来不属于它的方法 接口设计是有限度的接口的设计粒度越小，系统越灵活，这是不争的事实。但是，灵活的同时也带来了结构的复杂化，开发难度增加，可维护性降低，这不是一个项目或产品所期望看到的，所以接口设计一定要注意适度，这个“度”如何来判断呢？根据经验和常识判断，没有一个固化或可测量的标准。","link":"/posts/3773597826.html"},{"title":"树","text":"基础认识 二叉树二叉树，顾名思义就是一个结点有两个分叉就是二叉树 创建一个二叉树 12345678910public static class BinaryTreeNode { private int data; //节点的具体数据 private BinaryTreeNode leftChirld; //左孩子 private BinaryTreeNode rightChirld; //右孩子 private BinaryTreeNode(int x) { data=x; }} 满二叉树所有结点（除了叶子结点外）都有左节点和右节点 完全二叉树假设完全二叉树高度为k，则完全二叉树需要符合以下两点： 1）所有叶子节点都出现在k层或k-1层，并且从1~k-1层必须达到最大节点数。 2）第k层可以是不满的，但是第k层的所有节点必须集中在最左边。 平衡二叉树二叉搜索树红黑树 根节点是【黑色】 每个节点要么是【黑色】要么是【红色】 每个【红色】节点的两个子节点一定都是【黑色】 每个叶子节点（NIL）都是【黑色】 任意一个节点的路径到叶子节点所包含的【黑色】节点的数量是相同的—这个也称之为【黑色完美平衡】 新插入的节点必须是【红色】-&gt;为什么？如果新插入的节点是【黑色】，那不管是在插入到那里，一定会破坏黑色完美平衡的，因为任意一个节点的路径到叶子节点的黑色节点的数量肯定不一样了（第 6 点我自己加的，实际特性的定义是前 5 个 左旋 以某个节点作为固定支撑点（围绕该节点旋转）,其右子节点变为旋转节点的父节点，右子节点的左子节点变为旋转节点的右子节点，左子节点保持不变 右旋 以某个节点作为固定支撑点（围绕该节点旋转）,其左子节点变为旋转节点的父节点，左子节点的右子节点变为旋转节点的左子节点，右子节点保持不变 节点数1234567// 定义：count(root) 返回以 root 为根的树有多少节点int count(TreeNode root) { // base case if (root == null) return 0; // 自己加上子树的节点数就是整棵树的节点数 return 1 + count(root.left) + count(root.right);} 遍历DFS深度优先遍历 在我的理解中，其实深度优先遍历很简单，比如说是[1,2,3],就是随机选择一个点开始，比如说选择1，然后接着就是2和3随机选择一个，比如说选择2，最后就是3，路径就是[1,2,3] 算法实现思路： 首先所有数据的组合是一个数组，这个数组以树的方式类进行排列 树的话遍历的画需要知道第几层，于是path这个栈来了 123456789101112131415161718192021222324252627282930313233343536373839404142package algorithm;import java.util.*;public class demo { public static List&lt;List&lt;Integer&gt;&gt; s(int[] nums) { int lens=nums.length; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(lens==0) { return res; } //path 递归到第一层 //path 已经选了哪些树 //used 表示哪个树已经被触碰过了 Deque&lt;Integer&gt; path = new ArrayDeque&lt;Integer&gt;(); boolean[] used = new boolean[lens]; dfs(nums,lens,0,path,used,res); return res; } private static void dfs(int[] nums, int lens, int depth, Deque&lt;Integer&gt; path, boolean[] used, List&lt;List&lt;Integer&gt;&gt; res) { if(depth==lens) { res.add(new ArrayList&lt;&gt;(path)); return; } for (int i = 0; i &lt; lens; i++) { if(used[i]) { continue; } path.addLast(nums[i]); used[i]=true; dfs(nums,lens,depth+1,path,used,res); path.removeLast(); used[i]=false; } } }} 层次遍历12345678910111213141516171819202122232425262728 public static List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) { List&lt;List&lt;Integer&gt;&gt; result= new ArrayList&lt;&gt;(); if(root==null){ return result; } //创建队列，将root加入，建立第一层 Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); //遍历每一层节点的同时将下一层节点放进队列 while(!queue.isEmpty()){ int size = queue.size(); List&lt;Integer&gt; level = new ArrayList&lt;&gt;(); //遍历上层节点，拓展队列，将下层节点加到队列 for(int i=0;i&lt;size;i++){ TreeNode node = queue.poll(); level.add(node.val); if(node.left!=null){ queue.offer(node.left); } if(node.right!=null){ queue.offer(node.right); } } //将第x层遍历list放进最终大的list result.add(level); } return result;} 前序遍历快速排序其实用的就是 二叉树的前序遍历 归并排序用的是 分治思想 12345678public void inorder(TreeNode root, List&lt;Integer&gt; res) { if (root == null) { return; }; res.add(root.val); inorder(root.left, res); inorder(root.right, res); } 前中后序遍历中的前中后，讲的是根节点的位置，比如 前序遍历：先是根节点，接着左节点，最后右节点 中序遍历：先是左节点，接着根节点，最后右节点 后序遍历：先是左节点，接着右节点，最后根节点 中序遍历12345678public void inorder(TreeNode root, List&lt;Integer&gt; res) { if (root == null) { return; }; inorder(root.left, res); res.add(root.val); inorder(root.right, res); } 后序遍历12345678public void inorder(TreeNode root, List&lt;Integer&gt; res) { if (root == null) { return; }; inorder(root.left, res); inorder(root.right, res); res.add(root.val); }","link":"/posts/2065787451.html"},{"title":"红包算法","text":"题目给定一个红包总金额和分红包的人数，输出每个人随机抢到的红包数量。 要求: 每个人都要抢到红包，并且金额随机。 每个人抢到的金额数不小于1 每个人抢到的金额数不超过总金额的30% 例如总金额100，人数10，输出【19 20 15 1 25 14 2 2 1 1】 思路 首先是这个人数必须要大于等于4，否则直接都不满足第三个条件 new 一个数组，让这个数据的每个数据都等于1 new一个随机数在1到30之间 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546 public static void main(String[] args) { int total=0; int count=50000; for (int i = 0; i &lt; count; i++) { int packet = getPacket(100, 10); total=total+packet; } //这个是来测试这段代码的性能，基本上在count=50的时候，已经稳定在了15了，时间复杂度基本上可以说是1.5N，N就是人员的数量 System.out.println(total/count);// for (int i : packet) {// System.out.print(i+&quot;,&quot;);// } } public static int getPacket(int money,int number){ if(number &lt; 4 || money &lt; number){ throw new IllegalArgumentException(&quot;illegal argument&quot;); } int[] result = new int[number]; for (int i = 0; i &lt; number; i++) { result[i] = 1; } double threshold = money * 0.3; int remain = money - number; int index = 0; Random random = new Random(); int count=0; while (remain &gt; 0){ int i = random.nextInt(remain); if(i == 0){ i = 1; } double now = result[index] + i; if(now &lt;= threshold){ result[index] = (int)now; remain -= i; } index ++; count++; if(index == number){ index = 0; } } //System.out.println(count); return count; } 收获 在所有数据都有一个最小值的数据的情况下，可以假设所有的值都是最小值，在此基础至上，来进行数据处理 random.nextInt（x）的方法，返回(0,x],即大于0小于等于x","link":"/posts/4285113589.html"},{"title":"红黑树","text":"介绍红黑树 定义红黑树是一种含有红黑结点并能自平衡的二叉查找树。它必须满足下面性质： 性质1：每个节点要么是黑色，要么是红色。 性质2：根节点是黑色。 性质3：每个叶子节点是黑色。 性质4：每个红色结点的两个子结点一定都是黑色。 性质5：任意一结点到每个叶子结点的路径都包含数量相同的黑结点。 自平衡左旋以某个结点作为支点(旋转结点)，其右子结点变为旋转结点的父结点，右子结点的左子结点变为旋转结点的右子结点，左子结点保持不变 这个图 就非常的形象的表示了左旋的结果，其实我们理解左旋可以分为以下两点 旋转节点的右子节点变为它的父节点，左子节点不变，新的右子节点是原来右子节点的左子节点 旋转节点的右子节点 左子节点为 旋转节点；右子节点不变 套用到上图就是 pivot是旋转节点，y变为pivot的父节点，左节点还是a，右节点变为b y的左节点是pivot，右节点不变，还是c 右旋以某个结点作为支点(旋转结点)，其左子结点变为旋转结点的父结点，左子结点的右子结点变为旋转结点的左子结点，右子结点保持不变 同左旋刚好相反，左和右的互换，两个点 旋转节点的左子节点变为它的父节点，右子节点不变，新的左子节点是原来左子节点的右子节点 旋转节点的左子节点 左子节点不变；右子节点为旋转节点 变色结点的颜色由红变黑或由黑变红 插入空树当树是空的时候，插入的时候，直接将其作为根节点，并且是黑色的 插入的key已经存在了 当前节点的颜色不变 更改当前节点的值 插入节点的父节点为黑色节点由于插入的节点颜色是红色的，所以直接插入即可，无需自平衡 插入节点的父节点为红色节点 叔叔节点存在并且为红色节点 叔叔节点为空，且祖父节点、父节点和新节点处于一条斜线上。 叔叔节点为空，且祖父节点、父节点和新节点不处于一条斜线上。 第一种情况将父节点和叔叔节点与祖父节点的颜色互换 第二种情况将B节点进行右旋操作，并且和父节点A互换颜色 第三种情况将C节点进行左旋，这样就从第三种情况转换成第二种情况了，然后针对case 2进行操作处理就行 删除 参考文章： 美团红黑树深入剖析及Java实现 https://blog.csdn.net/v_july_v/article/details/6105630","link":"/posts/4171019779.html"},{"title":"缓存","text":"经典的缓存架构 可以支持百万流量的三级缓存 cache aside pattern 读数据的时候，先读缓存，缓存没有的话，就读数据库，放入缓存，返回响应 写数据的时候，先删除缓存，再更新数据库 为什么是删除缓存，而不是更新缓存那？ 原因很简单，试想这么场景，数据频繁的被修改，那么缓存就也要不断的重新计算，这个计算是非常消耗资源的，而且你更新了这么多次缓存还不一定能用到，就有点得不偿失了，所以就有了先删除缓存，再更新数据的做法，这样就是减少了非常多次的缓存重新计算的工作量，而是用到了数据的时候，再从数据库中加载到缓存中 数据库和缓存双写问题初级先写数据库，后写缓存，如果写缓存失败，就会出现数据库和缓存不一致的情况 解决方案：很简单，写数据的时候，先删除缓存，再更新数据库就可以了 其实就是从两个操作失败的场景来分析就会明白了 删除缓存失败，那么就会再次执行删除缓存的操作，可以自定义重试几次，删除换缓存失败的情况，缓存和数据库数据一直，删除缓存成功，会以数据库数据为主，将相应数据写入数据库，等到再次请求缓存，用到这个数据的时候，再将数据放入到缓存中 更新数据库失败，更新数据库失败就会回滚，此时缓存中没有数据，以数据库的数据为主，无非就是多次重试执行sql，直到成功或者说规定重试几次，返回更新数据失败的提示 进阶看完上面的解释，你是不是以为使用了先删除缓存，再写数据库的方式就可以避免数据库和缓存数据不一致的情况，我在这里告诉你，不是的！ 有一种场景，在一个线程删除缓存成功，准备将数据写数据库的时候，此时另外一个线程来请求这个数据，发现缓存中没有，直接去请求数据库，将数据库的数据读入到了缓存中，然后第一个线程才将数据写入数据库，此时缓存和数据库的数据出现了不一致的情况，数据库中是最新的数据，而缓存中是原来的数据。你以为已经给它删除了，但是如果一个线程出现上述这场景，就和没有删除的情况一样，仍然出现了缓存和数据库数据不一致的问题 解决方案： 数据库与缓存更新与读取操作进行异步串行化 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送一个jvm内部的队列中 读取数据的时候，如果发现数据不在缓存中，那么将重新拉取数据+更新缓存的造作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 一个队列对应一个工作线程 每个工作线程串行拿到对应的操作，然后一条一条的执行选择的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还米有完成更新 此时如果一个读请求过来，读到了空的缓存，那么可以优先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 这有有一个优化的点，一个队列中，其实多个更新缓存请求串在一个是没有意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的队列了，那么就不用再放一个更新的操作了，直接等待前面的更新操作完成 待那个队列对应的工作线程完成了上一个操作的数据的数据库的修改之后，才会去执行下一个操作，此时会从数据库获取最新的值，然后写入缓存，如果请求还在等待时间范围，不断轮询可以发现可以直接取到值了，那么直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的值 ​ 注意在进阶中的方案仍然有有一些问题需要实际考虑 由于读请求进行了非常轻度的异步化，所以一定要注意超时时间的问题，每个读请求必须在超时时间内返回 该解决方案最大的风险点在于，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库 务必通过一些模拟真实的测试，看看更新数据的频繁是怎样的 另外一点，因为在 一个队列中，可能会积压针对多个数据项的更新操作，因此要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据更新的操作，如果一个内存队列里居然会积压100个商品的库存修改操作，每个库存修改都要豪飞10ms完成，那么最后一个商品的去请求，可能要等待1s，这个时候就导致读请求长时间的阻塞 一定要根据实际业务系统的运行情况，去做一些压力测试，和模拟线上环境，去看看罪犯满给的时候，内存队列可能会挤压多少更新操作，如果读请求在200ms内返回，那还是可以接受的 如果一个内存队列中可挤压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的机器，那么就会提高读请求的响应时间 其实根据之前的项目经验，一般数据的写频率都是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的 多实例部署的请求路由可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务实例上 热点商品问题，导致请求的倾斜对某一个商品的请求特别高，全部请求到相同机器的相同的队列去了，可能会造成某台机器压力过大 缓存雪崩缓存雪崩这种场景，在缓存架构中是非常重要的一个环节，应对缓存雪崩的解决方案，避免缓存雪崩的时候，造成整个系统崩溃，带来巨大的经济损失 redis集群彻底崩溃 缓存服务大龄对redis的请求hang主，占用资源 缓存服务大量的骑牛打到源头服务去查mysql，直接打死mysql 源头服务因为mysql被打死也崩溃，对原服务的请求也hang主，占用资源 缓存服务大量的资源全部耗费访问redis和源服务无果，最后自己被拖死，无法提供服务 ng无法访问缓存服务，redis和源服务，只能基于本地缓存提供服务，但是缓存过期后，没有数据提供 网站崩溃 事前redis本身的高可用性，主从架构 建议双机房部署，可以是一套redis cluster，不同机器的，也可以是不同的redis cluster，两套redis cluster之间做一个数据同步，redis集群是可以搭建成树状结构的 事中ehcache本地缓存主要是为了应对redis中的数据被清除的现象和预防redis彻底崩溃 多台机器上部署的缓存服务实例的内存中，还有要ehcache的缓存 ehcache还能支撑一阵子 对redis访问的资源隔离目的：为了避免所有资源hang在redis上 对redis cluster访问失败的情况，做下熔断策略 什么时候判断redis死了，就自动给他熔断，部署redis cluster的降级策略 降级机制: fallback fail silent模式，failback里面直接返回一个空值，比如一个null，最简单了 对源服务访问的限流以及资源隔离资源隔离：限制访问商品服务的资源，避免商品故障的时候，所有资源都在访问该商品 目的：为了源头服务在mysql死掉的情况下，可以存活一阵 事后redis做了备份redis数据做了备份，直接根据redis的持久化策略来恢复数据 快速缓存预约缓存穿透其实很简单，就是你查询的数据，多级缓存中都没有，大量的数据都直接走到了数据库，容易导致数据库崩溃 缓存一致性","link":"/posts/2316692063.html"},{"title":"迪米特法则","text":"迪米特法则 定义迪米特法则（Law of Demeter，LoD）也称为最少知识原则（Least KnowledgePrinciple，LKP），虽然名字不同，但描述的是同一个规则：一个对象应该对其他对象有最少的了解。通俗地讲，一个类应该对自己需要耦合或调用的类知道得最少，你（被耦合或调用的类）的内部是如何复杂都和我没关系，那是你的事情，我就知道你提供的这么多public方法，我就调用这么多，其他的我一概不关心。 要求只和朋友类交流出现在成员变量、方法的输入输出参数中的类称为成员朋友类 要和朋友保持距离一个类公开的public属性或方法越多，修改时涉及的面也就越大，变更引起的风险扩散也就越大。因此，为了保持朋友类间的距离，在设计时需要反复衡量：是否还可以再减少public方法和属性，是否可以修改为private、package-private（包类型，在类、方法、变量前不加访问权限，则默认为包类型）、protected等访问权限，是否可以加上final关键字等。 注意 迪米特法则要求类“羞涩”一点，尽量不要对外公布太多的public方法和非静态的public变量，尽量内敛，多使用private、package-private、protected等访问权限 是自己的就是自己的在实际应用中经常会出现这样一个方法：放在本类中也可以，放在其他类中也没有错，那怎么去衡量呢？你可以坚持这样一个原则：如果一个方法放在本类中，既不增加类间关系，也对本类不产生负面影响，那就放置在本类中。 谨慎使用Serializable因为在使用vo类也就是实体类的时候，经常要序列化才行，如果类中的方法从原来的私有属性，改为了public，就可能会导致接口出现问题，但是这个问题应该是项目管理的问题，不可能服务端修改接口的相应代码，客户端没有接受到通知吧 总结迪米特法则的核心观念就是类间解耦，弱耦合，只有弱耦合了以后，类的复用率才可以提高。其要求的结果就是产生了大量的中转或跳转类，导致系统的复杂性提高，同时也为维护带来了难度。读者在采用迪米特法则时需要反复权衡，既做到让结构清晰，又做到高内聚低耦合。 一般来说，在实际应用中，如果一个类跳转两次以上才能访问到另一个类，就需要想办法进行重构了，为什么是两次以上呢？因为一个系统的成功不仅仅是一个标准或是原则就能够决定的，有非常多的外在因素决定，跳转次数越多，系统越复杂，维护就越困难，所以只要跳转不超过两次都是可以忍受的，这需要具体问题具体分析。","link":"/posts/3956989639.html"},{"title":"零拷贝","text":"主要是介绍零拷贝的两种技术原理 what零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。 实际上，零拷贝是有广义和狭义之分，目前我们通常听到的零拷贝，包括上面这个定义减少不必要的拷贝次数都是广义上的零拷贝。其实了解到这点就足够了。 我们知道，减少不必要的拷贝次数，就是为了提高效率。那零拷贝之前，是怎样的呢？ 传统IO 传统IO 将磁盘文件，拷贝到操作系统内核缓冲区 将内核缓冲区的数据，拷贝到应用程序的buffer 将应用程序buffer中的数据，再拷贝到socket网络发送缓冲区（属于内核缓冲区） 将socket buffer数据，拷贝到网卡，由网卡进行网络传输 传统IO方式，读取磁盘文件进行网络发送，经过4次数据拷贝。但是第2、3次的拷贝明显没有什么帮助。 传统IO存在多次无效拷贝，还伴随着大量的上下文切换。 MMAP 这种方式使用mmap()代替了read() 磁盘的数据通过DMA拷贝到内核缓冲区 操作系统把这块内核缓冲区与应用程序共享，避免了用户缓冲区和内核缓冲区的跨界复制 应用程序调用write()直接从内核缓冲区的内容拷贝到socket缓冲区 最后系统将socket的数据传输到网卡，由网卡进行传输 MMAP减少了一次拷贝，提升了效率，但是并不减少上下文切换的次数。 SendFile 这种方式是使用sendfile代替了read+write操作 首先sendfile系统调用，通过DMA引擎将磁盘文件拷贝到内存缓冲区 在内核缓冲区，内核将数据拷贝到socket缓冲区 最后，DMA将数据从内核拷贝到网卡，由网卡传输 数据总共发生3次拷贝 对比 都是Linux内核提供，实现零拷贝的API sendfile是将读到内核缓冲区的数据，直接转到socket buffer，进行网络发送 mmap是将磁盘文件读取到内核缓冲区后进行映射，和用户缓冲区共享数据，然后CPU在拷贝数据到socket buffer，进行网络发送 参考链接 kafka的零拷贝技术","link":"/posts/1857813313.html"},{"title":"JVM-2","text":"主要是用来介绍各种垃圾回收器的执行过程和优缺点 垃圾回收器的种类Serial收集器最早的垃圾回收器，是个单线程工作的收集器，但是他的单线程并不是真的垃圾会受到时候真的使用的是单线程，而是说进行垃圾回收的时候，会stop the world ，会停止所有的用户线程工作 特点： 垃圾回收的时候，会停止所有的用户线程进行工作 在资源受限的情况下，与其他收集器的单线程相比，效率依然是最高的 ParNew收集器其实这就是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外，其余的行为都和Serial收集器一样 特点： 除了Serial收集器外，目前只有它能与CMS收集器配合工作 一般来说是CMS和ParNew来一起使用，CMS 主要用来回收老年代，ParNew主要用来回收新生代 Parallel Scavenge收集器Parallel Scavenge收集器也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器 特点： 它主要关注的点是达到一个可控制的吞吐量 自适应调节策略：自动进行内存的调优 Serial Old收集器Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法 两种用途：一种是在JDK 5以及之前的版本中与Parallel Scavenge收集器搭配使用[插图]，另外一种就是作为CMS收集器发生失败时的后备预案，在并发收集发生Concurrent Mode Failure时使用 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现 在注重吞吐量或者处理器资源较为稀缺的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器这个组合 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器,是基于标记-清楚算法实现的 过程分为四个步骤： 初始化标记：就是标记GC ROOTs，静态变量和方法中的成员变量都数据GCROOTS，速度很快 并发标记 ：链路追踪，标记GGROOT 中引用其他的 重新标记：标记并发标记引用变动的对象 并发清理：并发清理掉可回收的内存，但是用户线程依旧在执行，所以会产生浮动垃圾 缺点： 对处理器资源比较敏感 无法处理“浮动垃圾” 由于使用的是标记-清理算法，故会产生大量的碎片空间 浮动垃圾：在CMS的并发标记和并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CMS无法在当次收集中处理掉它们，只好留待下一次垃圾收集时再清理掉。这一部分垃圾就称为“浮动垃圾” 实际处理 -XX:CMSlnitiatingOccupancyFaction 用来设置老年代占用多少比例的时候触发CMS垃圾回收 jsk1.6默认的值是92% 如果Cms垃圾回收期间，系统程序要放入老年代的对象大于可用内存空间，会发生Concurrent Mode Failure ，就是说并发垃圾回收失败了，我一遍回收，你一遍把对象放入老年带中，内存不够了 此时也会自动启用“Seral Old”垃圾回收器，就是直接把系统程序”Stop the World” ，重新进行长时间的GC Roots追踪，标记出来全部垃圾对象，不允许新的对象产生，然后一次性把垃圾对象都回收掉，完事儿了再回复系统线程 注意、 cms垃圾回收期也不是仅仅用“标记-清理”算法的，由于太多的内存随便实际上会更加频繁的full gc cms 有一个-XX：+UseCMSCompactAtFullCollection 默认开启 这个参数的意思是full gc之后要再次进行”Stop the World”，停止工作线程，然后进行碎片整理，就是把存活的对象挪到一起，空出来大片连续的内存空间，避免内存碎片 -XX：CMSFullGCsBeforeCompaction 这个参数的意思是执行多少期Full gc之后再执行一次内存碎片的整理工作，默认是0，意思就是每次full gc之后都会进行一次内存整理 Garbage First收集器简称G1收集器，它开创了收集器面向局部收集的设计思路和基于Region的内存布局形式 在JDK9中，，G1宣告取代Parallel Scavenge加ParallelOld组合，成为服务端模式下的默认垃圾收集器 垃圾收集的目标范围要么是整个新生代（Minor GC），要么就是整个老年代（Major GC），再要么就是整个Java堆（Full GC）。而G1跳出了这个樊笼，它可以面向堆内存任何部分来组成回收集（Collection Set，一般简称CSet）进行回收，衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大，这就是G1收集器的MixedGC模式 G1会将连续的Java堆划分为多个大小相等的独立区域（Region），每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的Region采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果 Region中还有一类特殊的Humongous区域，专门用来存储大对象。G1认为只要大小超过了一个Region容量一半的对象即可判定为大对象。每个Region的大小可以通过参数-XX：G1HeapRegionSize设定，取值范围为1MB～32MB，且应为2的N次幂。而对于那些超过了整个Region容量的超级大对象，将会被存放在N个连续的Humongous Region之中，G1的大多数行为都把Humongous Region作为老年代的一部分来进行看待 垃圾回收思路： 让G1收集器去跟踪各个Region里面的垃圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，根据优先级列表来进行垃圾回收 Shenandoah收集器低延迟收集器 Shenandoah相比起G1又有什么改进呢？ 支持并发的整理算法，而不是只会支持多线程回收并行 默认不使用分代收集的，没有实现分代，主要通过“性价比”来衡量垃圾回收的优先级 记录跨Region的引用关系发生改变，不使用耗费大量内存和计算资源去维护的记忆集，改用名为“连接矩阵” 工作过程： ZGC收集器 垃圾回收的类型部分收集（Partial GC）指目标不是完整收集整个Java堆的垃圾收集 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。另外请注意“Major GC”这个说法现在有点混淆，在不同资料上常有不同所指，读者需按上下文区分到底是指老年代的收集还是整堆收集 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾收集。 垃圾回收算法标记清除算法顾名思义标记所有需要回收的对象，统一回收被标记的对象 两个过程：标记过程和清理过程 标记过程其实就是判断对象是否需要回收的过程，也就是对象是否已死的过程 清理过程就是清理掉所有的已死对象 关注延迟的CMS收集器则是基于标记-清除算法的 缺点： 执行效率低 如果对象需要回收的数量比较大，那么它的执行效率必然会随着数量的增加而减少 内部空间碎片化 由于被回收的对象位置不能保证是连续的，必然会产生大量的不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作 标记复制算法新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间 HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1，也即每次新生代中可用内存空间为整个新生代容量的90%（Eden的80%加上一个Survivor的10%），只有一个Survivor空间，即10%的新生代是会被“浪费”的。当然，98%的对象可被回收仅仅是“普通场景”下测得的数据，任何人都没有办法百分百保证每次回收都只有不多于10%的对象存活，因此Appel式回收还有一个充当罕见情况的“逃生门”的安全设计，当Survivor空间不足以容纳一次Minor GC之后存活的对象时，就需要依赖其他内存区域（实际上大多就是老年代）进行分配担保（Handle Promotion） 标记-整理算法标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存 标记-整理算法和标记复制算法区别 Stop TheWorld定义：全程暂停用户应用程序才能进行的过程 如果移动存活对象，尤其是在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行[插图]，这就更加让使用者不得不小心翼翼地权衡其弊端了，像这样的停顿被最初的虚拟机设计者形象地描述为“Stop TheWorld” 关注吞吐量的Parallel Scavenge收集器是基于标记-整理算法的 方法区的回收回收类型两种：废弃的常量和不再使用的类 废弃的常量假如一个字符串“java”曾经进入常量池中，但是当前系统又没有任何一个字符串对象的值是“java”，换句话说，已经没有任何字符串对象引用常量池中的“java”常量，且虚拟机中也没有其他地方引用这个字面量。如果在这时发生内存回收，而且垃圾收集器判断确有必要的话，这个“java”常量就将会被系统清理出常量池 不再使用的类如何确定一个类是一个不再使用的类那，需要满足一下三个条件 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法 垃圾回收(堆)的具体过程开始开始的时候，一定是stop the word的状态，就是所有的用户线程都会停止的，但是具体什么时候进行垃圾回收那，就是到达**安全点**的时候 安全点的选取标准是是否具有让程序长时间执行的特征”为标准进行选定的。“长时间执行”的最明显特征就是指令序列的复用，例如方法调用、循环跳转、异常跳转等都属于指令序列复用，所以只有具有这些功能的指令才会产生安全点 到达安全点会停顿下来，那么停顿方式是什么那？ 抢占式中断 主动式中断 抢占式中断：抢先式中断不需要线程的执行代码主动去配合，在垃圾收集发生时，系统首先把所有用户线程全部中断，如果发现有用户线程中断的地方不在安全点上，就恢复这条线程执行，让它一会再重新中断，直到跑到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程响应GC事件 主动式中断：当垃圾收集需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志位，各个线程执行过程时会不停地主动去轮询这个标志，一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的，另外还要加上所有创建对象和其他需要在Java堆上分配内存的地方，这是为了检查是否即将要发生垃圾收集，避免没有足够内存分配新对象 安全区域：安全区域是指能够确保在某一段代码片段之中，引用关系不会发生变化，因此，在这个区域中任意地方开始垃圾收集都是安全的。我们也可以把安全区域看作被扩展拉伸了的安全点。 程序执行的时候可以使用到达安全点的方法来进行垃圾回收，程序不执行的时候，就需要采用安全区域的方来进行垃圾回收了 当用户线程执行到安全区域里面的代码时，首先会标识自己已经进入了安全区域，那样当这段时间里虚拟机要发起垃圾收集时就不必去管这些已声明自己在安全区域内的线程了。当线程要离开安全区域时，它要检查虚拟机是否已经完成了根节点枚举（或者垃圾收集过程中其他需要暂停用户线程的阶段），如果完成了，那线程就当作没事发生过，继续执行；否则它就必须一直等待，直到收到可以离开安全区域的信号为止 根节点枚举如果采用的是可达性分析性算法将会从GC Roots集合中查找引用链 GC Roots集合：通常是常量、类静态属性、栈帧中的本地变量表 由于目前java应用越做越大，每次都要从GC Roots集合中找会很慢，于是HotSpot给出了一种解决方案，使用OopMap的数据结构来达到这个目的。一旦类加载动作完成的时候，HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，在即时编译（见第11章）过程中，也会在特定的位置记录下栈里和寄存器里哪些位置是引用。这样收集器在扫描时就可以直接得知这些信息了，并不需要真正一个不漏地从方法区等GC Roots开始查找 但是并不是每一次的操作都会存到OopMap的数据结构中，这样做的话所需要的空间太大了，违背最初节省空空间的想法，因此通常是到了安全点之后，才会进行 数据结构OopMap 记忆集 作用：记录从非收集区域指向收集区域的指针集合的抽象数据结构 目前经常采用的实现记忆集的方式是卡表 卡表和记忆集的关系可以用hashmap和map之间的关系来对比，一个是具体实现方式，一个是抽象数据结构 一个卡页的内存中通常包含不止一个对象，只要卡页内有一个（或更多）对象的字段存在着跨代指针，那就将对应卡表的数组元素的值标识为1，称为这个元素变脏（Dirty），没有则标识为0。在垃圾收集发生时，只要筛选出卡表中变脏的元素，就能轻易得出哪些卡页内存块中包含跨代指针，把它们加入GC Roots中一并扫描。 并发情况下是如何保证卡表变脏的，是采用写屏障的方式 垃圾回收器的优化把遍历对象图中遇到的对象按照“是否访问过”这个条件标记成以下三种颜色 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段，所有的对象都是白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达 黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象）指向某个白色对象 灰色：表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过 “对象消失”的问题需要同时满足以下两种条件 赋值器插入了一条或多条从黑色对象到白色对象的新引用 ·赋值器删除了全部从灰色对象到该白色对象的直接或间接引用 因此，我们要解决并发扫描时的对象消失问题，只需破坏这两个条件的任意一个即可。由此分别产生了两种解决方案：增量更新（Incremental Update）和原始快照（Snapshot At The Beginning，SATB） 增量更新要破坏的是第一个条件，当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中的黑色对象为根，重新扫描一次。这可以简化理解为，黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了 原始快照要破坏的是第二个条件，当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。这也可以简化理解为，无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索 增量更新和原始快照这两种解决方案都有实际应用，譬如，CMS是基于增量更新来做并发标记的，G1、Shenandoah则是用原始快照来实现。","link":"/posts/2864247415.html"}],"tags":[{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"RabbitMQ","slug":"RabbitMQ","link":"/tags/RabbitMQ/"},{"name":"rocketmq","slug":"rocketmq","link":"/tags/rocketmq/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/tags/SpringBoot/"},{"name":"SpringCloud","slug":"SpringCloud","link":"/tags/SpringCloud/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"java并发","slug":"java并发","link":"/tags/java%E5%B9%B6%E5%8F%91/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"idea","slug":"idea","link":"/tags/idea/"},{"name":"java基础","slug":"java基础","link":"/tags/java%E5%9F%BA%E7%A1%80/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"rediss","slug":"rediss","link":"/tags/rediss/"},{"name":"gradle","slug":"gradle","link":"/tags/gradle/"},{"name":"网络","slug":"网络","link":"/tags/%E7%BD%91%E7%BB%9C/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"闲聊","slug":"闲聊","link":"/tags/%E9%97%B2%E8%81%8A/"},{"name":"互联网","slug":"互联网","link":"/tags/%E4%BA%92%E8%81%94%E7%BD%91/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"栈","slug":"栈","link":"/tags/%E6%A0%88/"},{"name":"年度总结","slug":"年度总结","link":"/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"缓存","slug":"缓存","link":"/tags/%E7%BC%93%E5%AD%98/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"categories":[]}